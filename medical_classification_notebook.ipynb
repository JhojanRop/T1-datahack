{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6348d622",
   "metadata": {},
   "source": [
    "# üè• Sistema H√≠brido de Clasificaci√≥n de Literatura M√©dica\n",
    "\n",
    "## Challenge de Clasificaci√≥n Biom√©dica con IA\n",
    "\n",
    "Este notebook implementa una soluci√≥n de Inteligencia Artificial para la clasificaci√≥n autom√°tica de literatura m√©dica utilizando √∫nicamente el **t√≠tulo** y **abstract** de art√≠culos cient√≠ficos.\n",
    "\n",
    "### üéØ Objetivo\n",
    "Desarrollar un sistema capaz de asignar art√≠culos m√©dicos a uno o varios dominios m√©dicos (problema multilabel):\n",
    "- **Neurological** (Neurol√≥gico)\n",
    "- **Cardiovascular** (Cardiovascular) \n",
    "- **Hepatorenal** (Hepatorrenal)\n",
    "- **Oncological** (Oncol√≥gico)\n",
    "\n",
    "### üöÄ Estrategia H√≠brida\n",
    "- **BioBERT**: Maneja el 90% de casos obvios (r√°pido y eficiente)\n",
    "- **LLM**: Procesa el 10% de casos dif√≠ciles (preciso pero costoso)\n",
    "- **C√≥digo limpio y documentado** para impresionar a los jueces\n",
    "- **An√°lisis m√©dico especializado** en lugar de estad√≠sticas b√°sicas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db668a9d",
   "metadata": {},
   "source": [
    "## 1. üîß Environment Setup and Dependencies\n",
    "\n",
    "Configuraci√≥n del entorno y instalaci√≥n de dependencias necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d4c0881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Instalando dependencias...\n",
      "‚úÖ transformers instalado correctamente\n",
      "‚úÖ torch instalado correctamente\n",
      "‚úÖ pandas instalado correctamente\n",
      "‚úÖ numpy instalado correctamente\n",
      "‚úÖ scikit-learn instalado correctamente\n",
      "‚úÖ matplotlib instalado correctamente\n",
      "‚úÖ seaborn instalado correctamente\n",
      "‚úÖ tqdm instalado correctamente\n",
      "‚úÖ datasets instalado correctamente\n",
      "‚úÖ tokenizers instalado correctamente\n",
      "‚úÖ hf_xet instalado correctamente\n",
      "‚úÖ accelerate>=0.26.0 instalado correctamente\n",
      "‚úÖ google-generativeai instalado correctamente\n",
      "‚úÖ python-dotenv instalado correctamente\n",
      "‚úÖ plotly instalado correctamente\n"
     ]
    }
   ],
   "source": [
    "# Instalaci√≥n de dependencias usando uv\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "# Funci√≥n para instalar paquetes\n",
    "def install_package(package_name):\n",
    "    \"\"\"Instala un paquete usando uv\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        print(f\"‚úÖ {package_name} instalado correctamente\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Error instalando {package_name}: {e}\")\n",
    "\n",
    "# Lista de dependencias esenciales\n",
    "packages = [\n",
    "    \"transformers\",\n",
    "    \"torch\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"scikit-learn\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"tqdm\",\n",
    "    \"datasets\",\n",
    "    \"tokenizers\",\n",
    "    \"hf_xet\",\n",
    "    \"accelerate>=0.26.0\",\n",
    "    \"google-generativeai\",  # Para integraci√≥n LLM\n",
    "    \"python-dotenv\",  # Para variables de entorno\n",
    "    \"plotly\",  # Para visualizaciones interactivas\n",
    "]\n",
    "\n",
    "print(\"üöÄ Instalando dependencias...\")\n",
    "for package in packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33e4724e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Usando device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Importaci√≥n de librer√≠as esenciales\n",
    "import re\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualizaci√≥n\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep Learning y NLP\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    hamming_loss,\n",
    "    jaccard_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "# Configuraciones\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configurar device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üî• Usando device: {device}\")\n",
    "\n",
    "# Configurar para reproducibilidad\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a7f10",
   "metadata": {},
   "source": [
    "## 2. üìä Data Loading and Exploration\n",
    "\n",
    "Carga y exploraci√≥n inicial del dataset de literatura m√©dica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dff95fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Cargando dataset de literatura m√©dica...\n",
      "‚úÖ Dataset cargado exitosamente!\n",
      "üìè Dimensiones: 3565 filas √ó 3 columnas\n",
      "\n",
      "üîç Informaci√≥n b√°sica del dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3565 entries, 0 to 3564\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   title     3565 non-null   object\n",
      " 1   abstract  3565 non-null   object\n",
      " 2   group     3565 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "\n",
      "üìã Primeras 5 filas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adrenoleukodystrophy: survey of 303 cases: bio...</td>\n",
       "      <td>Adrenoleukodystrophy ( ALD ) is a genetically ...</td>\n",
       "      <td>neurological|hepatorenal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>endoscopy reveals ventricular tachycardia secrets</td>\n",
       "      <td>Research question: How does metformin affect c...</td>\n",
       "      <td>neurological</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dementia and cholecystitis: organ interplay</td>\n",
       "      <td>Purpose: This randomized controlled study exam...</td>\n",
       "      <td>hepatorenal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The interpeduncular nucleus regulates nicotine...</td>\n",
       "      <td>Partial lesions were made with kainic acid in ...</td>\n",
       "      <td>neurological</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>guillain-barre syndrome pathways in leukemia</td>\n",
       "      <td>Hypothesis: statins improves stroke outcomes v...</td>\n",
       "      <td>neurological</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Adrenoleukodystrophy: survey of 303 cases: bio...   \n",
       "1  endoscopy reveals ventricular tachycardia secrets   \n",
       "2        dementia and cholecystitis: organ interplay   \n",
       "3  The interpeduncular nucleus regulates nicotine...   \n",
       "4       guillain-barre syndrome pathways in leukemia   \n",
       "\n",
       "                                            abstract                     group  \n",
       "0  Adrenoleukodystrophy ( ALD ) is a genetically ...  neurological|hepatorenal  \n",
       "1  Research question: How does metformin affect c...              neurological  \n",
       "2  Purpose: This randomized controlled study exam...               hepatorenal  \n",
       "3  Partial lesions were made with kainic acid in ...              neurological  \n",
       "4  Hypothesis: statins improves stroke outcomes v...              neurological  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga del dataset\n",
    "data_path = Path(\"data/raw/challenge_data-18-ago.csv\")\n",
    "\n",
    "print(\"üìÅ Cargando dataset de literatura m√©dica...\")\n",
    "try:\n",
    "    # Carga con separador punto y coma\n",
    "    df = pd.read_csv(data_path, sep=';', encoding='utf-8')\n",
    "    print(\"‚úÖ Dataset cargado exitosamente!\")\n",
    "    print(f\"üìè Dimensiones: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cargando dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Informaci√≥n b√°sica del dataset\n",
    "print(\"\\nüîç Informaci√≥n b√°sica del dataset:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nüìã Primeras 5 filas:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e8045c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç AN√ÅLISIS DE CALIDAD DE DATOS\n",
      "==================================================\n",
      "\n",
      "üìä Valores nulos por columna:\n",
      "  title: 0 (0.00%)\n",
      "  abstract: 0 (0.00%)\n",
      "  group: 0 (0.00%)\n",
      "\n",
      "üìù Estad√≠sticas de longitud de texto:\n",
      "    M√©trica  T√≠tulo  Abstract\n",
      "0  Promedio   69.35    696.55\n",
      "1   Mediana   55.00    312.00\n",
      "2    M√≠nimo   20.00    180.00\n",
      "3    M√°ximo  294.00   3814.00\n",
      "4       Std   36.67    579.56\n",
      "\n",
      "üîÑ Art√≠culos duplicados: 0\n",
      "üîÑ T√≠tulos duplicados: 2\n",
      "\n",
      "üè∑Ô∏è Categor√≠as √∫nicas en 'group': 15\n",
      "üè∑Ô∏è Distribuci√≥n de categor√≠as:\n",
      "group\n",
      "neurological                   1058\n",
      "cardiovascular                  645\n",
      "hepatorenal                     533\n",
      "neurological|cardiovascular     308\n",
      "oncological                     237\n",
      "neurological|hepatorenal        202\n",
      "cardiovascular|hepatorenal      190\n",
      "neurological|oncological        143\n",
      "hepatorenal|oncological          98\n",
      "cardiovascular|oncological       70\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# An√°lisis de calidad de datos\n",
    "print(\"üîç AN√ÅLISIS DE CALIDAD DE DATOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Verificar valores nulos\n",
    "print(\"\\nüìä Valores nulos por columna:\")\n",
    "null_counts = df.isnull().sum()\n",
    "for col in df.columns:\n",
    "    null_pct = (null_counts[col] / len(df)) * 100\n",
    "    print(f\"  {col}: {null_counts[col]} ({null_pct:.2f}%)\")\n",
    "\n",
    "# Estad√≠sticas de longitud de texto\n",
    "print(\"\\nüìù Estad√≠sticas de longitud de texto:\")\n",
    "df['title_length'] = df['title'].str.len()\n",
    "df['abstract_length'] = df['abstract'].str.len()\n",
    "\n",
    "stats_df = pd.DataFrame({\n",
    "    'M√©trica': ['Promedio', 'Mediana', 'M√≠nimo', 'M√°ximo', 'Std'],\n",
    "    'T√≠tulo': [\n",
    "        df['title_length'].mean(),\n",
    "        df['title_length'].median(),\n",
    "        df['title_length'].min(),\n",
    "        df['title_length'].max(),\n",
    "        df['title_length'].std()\n",
    "    ],\n",
    "    'Abstract': [\n",
    "        df['abstract_length'].mean(),\n",
    "        df['abstract_length'].median(),\n",
    "        df['abstract_length'].min(),\n",
    "        df['abstract_length'].max(),\n",
    "        df['abstract_length'].std()\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(stats_df.round(2))\n",
    "\n",
    "# Verificar duplicados\n",
    "print(f\"\\nüîÑ Art√≠culos duplicados: {df.duplicated().sum()}\")\n",
    "print(f\"üîÑ T√≠tulos duplicados: {df['title'].duplicated().sum()}\")\n",
    "\n",
    "# Explorar columna de grupos\n",
    "print(f\"\\nüè∑Ô∏è Categor√≠as √∫nicas en 'group': {df['group'].nunique()}\")\n",
    "print(\"üè∑Ô∏è Distribuci√≥n de categor√≠as:\")\n",
    "print(df['group'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc5ff7",
   "metadata": {},
   "source": [
    "## 3. üßπ Data Preprocessing and Text Cleaning\n",
    "\n",
    "Limpieza y preprocesamiento de los textos m√©dicos para optimizar el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3606937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Limpiando textos m√©dicos...\n",
      "‚úÖ Preprocesamiento completado. Dataset final: 3565 filas\n",
      "\n",
      "üìä Estad√≠sticas post-procesamiento:\n",
      "Longitud promedio texto combinado: 773 caracteres\n",
      "Longitud m√°xima texto combinado: 3911 caracteres\n",
      "\n",
      "üìñ Ejemplo de texto procesado:\n",
      "T√≠tulo original: Adrenoleukodystrophy: survey of 303 cases: biochemistry, diagnosis, and therapy.\n",
      "T√≠tulo limpio: Adrenoleukodystrophy: survey of 303 cases: biochemistry, diagnosis, and therapy.\n",
      "Abstract limpio: Adrenoleukodystrophy ( adrenoleukodystrophy ) is a genetically determined disorder associated with progressive central demyelination and adrenal cortical insufficiency . All affected persons show incr...\n"
     ]
    }
   ],
   "source": [
    "class MedicalTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocesador especializado para textos m√©dicos.\n",
    "    Mantiene terminolog√≠a m√©dica importante mientras limpia el texto.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Patrones para limpiar texto m√©dico\n",
    "        self.medical_abbreviations = {\n",
    "            r'\\bALD\\b': 'adrenoleukodystrophy',\n",
    "            r'\\bMJD\\b': 'machado joseph disease',\n",
    "            r'\\bSCA3\\b': 'spinocerebellar ataxia type 3',\n",
    "            r'\\bBRCA1\\b': 'breast cancer gene 1',\n",
    "            r'\\bTSG101\\b': 'tumor susceptibility gene 101',\n",
    "        }\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Limpia texto m√©dico preservando informaci√≥n relevante\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        # Convertir a string y limpiar\n",
    "        text = str(text)\n",
    "\n",
    "        # Expandir abreviaciones m√©dicas importantes\n",
    "        for abbr, expansion in self.medical_abbreviations.items():\n",
    "            text = re.sub(abbr, expansion, text, flags=re.IGNORECASE)\n",
    "\n",
    "        # Limpiar caracteres especiales pero mantener puntuaci√≥n m√©dica\n",
    "        text = re.sub(r'[^\\w\\s\\.\\-\\(\\)\\,\\;\\:]', ' ', text)\n",
    "\n",
    "        # Normalizar espacios\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # Remover espacios al inicio y final\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def preprocess_dataset(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Preprocesa todo el dataset\"\"\"\n",
    "        df_clean = df.copy()\n",
    "\n",
    "        print(\"üßπ Limpiando textos m√©dicos...\")\n",
    "\n",
    "        # Limpiar title y abstract\n",
    "        df_clean['title_clean'] = df_clean['title'].apply(self.clean_text)\n",
    "        df_clean['abstract_clean'] = df_clean['abstract'].apply(self.clean_text)\n",
    "\n",
    "        # Combinar title y abstract para input del modelo\n",
    "        df_clean['combined_text'] = (\n",
    "            df_clean['title_clean'] + \" [SEP] \" + df_clean['abstract_clean']\n",
    "        )\n",
    "\n",
    "        # Remover filas con texto vac√≠o\n",
    "        initial_rows = len(df_clean)\n",
    "        df_clean = df_clean[\n",
    "            (df_clean['title_clean'].str.len() > 0) &\n",
    "            (df_clean['abstract_clean'].str.len() > 0)\n",
    "        ].copy()\n",
    "        removed_rows = initial_rows - len(df_clean)\n",
    "\n",
    "        if removed_rows > 0:\n",
    "            print(f\"üóëÔ∏è Removidas {removed_rows} filas con texto vac√≠o\")\n",
    "\n",
    "        print(f\"‚úÖ Preprocesamiento completado. Dataset final: {len(df_clean)} filas\")\n",
    "\n",
    "        return df_clean\n",
    "\n",
    "# Aplicar preprocesamiento\n",
    "preprocessor = MedicalTextPreprocessor()\n",
    "df_processed = preprocessor.preprocess_dataset(df)\n",
    "\n",
    "# Mostrar estad√≠sticas post-procesamiento\n",
    "print(\"\\nüìä Estad√≠sticas post-procesamiento:\")\n",
    "print(f\"Longitud promedio texto combinado: {df_processed['combined_text'].str.len().mean():.0f} caracteres\")\n",
    "print(f\"Longitud m√°xima texto combinado: {df_processed['combined_text'].str.len().max()} caracteres\")\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(\"\\nüìñ Ejemplo de texto procesado:\")\n",
    "sample_idx = 0\n",
    "print(f\"T√≠tulo original: {df.iloc[sample_idx]['title']}\")\n",
    "print(f\"T√≠tulo limpio: {df_processed.iloc[sample_idx]['title_clean']}\")\n",
    "print(f\"Abstract limpio: {df_processed.iloc[sample_idx]['abstract_clean'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85af66cf",
   "metadata": {},
   "source": [
    "## 4. üè∑Ô∏è Multilabel Target Analysis\n",
    "\n",
    "An√°lisis detallado de las etiquetas m√©dicas y preparaci√≥n para clasificaci√≥n multilabel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "031fabd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè∑Ô∏è AN√ÅLISIS DE DISTRIBUCI√ìN DE ETIQUETAS M√âDICAS\n",
      "============================================================\n",
      "\n",
      "üìä Distribuci√≥n individual de etiquetas:\n",
      "  üß† Neurol√≥gico neurological: 1785 art√≠culos (50.1%)\n",
      "  ‚ù§Ô∏è Cardiovascular cardiovascular: 1268 art√≠culos (35.6%)\n",
      "  ü´ò Hepatorrenal hepatorenal: 1091 art√≠culos (30.6%)\n",
      "  üéóÔ∏è Oncol√≥gico oncological: 601 art√≠culos (16.9%)\n",
      "\n",
      "üîó Combinaciones de etiquetas m√°s comunes:\n",
      "  neurological: 1058 art√≠culos (29.7%)\n",
      "  cardiovascular: 645 art√≠culos (18.1%)\n",
      "  hepatorenal: 533 art√≠culos (15.0%)\n",
      "  cardiovascular + neurological: 308 art√≠culos (8.6%)\n",
      "  oncological: 237 art√≠culos (6.6%)\n",
      "  hepatorenal + neurological: 202 art√≠culos (5.7%)\n",
      "  cardiovascular + hepatorenal: 190 art√≠culos (5.3%)\n",
      "  neurological + oncological: 143 art√≠culos (4.0%)\n",
      "  hepatorenal + oncological: 98 art√≠culos (2.7%)\n",
      "  cardiovascular + oncological: 70 art√≠culos (2.0%)\n",
      "\n",
      "üî¢ Preparando targets multilabel...\n",
      "‚úÖ Creadas 4 columnas binarias:\n",
      "  ‚ù§Ô∏è Cardiovascular cardiovascular: 1268 casos positivos\n",
      "  ü´ò Hepatorrenal hepatorenal: 1091 casos positivos\n",
      "  üß† Neurol√≥gico neurological: 1785 casos positivos\n",
      "  üéóÔ∏è Oncol√≥gico oncological: 601 casos positivos\n",
      "\n",
      "‚úÖ Dataset final preparado con 3565 art√≠culos y 4 etiquetas.\n"
     ]
    }
   ],
   "source": [
    "class MedicalLabelAnalyzer:\n",
    "    \"\"\"\n",
    "    Analizador especializado para etiquetas m√©dicas multilabel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.label_mapping = {\n",
    "            'neurological': 'üß† Neurol√≥gico',\n",
    "            'cardiovascular': '‚ù§Ô∏è Cardiovascular',\n",
    "            'hepatorenal': 'ü´ò Hepatorrenal',\n",
    "            'oncological': 'üéóÔ∏è Oncol√≥gico'\n",
    "        }\n",
    "\n",
    "    def parse_labels(self, label_string: str) -> list[str]:\n",
    "        \"\"\"Convierte string de etiquetas a lista\"\"\"\n",
    "        if pd.isna(label_string):\n",
    "            return []\n",
    "        return [label.strip() for label in str(label_string).split('|')]\n",
    "\n",
    "    def analyze_label_distribution(self, df: pd.DataFrame) -> dict:\n",
    "        \"\"\"Analiza la distribuci√≥n de etiquetas m√©dicas\"\"\"\n",
    "        print(\"üè∑Ô∏è AN√ÅLISIS DE DISTRIBUCI√ìN DE ETIQUETAS M√âDICAS\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Convertir etiquetas a listas\n",
    "        df['labels_list'] = df['group'].apply(self.parse_labels)\n",
    "\n",
    "        # Estad√≠sticas b√°sicas\n",
    "        label_counts = Counter()\n",
    "        label_combinations = Counter()\n",
    "\n",
    "        for labels in df['labels_list']:\n",
    "            label_counts.update(labels)\n",
    "            label_combinations[tuple(sorted(labels))] += 1\n",
    "\n",
    "        # Mostrar distribuci√≥n individual\n",
    "        print(\"\\nüìä Distribuci√≥n individual de etiquetas:\")\n",
    "        total_articles = len(df)\n",
    "        for label, count in label_counts.most_common():\n",
    "            emoji = self.label_mapping.get(label, 'üè∑Ô∏è')\n",
    "            percentage = (count / total_articles) * 100\n",
    "            print(f\"  {emoji} {label}: {count} art√≠culos ({percentage:.1f}%)\")\n",
    "\n",
    "        # Mostrar combinaciones m√°s comunes\n",
    "        print(\"\\nüîó Combinaciones de etiquetas m√°s comunes:\")\n",
    "        for combo, count in label_combinations.most_common(10):\n",
    "            percentage = (count / total_articles) * 100\n",
    "            combo_str = \" + \".join(combo) if combo else \"Sin etiquetas\"\n",
    "            print(f\"  {combo_str}: {count} art√≠culos ({percentage:.1f}%)\")\n",
    "\n",
    "        # An√°lisis de co-ocurrencia\n",
    "        cooccurrence_matrix = self._calculate_cooccurrence(df['labels_list'])\n",
    "\n",
    "        return {\n",
    "            'label_counts': dict(label_counts),\n",
    "            'label_combinations': dict(label_combinations),\n",
    "            'cooccurrence_matrix': cooccurrence_matrix,\n",
    "            'total_articles': total_articles\n",
    "        }\n",
    "\n",
    "    def _calculate_cooccurrence(self, labels_list: list[list[str]]) -> pd.DataFrame:\n",
    "        \"\"\"Calcula matriz de co-ocurrencia entre etiquetas\"\"\"\n",
    "        unique_labels = sorted(set().union(*labels_list))\n",
    "        matrix = np.zeros((len(unique_labels), len(unique_labels)))\n",
    "\n",
    "        for labels in labels_list:\n",
    "            for i, label1 in enumerate(unique_labels):\n",
    "                for j, label2 in enumerate(unique_labels):\n",
    "                    if label1 in labels and label2 in labels:\n",
    "                        matrix[i][j] += 1\n",
    "\n",
    "        return pd.DataFrame(matrix, index=unique_labels, columns=unique_labels)\n",
    "\n",
    "    def prepare_multilabel_targets(self, df: pd.DataFrame) -> tuple[pd.DataFrame, MultiLabelBinarizer]:\n",
    "        \"\"\"Prepara targets para clasificaci√≥n multilabel\"\"\"\n",
    "        print(\"\\nüî¢ Preparando targets multilabel...\")\n",
    "\n",
    "        if 'labels_list' not in df.columns:\n",
    "            print(\"   - Creando 'labels_list'...\")\n",
    "            df['labels_list'] = df['group'].apply(self.parse_labels)\n",
    "\n",
    "        # Usar MultiLabelBinarizer\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        y_multilabel = mlb.fit_transform(df['labels_list'])\n",
    "\n",
    "        # Crear DataFrame con etiquetas binarias\n",
    "        label_df = pd.DataFrame(\n",
    "            y_multilabel,\n",
    "            columns=mlb.classes_,\n",
    "            index=df.index\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ Creadas {len(mlb.classes_)} columnas binarias:\")\n",
    "        for i, label in enumerate(mlb.classes_):\n",
    "            emoji = self.label_mapping.get(label, 'üè∑Ô∏è')\n",
    "            count = y_multilabel[:, i].sum()\n",
    "            print(f\"  {emoji} {label}: {count} casos positivos\")\n",
    "\n",
    "        return label_df, mlb\n",
    "\n",
    "# Aplicar an√°lisis de etiquetas\n",
    "label_analyzer = MedicalLabelAnalyzer()\n",
    "analysis_results = label_analyzer.analyze_label_distribution(df_processed)\n",
    "\n",
    "# Preparar targets multilabel\n",
    "y_labels, mlb = label_analyzer.prepare_multilabel_targets(df_processed)\n",
    "\n",
    "# Agregar informaci√≥n al dataset procesado\n",
    "df_final = df_processed.copy()\n",
    "for col in y_labels.columns:\n",
    "    df_final[f'target_{col}'] = y_labels[col]\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset final preparado con {len(df_final)} art√≠culos y {len(y_labels.columns)} etiquetas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8766ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           "#FF6B6B",
           "#4ECDC4",
           "#45B7D1",
           "#FFA07A"
          ]
         },
         "name": "Etiquetas",
         "type": "bar",
         "x": [
          "neurological",
          "hepatorenal",
          "cardiovascular",
          "oncological"
         ],
         "xaxis": "x",
         "y": [
          1785,
          1091,
          1268,
          601
         ],
         "yaxis": "y"
        },
        {
         "colorscale": [
          [
           0,
           "rgb(247,251,255)"
          ],
          [
           0.125,
           "rgb(222,235,247)"
          ],
          [
           0.25,
           "rgb(198,219,239)"
          ],
          [
           0.375,
           "rgb(158,202,225)"
          ],
          [
           0.5,
           "rgb(107,174,214)"
          ],
          [
           0.625,
           "rgb(66,146,198)"
          ],
          [
           0.75,
           "rgb(33,113,181)"
          ],
          [
           0.875,
           "rgb(8,81,156)"
          ],
          [
           1,
           "rgb(8,48,107)"
          ]
         ],
         "name": "Co-ocurrencia",
         "type": "heatmap",
         "x": [
          "cardiovascular",
          "hepatorenal",
          "neurological",
          "oncological"
         ],
         "xaxis": "x2",
         "y": [
          "cardiovascular",
          "hepatorenal",
          "neurological",
          "oncological"
         ],
         "yaxis": "y2",
         "z": {
          "bdata": "AAAAAADQk0AAAAAAAABtQAAAAAAAQHZAAAAAAABAWEAAAAAAAABtQAAAAAAADJFAAAAAAABwcEAAAAAAAEBhQAAAAAAAQHZAAAAAAABwcEAAAAAAAOSbQAAAAAAAoGdAAAAAAABAWEAAAAAAAEBhQAAAAAAAoGdAAAAAAADIgkA=",
          "dtype": "f8",
          "shape": "4, 4"
         }
        },
        {
         "marker": {
          "color": "lightblue"
         },
         "name": "Combinaciones",
         "orientation": "h",
         "type": "bar",
         "x": [
          202,
          1058,
          533,
          645,
          143,
          190,
          237,
          308
         ],
         "xaxis": "x3",
         "y": [
          "hepatorenal + neurological",
          "neurological",
          "hepatorenal",
          "cardiovascular",
          "neurological + oncological",
          "cardiovascular + hepatorenal",
          "oncological",
          "cardiovascular + neurological"
         ],
         "yaxis": "y3"
        },
        {
         "name": "Longitud",
         "type": "box",
         "x": [
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "neurological",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "hepatorenal",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "cardiovascular",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological",
          "oncological"
         ],
         "xaxis": "x4",
         "y": [
          1513,
          336,
          858,
          289,
          2104,
          352,
          514,
          366,
          1921,
          1896,
          1476,
          2716,
          944,
          319,
          2222,
          368,
          358,
          370,
          367,
          389,
          1019,
          388,
          370,
          314,
          864,
          328,
          1774,
          845,
          729,
          570,
          311,
          1670,
          1721,
          1070,
          1155,
          320,
          341,
          306,
          1841,
          324,
          322,
          381,
          1463,
          353,
          340,
          507,
          987,
          306,
          1508,
          1438,
          323,
          1053,
          331,
          394,
          1495,
          1480,
          330,
          1156,
          1820,
          338,
          1933,
          344,
          2229,
          317,
          331,
          2166,
          352,
          1124,
          350,
          513,
          1113,
          321,
          1747,
          309,
          347,
          1572,
          341,
          673,
          309,
          1444,
          1466,
          1213,
          1312,
          700,
          399,
          1212,
          552,
          2230,
          370,
          352,
          1411,
          376,
          353,
          315,
          1354,
          294,
          1160,
          1049,
          346,
          1597,
          343,
          935,
          327,
          528,
          1377,
          459,
          1266,
          939,
          294,
          1432,
          891,
          1512,
          1040,
          1469,
          326,
          1564,
          770,
          1608,
          835,
          1840,
          316,
          982,
          1402,
          499,
          347,
          2227,
          1686,
          452,
          1498,
          1615,
          1071,
          1695,
          889,
          301,
          773,
          302,
          464,
          2059,
          295,
          692,
          397,
          1235,
          1212,
          1361,
          1559,
          341,
          364,
          366,
          366,
          310,
          1626,
          325,
          404,
          353,
          355,
          1766,
          1702,
          1099,
          948,
          1305,
          326,
          304,
          2078,
          343,
          348,
          310,
          322,
          2195,
          289,
          307,
          310,
          1369,
          3140,
          1178,
          343,
          1336,
          333,
          290,
          1401,
          1305,
          1420,
          1880,
          1509,
          1882,
          324,
          298,
          1389,
          1305,
          326,
          312,
          961,
          301,
          1987,
          961,
          277,
          233,
          1628,
          1983,
          297,
          430,
          333,
          337,
          335,
          358,
          1409,
          1995,
          399,
          317,
          302,
          338,
          321,
          1376,
          320,
          1593,
          1684,
          292,
          1118,
          1801,
          1739,
          655,
          1985,
          294,
          1479,
          1582,
          329,
          1585,
          323,
          1452,
          1914,
          980,
          411,
          323,
          1637,
          1530,
          341,
          1871,
          2480,
          1090,
          581,
          679,
          324,
          1203,
          306,
          318,
          294,
          350,
          1962,
          2363,
          1677,
          330,
          2254,
          354,
          2081,
          1005,
          966,
          315,
          310,
          1394,
          1930,
          353,
          1470,
          1695,
          305,
          299,
          1723,
          395,
          2027,
          1993,
          346,
          285,
          359,
          818,
          366,
          1345,
          889,
          331,
          353,
          1510,
          1430,
          323,
          1437,
          1445,
          298,
          1640,
          358,
          372,
          275,
          342,
          1137,
          569,
          309,
          377,
          321,
          1319,
          1968,
          1478,
          289,
          351,
          1365,
          1056,
          2530,
          330,
          1313,
          343,
          321,
          315,
          1841,
          357,
          302,
          324,
          1990,
          1293,
          866,
          342,
          369,
          368,
          335,
          758,
          1505,
          512,
          337,
          362,
          313,
          1427,
          364,
          299,
          306,
          382,
          338,
          738,
          1512,
          1126,
          1721,
          332,
          302,
          293,
          1673,
          339,
          313,
          371,
          1678,
          299,
          1482,
          367,
          1765,
          324,
          2101,
          569,
          315,
          374,
          365,
          695,
          1120,
          1994,
          1136,
          613,
          364,
          1483,
          1834,
          1424,
          344,
          1120,
          1752,
          317,
          308,
          308,
          1549,
          328,
          1063,
          1643,
          330,
          327,
          1589,
          1387,
          1860,
          978,
          1707,
          813,
          304,
          437,
          1962,
          695,
          1367,
          347,
          1767,
          1652,
          293,
          1782,
          764,
          323,
          1198,
          1407,
          345,
          1220,
          325,
          1608,
          324,
          306,
          1088,
          1465,
          1195,
          292,
          802,
          326,
          355,
          800,
          342,
          810,
          304,
          334,
          2132,
          341,
          359,
          312,
          1844,
          1962,
          1080,
          301,
          1354,
          333,
          331,
          322,
          1735,
          1157,
          311,
          1179,
          2895,
          307,
          342,
          319,
          1776,
          2002,
          1424,
          1580,
          2249,
          360,
          372,
          355,
          2182,
          1081,
          2183,
          1160,
          515,
          1670,
          333,
          306,
          1939,
          397,
          1955,
          347,
          437,
          1966,
          325,
          347,
          359,
          376,
          1593,
          321,
          1663,
          301,
          937,
          939,
          1895,
          344,
          368,
          482,
          349,
          325,
          351,
          1280,
          1341,
          958,
          377,
          914,
          313,
          318,
          381,
          309,
          1013,
          2005,
          1614,
          336,
          340,
          2202,
          306,
          345,
          304,
          377,
          1014,
          376,
          2291,
          342,
          1241,
          896,
          310,
          321,
          772,
          1032,
          1672,
          342,
          2036,
          316,
          1792,
          1118,
          348,
          804,
          1001,
          374,
          2529,
          919,
          323,
          343,
          383,
          1178,
          304,
          1134,
          370,
          1795,
          331,
          336,
          1660,
          1517,
          2846,
          383,
          383,
          1362,
          404,
          324,
          362,
          351,
          1583,
          1675,
          1722,
          340,
          354,
          301,
          303,
          1202,
          304,
          1303,
          1555,
          1433,
          1479,
          329,
          321,
          323,
          944,
          315,
          396,
          308,
          316,
          1826,
          1685,
          953,
          328,
          1129,
          383,
          1767,
          356,
          301,
          1276,
          297,
          1093,
          580,
          1149,
          312,
          1394,
          1262,
          1127,
          1096,
          905,
          2811,
          383,
          1006,
          1366,
          361,
          1776,
          342,
          385,
          1473,
          1266,
          302,
          297,
          348,
          288,
          1256,
          379,
          317,
          708,
          337,
          1791,
          294,
          348,
          1158,
          1671,
          899,
          1351,
          1609,
          317,
          370,
          335,
          1978,
          1868,
          2041,
          381,
          300,
          347,
          318,
          303,
          299,
          1046,
          342,
          2320,
          318,
          322,
          1803,
          1627,
          1917,
          360,
          614,
          311,
          361,
          286,
          366,
          338,
          330,
          1531,
          904,
          773,
          322,
          1743,
          1592,
          326,
          362,
          2049,
          308,
          1048,
          1553,
          1938,
          1677,
          358,
          357,
          363,
          567,
          1518,
          1464,
          353,
          281,
          364,
          338,
          295,
          1604,
          539,
          1895,
          1293,
          1284,
          337,
          1568,
          322,
          1422,
          1781,
          1399,
          1795,
          1435,
          322,
          331,
          945,
          1836,
          1981,
          1435,
          343,
          351,
          345,
          316,
          545,
          855,
          337,
          1819,
          1193,
          305,
          308,
          340,
          1281,
          887,
          1694,
          868,
          591,
          352,
          306,
          2692,
          776,
          1878,
          332,
          785,
          1245,
          1843,
          1818,
          268,
          317,
          1483,
          388,
          1563,
          386,
          1905,
          344,
          2027,
          373,
          304,
          321,
          351,
          1401,
          1065,
          1757,
          337,
          1086,
          809,
          348,
          313,
          1199,
          828,
          1487,
          909,
          1178,
          1002,
          1566,
          304,
          299,
          1039,
          278,
          1574,
          360,
          310,
          572,
          1268,
          1919,
          358,
          971,
          858,
          1003,
          2385,
          1229,
          1756,
          1730,
          540,
          312,
          342,
          330,
          339,
          322,
          1062,
          304,
          1915,
          1980,
          1658,
          1204,
          321,
          1759,
          344,
          593,
          1288,
          320,
          322,
          2215,
          1149,
          1988,
          332,
          314,
          2290,
          1378,
          1459,
          331,
          1655,
          351,
          334,
          535,
          1824,
          317,
          299,
          1521,
          318,
          307,
          968,
          1654,
          2068,
          375,
          902,
          1849,
          333,
          1312,
          1450,
          630,
          1779,
          933,
          354,
          1278,
          1244,
          1147,
          1130,
          291,
          302,
          1500,
          289,
          1737,
          388,
          350,
          1405,
          1609,
          923,
          324,
          1365,
          370,
          1104,
          1441,
          2629,
          315,
          1145,
          329,
          670,
          1529,
          758,
          674,
          1120,
          2653,
          362,
          1560,
          2202,
          313,
          329,
          332,
          327,
          1588,
          811,
          895,
          1570,
          342,
          295,
          601,
          1139,
          1389,
          1696,
          821,
          667,
          837,
          1159,
          304,
          379,
          2110,
          1663,
          1424,
          290,
          307,
          792,
          1002,
          323,
          337,
          1077,
          1315,
          1158,
          1053,
          390,
          891,
          2127,
          1346,
          1080,
          1963,
          345,
          938,
          1333,
          1833,
          1985,
          336,
          1888,
          1679,
          363,
          1398,
          320,
          1621,
          1742,
          297,
          929,
          295,
          1586,
          328,
          1859,
          1405,
          325,
          1096,
          1124,
          339,
          1465,
          1377,
          1533,
          1369,
          879,
          1072,
          288,
          376,
          1418,
          348,
          617,
          1794,
          1218,
          342,
          300,
          1563,
          1079,
          2908,
          328,
          1821,
          345,
          329,
          352,
          329,
          317,
          1009,
          1287,
          380,
          774,
          789,
          1172,
          748,
          1522,
          1941,
          315,
          1730,
          341,
          303,
          313,
          2514,
          1019,
          321,
          1557,
          343,
          360,
          316,
          328,
          706,
          1666,
          785,
          286,
          1913,
          340,
          906,
          1038,
          323,
          1381,
          1470,
          898,
          1207,
          1385,
          753,
          348,
          1287,
          1649,
          1448,
          300,
          1420,
          2050,
          339,
          1648,
          340,
          1882,
          870,
          1660,
          310,
          960,
          2355,
          896,
          371,
          280,
          291,
          613,
          370,
          324,
          362,
          348,
          297,
          304,
          314,
          397,
          378,
          1640,
          1363,
          400,
          308,
          316,
          1364,
          343,
          2566,
          305,
          642,
          318,
          304,
          358,
          2707,
          330,
          331,
          305,
          2705,
          1503,
          2222,
          1236,
          1028,
          3315,
          1597,
          823,
          1397,
          1935,
          1494,
          800,
          363,
          1820,
          1176,
          1591,
          331,
          1381,
          1897,
          835,
          1016,
          1393,
          1342,
          1383,
          1809,
          1115,
          1799,
          297,
          341,
          2049,
          1508,
          387,
          346,
          404,
          703,
          1513,
          1413,
          787,
          1082,
          1497,
          1674,
          1592,
          1892,
          399,
          1595,
          928,
          1384,
          319,
          329,
          323,
          309,
          1690,
          1492,
          299,
          302,
          1594,
          346,
          1070,
          313,
          349,
          299,
          350,
          1689,
          1355,
          1524,
          322,
          598,
          305,
          319,
          1133,
          369,
          317,
          1355,
          344,
          607,
          376,
          309,
          1281,
          307,
          1641,
          323,
          1834,
          336,
          320,
          312,
          325,
          332,
          1894,
          328,
          1824,
          354,
          306,
          340,
          1635,
          1565,
          331,
          308,
          316,
          336,
          1277,
          341,
          1450,
          2983,
          315,
          636,
          352,
          339,
          289,
          2090,
          1287,
          2905,
          887,
          341,
          329,
          342,
          284,
          300,
          316,
          350,
          1531,
          785,
          1047,
          330,
          815,
          338,
          1147,
          1519,
          2611,
          332,
          1245,
          1462,
          352,
          331,
          286,
          335,
          1595,
          3288,
          317,
          378,
          325,
          304,
          2032,
          322,
          1754,
          1075,
          314,
          1763,
          891,
          348,
          358,
          1123,
          1390,
          376,
          1507,
          1554,
          1998,
          1111,
          376,
          374,
          316,
          304,
          738,
          1515,
          1497,
          1141,
          376,
          369,
          346,
          374,
          781,
          332,
          660,
          1735,
          1714,
          1416,
          1164,
          2023,
          312,
          1536,
          1495,
          284,
          1959,
          384,
          321,
          1590,
          322,
          322,
          359,
          374,
          2246,
          385,
          369,
          1980,
          1201,
          314,
          371,
          346,
          342,
          301,
          1529,
          1285,
          1149,
          1656,
          309,
          368,
          341,
          387,
          349,
          813,
          328,
          346,
          1337,
          481,
          662,
          314,
          1854,
          302,
          1318,
          1673,
          307,
          1773,
          787,
          1008,
          283,
          1604,
          308,
          1803,
          1425,
          317,
          372,
          382,
          983,
          1500,
          991,
          291,
          849,
          1539,
          293,
          1842,
          978,
          950,
          1829,
          1548,
          303,
          1463,
          1600,
          371,
          813,
          2249,
          1535,
          341,
          316,
          310,
          786,
          1834,
          1406,
          1478,
          743,
          2182,
          1371,
          2266,
          380,
          293,
          1695,
          308,
          1591,
          341,
          317,
          998,
          363,
          1460,
          347,
          2029,
          358,
          1387,
          363,
          297,
          360,
          326,
          761,
          290,
          306,
          296,
          350,
          1848,
          1881,
          814,
          298,
          740,
          300,
          1016,
          298,
          346,
          970,
          2011,
          1498,
          347,
          318,
          284,
          1404,
          1336,
          2607,
          337,
          334,
          1663,
          365,
          311,
          357,
          1895,
          676,
          1254,
          1922,
          348,
          351,
          1791,
          379,
          946,
          988,
          1878,
          1377,
          1526,
          1331,
          278,
          309,
          307,
          771,
          1036,
          1507,
          1820,
          311,
          1735,
          357,
          299,
          1796,
          343,
          428,
          366,
          1700,
          365,
          1110,
          305,
          564,
          1580,
          714,
          1404,
          1341,
          1116,
          868,
          1490,
          363,
          739,
          295,
          786,
          1166,
          1727,
          286,
          854,
          351,
          372,
          368,
          1698,
          326,
          1334,
          635,
          421,
          936,
          1485,
          1202,
          324,
          1992,
          562,
          325,
          2094,
          591,
          320,
          350,
          330,
          354,
          1582,
          336,
          262,
          1681,
          1290,
          1718,
          1926,
          990,
          950,
          307,
          364,
          2080,
          329,
          388,
          347,
          809,
          333,
          331,
          1360,
          330,
          312,
          347,
          745,
          958,
          336,
          360,
          313,
          319,
          321,
          1440,
          355,
          377,
          1380,
          313,
          1322,
          1013,
          319,
          872,
          452,
          337,
          285,
          531,
          1427,
          1148,
          304,
          375,
          331,
          326,
          2065,
          1109,
          745,
          287,
          302,
          1571,
          1223,
          1632,
          1066,
          681,
          311,
          296,
          335,
          341,
          1733,
          334,
          340,
          1030,
          851,
          1806,
          800,
          294,
          315,
          1755,
          1391,
          300,
          1642,
          318,
          1060,
          2780,
          331,
          514,
          313,
          1934,
          318,
          328,
          894,
          323,
          1115,
          2267,
          1168,
          1698,
          497,
          1064,
          306,
          365,
          348,
          359,
          815,
          359,
          326,
          1565,
          374,
          1412,
          878,
          858,
          301,
          1027,
          1732,
          371,
          321,
          1810,
          426,
          1057,
          1496,
          1566,
          1541,
          686,
          315,
          331,
          312,
          314,
          330,
          968,
          397,
          1600,
          1582,
          2109,
          351,
          1538,
          935,
          351,
          1410,
          1660,
          339,
          311,
          373,
          772,
          365,
          311,
          302,
          321,
          349,
          1218,
          305,
          1621,
          307,
          289,
          349,
          440,
          1535,
          311,
          334,
          1332,
          323,
          1855,
          317,
          629,
          329,
          320,
          1315,
          345,
          817,
          2028,
          346,
          315,
          1561,
          891,
          1059,
          1514,
          366,
          1877,
          849,
          880,
          308,
          284,
          307,
          309,
          1120,
          312,
          1343,
          301,
          1880,
          409,
          408,
          335,
          1759,
          317,
          363,
          241,
          294,
          1481,
          941,
          1399,
          1355,
          1858,
          308,
          1013,
          379,
          339,
          504,
          384,
          1804,
          649,
          354,
          397,
          1725,
          1826,
          460,
          272,
          1172,
          1944,
          1226,
          330,
          312,
          355,
          1293,
          1523,
          846,
          1132,
          1618,
          296,
          382,
          322,
          2283,
          1012,
          312,
          1766,
          293,
          369,
          1331,
          332,
          300,
          347,
          1824,
          303,
          1631,
          326,
          318,
          350,
          376,
          2132,
          379,
          322,
          346,
          320,
          1071,
          1682,
          307,
          319,
          1058,
          317,
          367,
          360,
          1028,
          386,
          316,
          335,
          1329,
          1049,
          327,
          2024,
          313,
          314,
          746,
          1329,
          1176,
          1911,
          1162,
          1211,
          376,
          1197,
          292,
          380,
          1844,
          353,
          1527,
          1845,
          300,
          300,
          1582,
          310,
          1230,
          338,
          315,
          362,
          299,
          1171,
          1500,
          1095,
          346,
          330,
          1732,
          349,
          1397,
          1051,
          731,
          371,
          1792,
          1938,
          305,
          340,
          315,
          380,
          401,
          1107,
          1104,
          315,
          314,
          1245,
          297,
          1663,
          1537,
          1718,
          349,
          1102,
          1569,
          1056,
          1790,
          699,
          1099,
          349,
          1485,
          1084,
          1588,
          1831,
          1961,
          321,
          1756,
          1762,
          1974,
          1532,
          564,
          1910,
          291,
          801,
          1729,
          1636,
          285,
          379,
          1151,
          1835,
          308,
          1729,
          1474,
          322,
          1862,
          627,
          298,
          344,
          316,
          1181,
          350,
          334,
          1970,
          928,
          1456,
          329,
          2007,
          1852,
          371,
          781,
          287,
          317,
          348,
          365,
          286,
          1276,
          336,
          1834,
          1597,
          922,
          1843,
          311,
          314,
          1570,
          1102,
          1188,
          313,
          342,
          317,
          1250,
          366,
          997,
          1996,
          364,
          1458,
          1143,
          336,
          316,
          314,
          1553,
          322,
          2009,
          2230,
          1593,
          899,
          1229,
          332,
          380,
          307,
          2477,
          1652,
          1657,
          455,
          1650,
          1931,
          2057,
          280,
          276,
          725,
          1513,
          341,
          1205,
          1026,
          310,
          320,
          336,
          2716,
          344,
          313,
          780,
          293,
          358,
          370,
          331,
          328,
          289,
          1019,
          388,
          303,
          1670,
          1721,
          2085,
          303,
          1070,
          1155,
          341,
          1841,
          974,
          1041,
          298,
          327,
          1463,
          306,
          1508,
          1438,
          764,
          404,
          1659,
          1498,
          1149,
          306,
          1820,
          285,
          338,
          300,
          901,
          354,
          316,
          2166,
          1912,
          318,
          734,
          308,
          1341,
          352,
          324,
          700,
          1212,
          1349,
          350,
          1145,
          315,
          309,
          317,
          1597,
          724,
          331,
          327,
          338,
          2115,
          314,
          309,
          1562,
          294,
          615,
          1609,
          939,
          1542,
          1040,
          300,
          362,
          2439,
          326,
          281,
          335,
          297,
          302,
          337,
          304,
          499,
          347,
          335,
          361,
          362,
          308,
          394,
          346,
          1936,
          290,
          313,
          315,
          334,
          304,
          301,
          326,
          1695,
          352,
          1779,
          359,
          376,
          338,
          348,
          373,
          343,
          298,
          335,
          310,
          316,
          488,
          1099,
          340,
          309,
          294,
          307,
          320,
          324,
          2195,
          1178,
          308,
          328,
          1401,
          316,
          334,
          298,
          906,
          303,
          1305,
          339,
          1633,
          300,
          1631,
          310,
          1819,
          358,
          1221,
          308,
          314,
          338,
          655,
          358,
          373,
          1631,
          304,
          980,
          339,
          346,
          334,
          365,
          292,
          376,
          581,
          315,
          303,
          1030,
          283,
          350,
          2363,
          349,
          1677,
          350,
          280,
          369,
          2098,
          1731,
          1930,
          1470,
          305,
          1723,
          719,
          321,
          809,
          818,
          373,
          1078,
          335,
          1874,
          1898,
          349,
          1505,
          1148,
          307,
          336,
          314,
          2015,
          2109,
          289,
          378,
          1400,
          1365,
          376,
          301,
          298,
          323,
          937,
          1313,
          326,
          304,
          1990,
          337,
          319,
          362,
          299,
          1833,
          322,
          382,
          355,
          315,
          1439,
          1512,
          332,
          745,
          328,
          347,
          362,
          310,
          371,
          367,
          1678,
          1589,
          320,
          1482,
          2531,
          1765,
          1374,
          389,
          315,
          1120,
          1136,
          348,
          294,
          941,
          367,
          308,
          1549,
          1384,
          344,
          358,
          336,
          324,
          361,
          664,
          795,
          323,
          306,
          554,
          304,
          1178,
          362,
          1440,
          285,
          364,
          301,
          1220,
          291,
          341,
          1465,
          1195,
          292,
          390,
          289,
          308,
          307,
          316,
          359,
          1962,
          337,
          311,
          342,
          335,
          1776,
          1583,
          322,
          906,
          1383,
          471,
          1492,
          300,
          515,
          1467,
          341,
          358,
          2091,
          347,
          273,
          326,
          348,
          639,
          1593,
          1235,
          301,
          412,
          321,
          1575,
          795,
          305,
          370,
          958,
          1296,
          329,
          1815,
          1609,
          1749,
          315,
          359,
          309,
          338,
          1923,
          342,
          318,
          307,
          1405,
          295,
          1573,
          311,
          364,
          479,
          343,
          2529,
          919,
          1090,
          1178,
          1323,
          1660,
          308,
          1145,
          2846,
          334,
          669,
          302,
          354,
          311,
          1362,
          302,
          297,
          319,
          327,
          301,
          651,
          326,
          323,
          1079,
          396,
          301,
          351,
          1202,
          1433,
          339,
          323,
          315,
          1826,
          477,
          1570,
          336,
          348,
          302,
          541,
          345,
          297,
          1093,
          306,
          386,
          316,
          390,
          345,
          915,
          329,
          307,
          1776,
          334,
          408,
          318,
          1883,
          1352,
          350,
          317,
          1080,
          1791,
          1578,
          320,
          726,
          325,
          1609,
          370,
          309,
          354,
          294,
          300,
          299,
          352,
          436,
          318,
          299,
          614,
          568,
          404,
          773,
          338,
          1592,
          308,
          1851,
          1553,
          1938,
          358,
          326,
          305,
          391,
          296,
          1206,
          1981,
          1848,
          547,
          309,
          361,
          378,
          1193,
          1506,
          358,
          1116,
          323,
          307,
          285,
          794,
          1245,
          336,
          1818,
          355,
          387,
          1563,
          902,
          2027,
          338,
          1235,
          331,
          996,
          310,
          1505,
          326,
          1487,
          1354,
          315,
          2396,
          356,
          320,
          299,
          1039,
          339,
          1667,
          360,
          356,
          858,
          322,
          1416,
          360,
          1067,
          348,
          1915,
          325,
          2215,
          322,
          330,
          546,
          339,
          657,
          320,
          323,
          296,
          310,
          1459,
          347,
          1940,
          332,
          313,
          1655,
          351,
          308,
          902,
          301,
          323,
          318,
          902,
          1849,
          361,
          304,
          363,
          1312,
          350,
          343,
          630,
          289,
          933,
          377,
          306,
          303,
          1244,
          1130,
          1166,
          1144,
          313,
          340,
          1533,
          321,
          1104,
          1135,
          2629,
          1032,
          319,
          378,
          351,
          1145,
          299,
          677,
          351,
          327,
          308,
          335,
          320,
          352,
          307,
          369,
          341,
          746,
          293,
          304,
          1663,
          1162,
          792,
          349,
          338,
          1485,
          1333,
          284,
          334,
          335,
          1888,
          306,
          363,
          1442,
          1398,
          299,
          1781,
          313,
          328,
          2778,
          1405,
          341,
          339,
          1369,
          328,
          295,
          347,
          357,
          304,
          678,
          656,
          345,
          2908,
          339,
          1421,
          308,
          366,
          317,
          335,
          301,
          789,
          1572,
          1172,
          319,
          295,
          303,
          327,
          2083,
          359,
          357,
          1347,
          376,
          271,
          1387,
          294,
          753,
          1891,
          327,
          405,
          284,
          772,
          1756,
          960,
          325,
          1178,
          360,
          329,
          2355,
          304,
          613,
          362,
          1926,
          316,
          1838,
          674,
          307,
          1363,
          294,
          1491,
          276,
          399,
          1265,
          1804,
          738,
          331,
          323,
          2705,
          320,
          1503,
          307,
          291,
          308,
          823,
          557,
          307,
          303,
          1494,
          800,
          363,
          366,
          311,
          364,
          363,
          368,
          299,
          835,
          303,
          1541,
          367,
          352,
          300,
          316,
          342,
          372,
          330,
          1115,
          1799,
          371,
          2049,
          338,
          1515,
          1557,
          1303,
          303,
          703,
          312,
          1413,
          1082,
          1497,
          329,
          336,
          1156,
          340,
          385,
          303,
          278,
          350,
          329,
          322,
          362,
          311,
          1995,
          389,
          302,
          1899,
          313,
          369,
          770,
          1355,
          1641,
          390,
          950,
          352,
          2408,
          320,
          321,
          344,
          360,
          1944,
          1495,
          293,
          346,
          306,
          402,
          1117,
          350,
          1306,
          1260,
          326,
          1147,
          1043,
          1859,
          620,
          962,
          308,
          298,
          353,
          735,
          1469,
          307,
          301,
          271,
          696,
          302,
          1986,
          1141,
          304,
          364,
          1714,
          1297,
          1152,
          355,
          328,
          2228,
          1888,
          314,
          324,
          385,
          1980,
          402,
          296,
          312,
          336,
          2195,
          1529,
          1285,
          1761,
          300,
          355,
          341,
          813,
          319,
          353,
          312,
          735,
          1736,
          1673,
          307,
          372,
          1008,
          294,
          362,
          1134,
          291,
          307,
          350,
          2249,
          290,
          308,
          297,
          290,
          3911,
          1478,
          358,
          317,
          327,
          392,
          1695,
          286,
          323,
          2029,
          363,
          1897,
          290,
          628,
          284,
          326,
          306,
          1881,
          1498,
          295,
          319,
          291,
          1699,
          562,
          1791,
          379,
          348,
          1878,
          362,
          1262,
          2206,
          322,
          1036,
          689,
          1785,
          289,
          308,
          2139,
          564,
          326,
          346,
          714,
          355,
          868,
          1380,
          314,
          332,
          298,
          1727,
          308,
          318,
          1082,
          340,
          1568,
          368,
          1334,
          1372,
          1212,
          309,
          333,
          322,
          1202,
          1511,
          1992,
          562,
          302,
          2094,
          306,
          343,
          1085,
          1311,
          321,
          354,
          307,
          1274,
          1738,
          1169,
          319,
          709,
          881,
          282,
          306,
          330,
          745,
          321,
          329,
          333,
          377,
          312,
          1650,
          337,
          289,
          3047,
          807,
          1427,
          1884,
          571,
          2061,
          766,
          367,
          296,
          311,
          999,
          851,
          325,
          306,
          1755,
          2780,
          323,
          872,
          305,
          352,
          316,
          582,
          326,
          330,
          315,
          348,
          341,
          375,
          317,
          339,
          350,
          330,
          1732,
          333,
          321,
          347,
          304,
          328,
          527,
          300,
          1510,
          299,
          356,
          371,
          320,
          302,
          1352,
          765,
          301,
          304,
          282,
          2024,
          361,
          985,
          355,
          299,
          323,
          1845,
          382,
          324,
          1559,
          323,
          326,
          624,
          338,
          320,
          364,
          376,
          387,
          318,
          1324,
          312,
          330,
          1537,
          382,
          318,
          336,
          1806,
          362,
          932,
          328,
          370,
          311,
          297,
          335,
          347,
          1477,
          879,
          1013,
          347,
          361,
          301,
          649,
          344,
          370,
          272,
          1172,
          1592,
          312,
          2019,
          1523,
          354,
          1012,
          312,
          345,
          335,
          322,
          311,
          287,
          322,
          316,
          318,
          1793,
          293,
          336,
          1165,
          1773,
          2132,
          1604,
          322,
          1039,
          1071,
          322,
          342,
          327,
          352,
          306,
          289,
          327,
          324,
          366,
          746,
          289,
          1946,
          341,
          1176,
          316,
          1598,
          332,
          366,
          554,
          306,
          380,
          335,
          319,
          314,
          1502,
          648,
          300,
          703,
          337,
          344,
          371,
          347,
          654,
          356,
          1128,
          1470,
          319,
          1107,
          1104,
          332,
          1625,
          283,
          298,
          297,
          1569,
          1099,
          317,
          382,
          1524,
          326,
          1961,
          317,
          1974,
          313,
          371,
          306,
          1558,
          307,
          581,
          379,
          1835,
          1729,
          357,
          310,
          1852,
          301,
          833,
          354,
          344,
          1978,
          1456,
          377,
          903,
          334,
          1765,
          365,
          1061,
          397,
          354,
          315,
          1843,
          374,
          317,
          1250,
          315,
          342,
          345,
          324,
          364,
          1033,
          388,
          328,
          1781,
          382,
          1317,
          2009,
          2230,
          304,
          326,
          351,
          899,
          296,
          308,
          1388,
          351,
          370,
          347,
          351,
          309,
          937,
          720,
          2057,
          312,
          383,
          415,
          310,
          320,
          353,
          944,
          287,
          359,
          314,
          333,
          341,
          337,
          322,
          1960,
          256,
          289,
          842,
          302,
          314,
          1774,
          864,
          333,
          310,
          729,
          343,
          634,
          1721,
          1070,
          306,
          322,
          291,
          1659,
          337,
          425,
          394,
          1495,
          377,
          376,
          1933,
          354,
          352,
          344,
          2229,
          297,
          331,
          352,
          350,
          336,
          334,
          1747,
          1764,
          347,
          308,
          2320,
          297,
          1123,
          309,
          360,
          352,
          552,
          1145,
          337,
          651,
          353,
          356,
          318,
          380,
          333,
          1084,
          1597,
          328,
          317,
          1377,
          738,
          669,
          330,
          360,
          349,
          1542,
          322,
          300,
          350,
          392,
          322,
          297,
          313,
          316,
          2227,
          1686,
          335,
          284,
          1498,
          354,
          1731,
          318,
          366,
          344,
          1695,
          354,
          334,
          304,
          329,
          336,
          397,
          307,
          352,
          341,
          761,
          373,
          407,
          1030,
          343,
          321,
          1140,
          404,
          301,
          1373,
          326,
          339,
          343,
          348,
          324,
          338,
          331,
          3140,
          1178,
          343,
          1401,
          308,
          334,
          1544,
          347,
          453,
          339,
          376,
          1983,
          1819,
          333,
          1221,
          1594,
          1409,
          321,
          1237,
          295,
          910,
          345,
          343,
          366,
          294,
          1582,
          320,
          1585,
          980,
          330,
          1131,
          421,
          348,
          310,
          308,
          315,
          371,
          341,
          1436,
          306,
          294,
          1455,
          377,
          1677,
          1291,
          451,
          349,
          359,
          2254,
          382,
          300,
          2098,
          1731,
          1763,
          362,
          333,
          1993,
          373,
          353,
          323,
          312,
          1485,
          337,
          1898,
          298,
          1640,
          349,
          328,
          329,
          314,
          303,
          314,
          1137,
          304,
          1478,
          296,
          1478,
          301,
          321,
          366,
          342,
          758,
          785,
          337,
          442,
          319,
          334,
          328,
          1890,
          306,
          322,
          338,
          636,
          363,
          302,
          1673,
          333,
          310,
          1491,
          1626,
          356,
          1589,
          356,
          320,
          1482,
          287,
          1374,
          1994,
          348,
          367,
          941,
          1296,
          344,
          1549,
          375,
          1571,
          348,
          330,
          327,
          664,
          795,
          312,
          631,
          358,
          1866,
          1767,
          293,
          285,
          764,
          2082,
          307,
          1220,
          324,
          306,
          341,
          1465,
          1195,
          308,
          307,
          355,
          316,
          342,
          2001,
          2132,
          308,
          312,
          1354,
          331,
          434,
          1179,
          2895,
          340,
          315,
          877,
          308,
          316,
          355,
          353,
          1383,
          340,
          418,
          367,
          360,
          308,
          372,
          334,
          2183,
          450,
          1277,
          304,
          344,
          306,
          358,
          2091,
          397,
          1955,
          352,
          376,
          318,
          338,
          334,
          321,
          327,
          1663,
          412,
          344,
          366,
          352,
          323,
          1385,
          314,
          381,
          2005,
          1424,
          331,
          306,
          322,
          1749,
          377,
          309,
          376,
          2019,
          342,
          1750,
          321,
          315,
          772,
          318,
          2036,
          293,
          331,
          1573,
          348,
          804,
          310,
          2529,
          323,
          304,
          370,
          1546,
          368,
          309,
          333,
          331,
          336,
          359,
          309,
          314,
          329,
          334,
          337,
          1779,
          383,
          786,
          404,
          346,
          319,
          320,
          790,
          1583,
          1660,
          354,
          1140,
          369,
          326,
          293,
          301,
          315,
          364,
          399,
          304,
          556,
          476,
          1479,
          324,
          671,
          436,
          1208,
          322,
          344,
          354,
          811,
          356,
          341,
          334,
          346,
          328,
          337,
          387,
          336,
          318,
          1127,
          391,
          345,
          361,
          915,
          388,
          371,
          1776,
          2187,
          385,
          334,
          350,
          358,
          1883,
          302,
          297,
          343,
          350,
          309,
          313,
          317,
          1671,
          321,
          340,
          877,
          322,
          354,
          294,
          381,
          298,
          315,
          319,
          303,
          906,
          299,
          1627,
          321,
          311,
          361,
          330,
          366,
          1531,
          338,
          353,
          362,
          367,
          297,
          1281,
          445,
          357,
          1518,
          1602,
          338,
          353,
          277,
          970,
          390,
          337,
          311,
          1781,
          409,
          3543,
          357,
          1220,
          391,
          359,
          316,
          1099,
          1327,
          352,
          378,
          328,
          1193,
          305,
          340,
          496,
          372,
          323,
          1506,
          358,
          287,
          413,
          323,
          2692,
          1878,
          794,
          825,
          510,
          1818,
          429,
          309,
          355,
          1942,
          389,
          346,
          317,
          354,
          314,
          321,
          341,
          321,
          437,
          1086,
          307,
          331,
          328,
          1450,
          311,
          336,
          359,
          315,
          2396,
          331,
          1430,
          356,
          320,
          278,
          305,
          384,
          316,
          310,
          348,
          366,
          332,
          1919,
          296,
          347,
          322,
          348,
          1965,
          330,
          339,
          1437,
          327,
          319,
          1288,
          322,
          378,
          330,
          777,
          357,
          339,
          362,
          331,
          347,
          374,
          296,
          306,
          332,
          280,
          340,
          323,
          321,
          375,
          902,
          319,
          346,
          350,
          290,
          373,
          321,
          1299,
          340,
          329,
          1533,
          348,
          799,
          350,
          304,
          370,
          1329,
          1441,
          1135,
          296,
          353,
          378,
          351,
          329,
          1529,
          758,
          384,
          304,
          2001,
          321,
          295,
          1873,
          1139,
          307,
          324,
          336,
          405,
          667,
          438,
          369,
          351,
          329,
          315,
          300,
          1947,
          832,
          323,
          1094,
          349,
          325,
          390,
          2127,
          1963,
          312,
          350,
          345,
          345,
          314,
          327,
          295,
          284,
          334,
          335,
          328,
          335,
          363,
          341,
          348,
          382,
          455,
          297,
          929,
          531,
          282,
          295,
          359,
          397,
          2076,
          328,
          1405,
          323,
          304,
          1465,
          352,
          1369,
          1954,
          627,
          295,
          286,
          1901,
          347,
          311,
          376,
          347,
          600,
          326,
          343,
          299,
          1600,
          342,
          1821,
          366,
          1322,
          2141,
          329,
          314,
          348,
          550,
          380,
          354,
          299,
          321,
          344,
          1572,
          326,
          1172,
          415,
          340,
          315,
          420,
          1730,
          1019,
          321,
          809,
          316,
          376,
          1387,
          293,
          906,
          291,
          317,
          1891,
          348,
          304,
          356,
          325,
          377,
          2050,
          332,
          1756,
          960,
          308,
          353,
          357,
          371,
          304,
          326,
          1642,
          1061,
          367,
          291,
          1784,
          307,
          324,
          348,
          298,
          1926,
          326,
          365,
          378,
          320,
          334,
          297,
          400,
          870,
          459,
          382,
          2705,
          330,
          315,
          339,
          325,
          337,
          327,
          1028,
          366,
          356,
          364,
          836,
          557,
          307,
          366,
          303,
          300,
          363,
          363,
          345,
          1381,
          518,
          343,
          363,
          303,
          1815,
          352,
          1796,
          526,
          313,
          1370,
          812,
          332,
          369,
          1515,
          382,
          312,
          1179,
          329,
          399,
          334,
          1690,
          319,
          302,
          1483,
          320,
          363,
          289,
          1689,
          310,
          308,
          389,
          353,
          340,
          2021,
          342,
          320,
          332,
          950,
          376,
          1579,
          307,
          324,
          328,
          320,
          327,
          344,
          344,
          1169,
          1635,
          1741,
          308,
          866,
          341,
          342,
          2983,
          324,
          315,
          320,
          293,
          315,
          346,
          2905,
          342,
          284,
          333,
          411,
          326,
          345,
          1527,
          338,
          345,
          327,
          2611,
          352,
          331,
          1595,
          3288,
          360,
          667,
          317,
          321,
          304,
          318,
          311,
          2032,
          314,
          348,
          317,
          382,
          376,
          1507,
          1554,
          1998,
          317,
          374,
          316,
          302,
          574,
          330,
          1141,
          304,
          378,
          320,
          299,
          340,
          1192,
          364,
          1480,
          1735,
          321,
          328,
          309,
          1287,
          1495,
          284,
          1959,
          2228,
          384,
          322,
          876,
          928,
          322,
          1888,
          317,
          314,
          324,
          324,
          1535,
          2246,
          296,
          454,
          1618,
          1201,
          378,
          328,
          346,
          2195,
          1519,
          318,
          322,
          366,
          300,
          417,
          355,
          307,
          365,
          2483,
          374,
          328,
          319,
          316,
          308,
          1736,
          380,
          1016,
          1673,
          372,
          1008,
          362,
          372,
          1460,
          321,
          1500,
          330,
          291,
          335,
          387,
          954,
          950,
          340,
          1829,
          1600,
          813,
          301,
          2070,
          316,
          755,
          293,
          351,
          285,
          317,
          2182,
          1371,
          293,
          327,
          317,
          998,
          286,
          1425,
          347,
          288,
          1387,
          376,
          2080,
          1897,
          326,
          314,
          314,
          306,
          296,
          350,
          306,
          1881,
          814,
          342,
          298,
          318,
          456,
          347,
          324,
          1562,
          1404,
          334,
          298,
          314,
          701,
          315,
          320,
          348,
          1878,
          332,
          395,
          1331,
          844,
          307,
          333,
          1507,
          284,
          311,
          286,
          313,
          1735,
          289,
          357,
          343,
          1700,
          363,
          319,
          329,
          312,
          363,
          346,
          356,
          1404,
          363,
          352,
          1490,
          363,
          330,
          333,
          1727,
          342,
          381,
          1698,
          902,
          322,
          318,
          323,
          591,
          1633,
          297,
          337,
          307,
          1738,
          303,
          307,
          1880,
          342,
          1912,
          298,
          445,
          341,
          382,
          312,
          308,
          745,
          360,
          390,
          1082,
          319,
          375,
          355,
          297,
          1013,
          347,
          3047,
          807,
          375,
          355,
          310,
          1223,
          766,
          311,
          338,
          368,
          702,
          294,
          344,
          344,
          327,
          364,
          344,
          346,
          346,
          284,
          2780,
          312,
          872,
          308,
          1934,
          1743,
          352,
          328,
          321,
          332,
          323,
          335,
          1115,
          521,
          2267,
          388,
          1068,
          341,
          361,
          359,
          338,
          351,
          374,
          333,
          323,
          421,
          304,
          686,
          312,
          333,
          310,
          1893,
          2109,
          315,
          333,
          1538,
          363,
          367,
          311,
          356,
          365,
          320,
          311,
          1352,
          1621,
          2024,
          349,
          328,
          985,
          381,
          334,
          312,
          375,
          320,
          365,
          345,
          817,
          1621,
          2028,
          315,
          317,
          354,
          327,
          1738,
          1877,
          376,
          302,
          318,
          308,
          662,
          373,
          336,
          1537,
          380,
          636,
          409,
          362,
          287,
          317,
          241,
          347,
          1241,
          1477,
          1858,
          308,
          379,
          354,
          1323,
          416,
          301,
          397,
          344,
          386,
          324,
          1471,
          360,
          1944,
          322,
          322,
          327,
          371,
          1824,
          378,
          1610,
          316,
          1793,
          1799,
          1165,
          379,
          1039,
          1433,
          462,
          363,
          416,
          617,
          305,
          1058,
          327,
          324,
          2024,
          366,
          289,
          1946,
          312,
          1601,
          335,
          1176,
          306,
          300,
          316,
          355,
          1911,
          328,
          330,
          351,
          319,
          314,
          1502,
          374,
          329,
          315,
          362,
          300,
          346,
          1732,
          349,
          344,
          731,
          355,
          301,
          356,
          303,
          315,
          1723,
          303,
          1470,
          346,
          985,
          1924,
          350,
          1696,
          285,
          330,
          1718,
          316,
          1102,
          321,
          1569,
          349,
          322,
          1099,
          317,
          335,
          313,
          352,
          581,
          308,
          390,
          310,
          286,
          833,
          354,
          316,
          336,
          350,
          334,
          1852,
          377,
          1925,
          307,
          781,
          1765,
          286,
          377,
          449,
          2394,
          1102,
          438,
          632,
          366,
          335,
          345,
          1596,
          324,
          314,
          330,
          328,
          328,
          314,
          408,
          462,
          322,
          328,
          2230,
          1482,
          326,
          300,
          318,
          2026,
          283,
          457,
          454,
          2057,
          276,
          342,
          366,
          1558,
          1896,
          1110,
          319,
          378,
          341,
          1019,
          347,
          1807,
          348,
          362,
          1670,
          327,
          1463,
          507,
          1508,
          1438,
          1659,
          313,
          870,
          376,
          1052,
          338,
          1625,
          336,
          1574,
          321,
          1572,
          341,
          2320,
          1312,
          289,
          1145,
          337,
          352,
          1139,
          1585,
          366,
          396,
          328,
          1597,
          331,
          314,
          1012,
          1335,
          321,
          1848,
          320,
          2439,
          284,
          322,
          313,
          313,
          2110,
          1338,
          373,
          326,
          1235,
          1652,
          360,
          348,
          364,
          321,
          325,
          337,
          1702,
          989,
          291,
          310,
          290,
          316,
          309,
          313,
          333,
          396,
          961,
          310,
          345,
          338,
          1804,
          310,
          390,
          1131,
          346,
          301,
          1090,
          310,
          349,
          1680,
          382,
          1930,
          374,
          1695,
          2143,
          395,
          809,
          1510,
          337,
          1445,
          358,
          303,
          304,
          1237,
          296,
          1400,
          298,
          341,
          329,
          323,
          309,
          1730,
          1833,
          1463,
          1890,
          355,
          1582,
          636,
          290,
          1126,
          347,
          1447,
          1906,
          318,
          315,
          695,
          1547,
          1537,
          312,
          1589,
          323,
          348,
          314,
          306,
          1707,
          358,
          1962,
          1440,
          1782,
          323,
          289,
          1421,
          301,
          304,
          1184,
          316,
          324,
          311,
          308,
          879,
          355,
          2002,
          340,
          326,
          1955,
          1805,
          1021,
          349,
          1895,
          1575,
          366,
          1314,
          370,
          1296,
          318,
          1614,
          2019,
          1241,
          342,
          1032,
          331,
          376,
          314,
          329,
          1180,
          1362,
          304,
          342,
          1675,
          325,
          1079,
          315,
          1567,
          873,
          359,
          338,
          353,
          336,
          302,
          987,
          336,
          345,
          1093,
          318,
          344,
          306,
          806,
          408,
          348,
          388,
          1245,
          708,
          379,
          1158,
          320,
          1351,
          1057,
          436,
          1627,
          1160,
          358,
          338,
          326,
          297,
          345,
          355,
          363,
          390,
          1293,
          1422,
          1220,
          337,
          361,
          285,
          316,
          1818,
          389,
          354,
          354,
          367,
          1757,
          1505,
          328,
          1178,
          1002,
          1574,
          339,
          321,
          1268,
          540,
          2385,
          296,
          1756,
          1067,
          1523,
          1075,
          319,
          2215,
          1286,
          341,
          374,
          327,
          1940,
          313,
          317,
          1824,
          325,
          346,
          1849,
          394,
          1331,
          1779,
          2076,
          1166,
          1500,
          975,
          1609,
          304,
          758,
          312,
          2653,
          1356,
          1052,
          351,
          307,
          308,
          347,
          317,
          335,
          320,
          1413,
          1389,
          883,
          304,
          379,
          329,
          318,
          337,
          334,
          2162,
          325,
          1315,
          1525,
          2141,
          1651,
          313,
          920,
          2076,
          2297,
          1102,
          1096,
          718,
          936,
          1072,
          313,
          1794,
          1218,
          299,
          345,
          2908,
          1244,
          310,
          1437,
          1172,
          748,
          415,
          326,
          295,
          1534,
          1569,
          809,
          1137,
          1070,
          328,
          1207,
          753,
          304,
          1648,
          960,
          329,
          2355,
          297,
          674,
          336,
          397,
          1640,
          307,
          305,
          299,
          318,
          1265,
          738,
          2707,
          330,
          326,
          365,
          1503,
          1028,
          993,
          557,
          366,
          1124,
          1820,
          1591,
          338,
          835,
          1342,
          1383,
          326,
          814,
          914,
          350,
          382,
          1513,
          1674,
          336,
          1690,
          376,
          1630,
          311,
          1899,
          770,
          376,
          309,
          1592,
          1635,
          1515,
          360,
          336,
          2983,
          315,
          324,
          2090,
          2905,
          402,
          316,
          1260,
          1510,
          327,
          296,
          1245,
          962,
          335,
          1687,
          1546,
          1754,
          382,
          312,
          1443,
          378,
          660,
          321,
          1416,
          355,
          1201,
          373,
          320,
          1535,
          327,
          336,
          1529,
          1285,
          1761,
          334,
          1128,
          813,
          353,
          662,
          1736,
          326,
          1425,
          1134,
          1460,
          849,
          335,
          341,
          290,
          1230,
          1425,
          323,
          298,
          345,
          288,
          301,
          761,
          648,
          1016,
          1551,
          2607,
          1922,
          1372,
          351,
          1791,
          946,
          1878,
          345,
          1526,
          1820,
          1608,
          329,
          714,
          1380,
          1727,
          1820,
          1082,
          1202,
          562,
          1169,
          319,
          950,
          388,
          323,
          282,
          321,
          3286,
          297,
          313,
          1650,
          361,
          933,
          1750,
          963,
          999,
          300,
          323,
          1043,
          316,
          335,
          334,
          1565,
          1412,
          878,
          350,
          2125,
          1541,
          331,
          1888,
          328,
          1667,
          2109,
          1510,
          307,
          1352,
          1218,
          1621,
          1777,
          360,
          355,
          1332,
          324,
          1559,
          346,
          1514,
          1738,
          364,
          518,
          307,
          387,
          1324,
          318,
          297,
          920,
          1386,
          1477,
          1355,
          286,
          370,
          324,
          308,
          368,
          396,
          312,
          343,
          1592,
          2019,
          1523,
          354,
          382,
          569,
          1148,
          327,
          369,
          1617,
          864,
          359,
          332,
          1610,
          1064,
          336,
          340,
          327,
          346,
          746,
          341,
          300,
          280,
          330,
          376,
          352,
          300,
          731,
          303,
          1938,
          755,
          319,
          944,
          315,
          303,
          298,
          1102,
          826,
          1056,
          1790,
          1974,
          1532,
          801,
          670,
          352,
          1558,
          364,
          348,
          1151,
          1474,
          1852,
          2007,
          1852,
          298,
          1765,
          1166,
          1843,
          311,
          314,
          874,
          335,
          342,
          1607,
          1596,
          314,
          388,
          996,
          336,
          1108,
          1553,
          300,
          2230,
          1229,
          314,
          1264,
          351,
          1650,
          342
         ],
         "yaxis": "y4"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Distribuci√≥n Individual",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Matriz de Co-ocurrencia",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Combinaciones Principales",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.375,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Longitud de Texto por Categor√≠a",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.375,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 800,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "üìä An√°lisis Completo de Etiquetas M√©dicas"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.625,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.625,
          1
         ]
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          0.375
         ]
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          0.375
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà RESUMEN ESTAD√çSTICO:\n",
      "üè∑Ô∏è Total de etiquetas √∫nicas: 4\n",
      "üîó Total de combinaciones √∫nicas: 15\n",
      "üìñ Art√≠culos con m√∫ltiples etiquetas: 11\n",
      "üìù Promedio de etiquetas por art√≠culo: 1.33\n"
     ]
    }
   ],
   "source": [
    "# Visualizaci√≥n de distribuci√≥n de etiquetas\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Distribuci√≥n Individual', 'Matriz de Co-ocurrencia',\n",
    "                   'Combinaciones Principales', 'Longitud de Texto por Categor√≠a'),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"heatmap\"}],\n",
    "           [{\"type\": \"bar\"}, {\"type\": \"box\"}]]\n",
    ")\n",
    "\n",
    "# 1. Distribuci√≥n individual\n",
    "labels = list(analysis_results['label_counts'].keys())\n",
    "counts = list(analysis_results['label_counts'].values())\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=labels, y=counts, marker_color=colors, name=\"Etiquetas\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Matriz de co-ocurrencia\n",
    "cooc_matrix = analysis_results['cooccurrence_matrix']\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=cooc_matrix.values,\n",
    "        x=cooc_matrix.columns,\n",
    "        y=cooc_matrix.index,\n",
    "        colorscale='Blues',\n",
    "        name=\"Co-ocurrencia\"\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Top combinaciones\n",
    "top_combos = list(analysis_results['label_combinations'].items())[:8]\n",
    "combo_labels = [\" + \".join(combo[0]) if combo[0] else \"Sin etiquetas\" for combo in top_combos]\n",
    "combo_counts = [combo[1] for combo in top_combos]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=combo_counts, y=combo_labels, orientation='h',\n",
    "           marker_color='lightblue', name=\"Combinaciones\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Longitud de texto por categor√≠a\n",
    "text_lengths_by_category = []\n",
    "category_names = []\n",
    "\n",
    "for label in labels:\n",
    "    mask = df_final[f'target_{label}'] == 1\n",
    "    lengths = df_final[mask]['combined_text'].str.len()\n",
    "    text_lengths_by_category.extend(lengths.tolist())\n",
    "    category_names.extend([label] * len(lengths))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=text_lengths_by_category, x=category_names, name=\"Longitud\"),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"üìä An√°lisis Completo de Etiquetas M√©dicas\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Resumen estad√≠stico\n",
    "print(\"\\nüìà RESUMEN ESTAD√çSTICO:\")\n",
    "print(f\"üè∑Ô∏è Total de etiquetas √∫nicas: {len(labels)}\")\n",
    "print(f\"üîó Total de combinaciones √∫nicas: {len(analysis_results['label_combinations'])}\")\n",
    "print(f\"üìñ Art√≠culos con m√∫ltiples etiquetas: {sum(1 for combo in analysis_results['label_combinations'] if len(combo) > 1)}\")\n",
    "print(f\"üìù Promedio de etiquetas por art√≠culo: {sum(len(combo) * count for combo, count in analysis_results['label_combinations'].items()) / analysis_results['total_articles']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af6311",
   "metadata": {},
   "source": [
    "## 5. üß¨ BioBERT Model Implementation\n",
    "\n",
    "Implementaci√≥n del modelo BioBERT especializado en textos biom√©dicos para manejar el 90% de casos obvios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c410b2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Clase BioBERTClassifierEnhanced implementada con mejoras\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# üîß IMPLEMENTACI√ìN DE MEJORAS EN BIOBERTCLASSIFIER\n",
    "# ==============================================================================\n",
    "\n",
    "class BioBERTClassifierEnhanced:\n",
    "    \"\"\"\n",
    "    Clasificador BioBERT mejorado con diagn√≥sticos avanzados y c√°lculo de confianza robusto.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name='dmis-lab/biobert-base-cased-v1.1', max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.is_trained = False\n",
    "        self.label_names = None  # MEJORA: Cache para etiquetas del modelo\n",
    "\n",
    "    def load_model_from_local(self, model_path: str):\n",
    "        \"\"\"Carga un modelo BioBERT fine-tuned con validaciones mejoradas\"\"\"\n",
    "        print(f\"üìÇ Cargando modelo fine-tuned desde: {model_path}\")\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "            # MEJORA: Verificar configuraci√≥n del modelo\n",
    "            if hasattr(self.model.config, 'num_labels'):\n",
    "                print(f\"üìä Modelo configurado para {self.model.config.num_labels} etiquetas\")\n",
    "\n",
    "            # MEJORA: Extraer nombres de etiquetas si est√°n disponibles\n",
    "            if hasattr(self.model.config, 'id2label'):\n",
    "                self.label_names = [self.model.config.id2label[i]\n",
    "                                  for i in range(self.model.config.num_labels)]\n",
    "                print(f\"üè∑Ô∏è Etiquetas del modelo: {self.label_names}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No se encontraron etiquetas en la configuraci√≥n del modelo\")\n",
    "\n",
    "            self.is_trained = True\n",
    "            print(\"‚úÖ Modelo local cargado y listo para predicci√≥n.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando modelo local: {e}\")\n",
    "            # MEJORA: Logging m√°s detallado del error\n",
    "            import traceback\n",
    "            print(f\"üìù Detalle del error: {traceback.format_exc()}\")\n",
    "            raise\n",
    "\n",
    "    def validate_model_compatibility(self, expected_labels: list):\n",
    "        \"\"\"NUEVA: Valida que el modelo cargado sea compatible con las etiquetas esperadas\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Modelo no cargado\")\n",
    "\n",
    "        model_num_labels = self.model.config.num_labels\n",
    "        expected_num_labels = len(expected_labels)\n",
    "\n",
    "        if model_num_labels != expected_num_labels:\n",
    "            raise ValueError(\n",
    "                f\"‚ùå Incompatibilidad: Modelo tiene {model_num_labels} etiquetas, \"\n",
    "                f\"pero se esperan {expected_num_labels}\"\n",
    "            )\n",
    "\n",
    "        print(f\"‚úÖ Modelo compatible: {model_num_labels} etiquetas\")\n",
    "        return True\n",
    "\n",
    "    def tokenize_data(self, texts: list[str]) -> Dataset:\n",
    "        \"\"\"Tokeniza los textos para BioBERT\"\"\"\n",
    "        print(f\"üî§ Tokenizando {len(texts)} textos...\")\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            return self.tokenizer(\n",
    "                examples['text'],\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "        dataset = Dataset.from_dict({'text': texts})\n",
    "        tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "        print(\"‚úÖ Tokenizaci√≥n completada\")\n",
    "        return tokenized_dataset\n",
    "\n",
    "    def calculate_confidence_scores_robust(self, predictions: np.ndarray, method: str = 'difference') -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        MEJORA: C√°lculo de confianza m√°s robusto con m√∫ltiples m√©todos\n",
    "\n",
    "        Args:\n",
    "            predictions: Array de logits del modelo\n",
    "            method: 'difference', 'entropy', 'max_prob'\n",
    "\n",
    "        Returns:\n",
    "            confidence_scores, probabilities\n",
    "        \"\"\"\n",
    "        # Aplicar sigmoid para obtener probabilidades\n",
    "        probabilities = 1 / (1 + np.exp(-predictions))\n",
    "\n",
    "        if method == 'difference':\n",
    "            # Confianza basada en la diferencia entre las dos probabilidades m√°s altas\n",
    "            max_probs = np.max(probabilities, axis=1)\n",
    "            # Usar partition para obtener la segunda probabilidad m√°s alta\n",
    "            second_max_probs = np.partition(probabilities, -2, axis=1)[:, -2]\n",
    "            confidence_scores = max_probs - second_max_probs\n",
    "\n",
    "        elif method == 'entropy':\n",
    "            # Confianza basada en entrop√≠a normalizada\n",
    "            epsilon = 1e-8  # Para evitar log(0)\n",
    "            entropy = -np.sum(probabilities * np.log(probabilities + epsilon), axis=1)\n",
    "            max_entropy = np.log(probabilities.shape[1])  # Entrop√≠a m√°xima posible\n",
    "            confidence_scores = 1 - (entropy / max_entropy)\n",
    "\n",
    "        elif method == 'max_prob':\n",
    "            # M√©todo original: solo la probabilidad m√°xima\n",
    "            confidence_scores = np.max(probabilities, axis=1)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"M√©todo no reconocido: {method}\")\n",
    "\n",
    "        print(f\"üìä Confianza calculada usando m√©todo '{method}'\")\n",
    "        print(f\"   Confianza promedio: {np.mean(confidence_scores):.3f}\")\n",
    "        print(f\"   Confianza std: {np.std(confidence_scores):.3f}\")\n",
    "\n",
    "        return confidence_scores, probabilities\n",
    "\n",
    "    def predict_with_confidence_enhanced(self, texts: list[str], confidence_threshold: float = 0.7,\n",
    "                                       confidence_method: str = 'difference') -> dict:\n",
    "        \"\"\"\n",
    "        Realiza predicciones con scores de confianza mejorados.\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"‚ùå Modelo no entrenado. Ejecutar load_model_from_local() primero.\")\n",
    "\n",
    "        print(f\"üîÆ Realizando predicciones para {len(texts)} textos...\")\n",
    "        print(f\"   M√©todo de confianza: {confidence_method}\")\n",
    "        print(f\"   Umbral de confianza: {confidence_threshold}\")\n",
    "\n",
    "        # Tokenizar\n",
    "        tokenized_data = self.tokenize_data(texts)\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "        dataloader = DataLoader(tokenized_data, batch_size=16, collate_fn=data_collator)\n",
    "\n",
    "        self.model.to(device).eval()\n",
    "        all_predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                inputs = {k: v.to(device) for k, v in batch.items() if k in self.tokenizer.model_input_names}\n",
    "                outputs = self.model(**inputs)\n",
    "                predictions = outputs.logits.cpu().numpy()\n",
    "                all_predictions.append(predictions)\n",
    "\n",
    "        # Concatenar todas las predicciones\n",
    "        all_predictions = np.vstack(all_predictions)\n",
    "\n",
    "        # Calcular confianza con m√©todo mejorado\n",
    "        confidence_scores, probabilities = self.calculate_confidence_scores_robust(\n",
    "            all_predictions, method=confidence_method\n",
    "        )\n",
    "\n",
    "        # Separar casos obvios vs dif√≠ciles\n",
    "        obvious_mask = confidence_scores >= confidence_threshold\n",
    "        difficult_mask = ~obvious_mask\n",
    "\n",
    "        results = {\n",
    "            'obvious_cases': {\n",
    "                'indices': np.where(obvious_mask)[0],\n",
    "                'predictions': probabilities[obvious_mask],\n",
    "                'confidence_scores': confidence_scores[obvious_mask],\n",
    "                'texts': [texts[i] for i in np.where(obvious_mask)[0]]\n",
    "            },\n",
    "            'difficult_cases': {\n",
    "                'indices': np.where(difficult_mask)[0],\n",
    "                'texts': [texts[i] for i in np.where(difficult_mask)[0]],\n",
    "                'confidence_scores': confidence_scores[difficult_mask]\n",
    "            },\n",
    "            'all_predictions': probabilities,\n",
    "            'all_confidence': confidence_scores,\n",
    "            'confidence_method': confidence_method\n",
    "        }\n",
    "\n",
    "        print(f\"üìä Casos obvios (BioBERT): {len(results['obvious_cases']['indices'])} ({len(results['obvious_cases']['indices'])/len(texts)*100:.1f}%)\")\n",
    "        print(f\"ü§î Casos dif√≠ciles (LLM): {len(results['difficult_cases']['indices'])} ({len(results['difficult_cases']['indices'])/len(texts)*100:.1f}%)\")\n",
    "\n",
    "        return results\n",
    "\n",
    "print(\"üîß Clase BioBERTClassifierEnhanced implementada con mejoras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a139b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß¨ Cargando modelo BioBERT con clasificador mejorado...\n",
      "üìÇ Cargando modelo fine-tuned desde: model/biobert_finetuned_v3\n",
      "üìä Modelo configurado para 4 etiquetas\n",
      "üè∑Ô∏è Etiquetas del modelo: ['LABEL_0', 'LABEL_1', 'LABEL_2', 'LABEL_3']\n",
      "‚úÖ Modelo local cargado y listo para predicci√≥n.\n",
      "‚úÖ Modelo compatible: 4 etiquetas\n",
      "\n",
      "üéâ BioBERT mejorado cargado y validado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# üß¨ CARGA DEL MODELO BIOBERT CON MEJORAS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üß¨ Cargando modelo BioBERT con clasificador mejorado...\")\n",
    "\n",
    "# Crear instancia del clasificador mejorado\n",
    "biobert_enhanced = BioBERTClassifierEnhanced()\n",
    "\n",
    "# Cargar el modelo fine-tuned\n",
    "local_model_path = \"model/biobert_finetuned_v3\"\n",
    "\n",
    "try:\n",
    "    biobert_enhanced.load_model_from_local(local_model_path)\n",
    "\n",
    "    # Mover al dispositivo correcto\n",
    "    if biobert_enhanced.model:\n",
    "        biobert_enhanced.model.to(device)\n",
    "\n",
    "    # Validar compatibilidad con las etiquetas del dataset\n",
    "    biobert_enhanced.validate_model_compatibility(y_labels.columns.tolist())\n",
    "\n",
    "    print(\"\\nüéâ BioBERT mejorado cargado y validado exitosamente!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e321239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß CORRIGIENDO MAPEO DE ETIQUETAS M√âDICAS...\n",
      "============================================================\n",
      "üìä Orden real de etiquetas en el dataset:\n",
      "   y_labels.columns: ['cardiovascular', 'hepatorenal', 'neurological', 'oncological']\n",
      "\n",
      "üè∑Ô∏è Mapeo correcto:\n",
      "   LABEL_0 -> ‚ù§Ô∏è Cardiovascular cardiovascular\n",
      "   LABEL_2 -> üß† Neurol√≥gico neurological\n",
      "   LABEL_1 -> ü´ò Hepatorrenal hepatorenal\n",
      "   LABEL_3 -> üéóÔ∏è Oncol√≥gico oncological\n",
      "\n",
      "‚úÖ Modelo BioBERT actualizado:\n",
      "   Etiquetas corregidas: ['cardiovascular', 'hepatorenal', 'neurological', 'oncological']\n",
      "\n",
      "üéâ ¬°Mapeo de etiquetas corregido exitosamente!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# üîß CORRECCI√ìN DEL MAPEO DE ETIQUETAS M√âDICAS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üîß CORRIGIENDO MAPEO DE ETIQUETAS M√âDICAS...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# El modelo fue entrenado con etiquetas en orden alfab√©tico (as√≠ funciona MultiLabelBinarizer)\n",
    "# Verificar el orden correcto de las etiquetas del dataset original\n",
    "print(\"üìä Orden real de etiquetas en el dataset:\")\n",
    "print(f\"   y_labels.columns: {list(y_labels.columns)}\")\n",
    "\n",
    "# Crear mapeo correcto entre √≠ndices del modelo y etiquetas m√©dicas\n",
    "# El orden debe coincidir con el orden que se us√≥ durante el entrenamiento\n",
    "true_label_mapping = {\n",
    "    0: 'cardiovascular',    # LABEL_0 -> cardiovascular\n",
    "    2: 'neurological',      # LABEL_2 -> neurological\n",
    "    1: 'hepatorenal',       # LABEL_1 -> hepatorenal\n",
    "    3: 'oncological'        # LABEL_3 -> oncological\n",
    "}\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Mapeo correcto:\")\n",
    "for idx, real_label in true_label_mapping.items():\n",
    "    emoji = label_analyzer.label_mapping.get(real_label, 'üè∑Ô∏è')\n",
    "    print(f\"   LABEL_{idx} -> {emoji} {real_label}\")\n",
    "\n",
    "# ACTUALIZAR el modelo BioBERT enhanced con el mapeo correcto\n",
    "biobert_enhanced.label_names = [true_label_mapping[i] for i in range(4)]\n",
    "biobert_enhanced.model.config.id2label = true_label_mapping\n",
    "biobert_enhanced.model.config.label2id = {v: k for k, v in true_label_mapping.items()}\n",
    "\n",
    "print(\"\\n‚úÖ Modelo BioBERT actualizado:\")\n",
    "print(f\"   Etiquetas corregidas: {biobert_enhanced.label_names}\")\n",
    "\n",
    "print(\"\\nüéâ ¬°Mapeo de etiquetas corregido exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27df2ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ VERIFICANDO MAPEO CORREGIDO DE ETIQUETAS\n",
      "============================================================\n",
      "üìù Texto de prueba (cardiovascular):\n",
      "   T√≠tulo: Cardiac arrhythmia detection using deep learning approaches\n",
      "   Abstract: This study focuses on heart rhythm disorders, myocardial infarction detection, and cardiovascular ri...\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.3\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22f21e217dd493ca7cbd0c8bc7c50b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.942\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 1 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "\n",
      "üîÆ RESULTADOS DE PREDICCI√ìN:\n",
      "   Confianza general: 0.942\n",
      "\n",
      "üìä Probabilidades por dominio m√©dico:\n",
      "   ‚ù§Ô∏è Cardiovascular Cardiovascular: 0.991 ‚úÖ PREDICHA\n",
      "   ü´ò Hepatorrenal Hepatorenal: 0.014 ‚ùå\n",
      "   üß† Neurol√≥gico Neurological: 0.049 ‚ùå\n",
      "   üéóÔ∏è Oncol√≥gico Oncological: 0.018 ‚ùå\n",
      "\n",
      "üéØ DOMINIO M√ÅS PROBABLE:\n",
      "   ‚ù§Ô∏è Cardiovascular Cardiovascular: 0.991\n",
      "   ‚úÖ ¬°√âXITO! El modelo predice correctamente 'cardiovascular'\n",
      "   üéâ El mapeo de etiquetas est√° funcionando correctamente\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# üß™ VERIFICACI√ìN DEL MAPEO CORREGIDO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üß™ VERIFICANDO MAPEO CORREGIDO DE ETIQUETAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Texto de prueba claramente cardiovascular\n",
    "test_title = \"Cardiac arrhythmia detection using deep learning approaches\"\n",
    "test_abstract = \"This study focuses on heart rhythm disorders, myocardial infarction detection, and cardiovascular risk assessment using ECG signal analysis.\"\n",
    "test_text = f\"{test_title} [SEP] {test_abstract}\"\n",
    "\n",
    "print(\"üìù Texto de prueba (cardiovascular):\")\n",
    "print(f\"   T√≠tulo: {test_title}\")\n",
    "print(f\"   Abstract: {test_abstract[:100]}...\")\n",
    "\n",
    "# Predecir con el modelo corregido\n",
    "result = biobert_enhanced.predict_with_confidence_enhanced(\n",
    "    [test_text],\n",
    "    confidence_threshold=0.3,  # Umbral bajo para ver todo\n",
    "    confidence_method='difference'\n",
    ")\n",
    "\n",
    "probabilities = result['all_predictions'][0]\n",
    "confidence = result['all_confidence'][0]\n",
    "\n",
    "print(\"\\nüîÆ RESULTADOS DE PREDICCI√ìN:\")\n",
    "print(f\"   Confianza general: {confidence:.3f}\")\n",
    "\n",
    "print(\"\\nüìä Probabilidades por dominio m√©dico:\")\n",
    "for i, (label_name, prob) in enumerate(zip(biobert_enhanced.label_names, probabilities, strict=False)):\n",
    "    emoji = label_analyzer.label_mapping.get(label_name, 'üè∑Ô∏è')\n",
    "    status = \"‚úÖ PREDICHA\" if prob > 0.5 else \"‚ùå\"\n",
    "    print(f\"   {emoji} {label_name.capitalize()}: {prob:.3f} {status}\")\n",
    "\n",
    "# Verificar que cardiovascular tiene la probabilidad m√°s alta\n",
    "max_idx = np.argmax(probabilities)\n",
    "predicted_domain = biobert_enhanced.label_names[max_idx]\n",
    "max_prob = probabilities[max_idx]\n",
    "\n",
    "print(\"\\nüéØ DOMINIO M√ÅS PROBABLE:\")\n",
    "print(f\"   {label_analyzer.label_mapping.get(predicted_domain, 'üè∑Ô∏è')} {predicted_domain.capitalize()}: {max_prob:.3f}\")\n",
    "\n",
    "if predicted_domain == 'cardiovascular' and max_prob > 0.7:\n",
    "    print(\"   ‚úÖ ¬°√âXITO! El modelo predice correctamente 'cardiovascular'\")\n",
    "    print(\"   üéâ El mapeo de etiquetas est√° funcionando correctamente\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è El modelo no est√° prediciendo como se esperaba\")\n",
    "    print(\"   üí° Puede necesitar m√°s entrenamiento o ajustes\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e50db486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ COMPARANDO M√âTODOS DE C√ÅLCULO DE CONFIANZA\n",
      "============================================================\n",
      "\n",
      "üìä Probando m√©todo: max_prob\n",
      "üîÆ Realizando predicciones para 10 textos...\n",
      "   M√©todo de confianza: max_prob\n",
      "   Umbral de confianza: 0.7\n",
      "üî§ Tokenizando 10 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4e837e06c04719b33ce4680b8f6bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'max_prob'\n",
      "   Confianza promedio: 0.990\n",
      "   Confianza std: 0.005\n",
      "üìä Casos obvios (BioBERT): 10 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "\n",
      "üìä Probando m√©todo: difference\n",
      "üîÆ Realizando predicciones para 10 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.7\n",
      "üî§ Tokenizando 10 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd717257586247bd90598d9da34fb5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.867\n",
      "   Confianza std: 0.265\n",
      "üìä Casos obvios (BioBERT): 9 (90.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 1 (10.0%)\n",
      "\n",
      "üìä Probando m√©todo: entropy\n",
      "üîÆ Realizando predicciones para 10 textos...\n",
      "   M√©todo de confianza: entropy\n",
      "   Umbral de confianza: 0.7\n",
      "üî§ Tokenizando 10 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1b6acbb75f441a8a650e360c07f5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'entropy'\n",
      "   Confianza promedio: 0.830\n",
      "   Confianza std: 0.026\n",
      "üìä Casos obvios (BioBERT): 10 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "\n",
      "üìà COMPARACI√ìN DE M√âTODOS:\n",
      "--------------------------------------------------------------------------------\n",
      "M√©todo       Media    Std      Min      Max      Obvios   Dif√≠ciles \n",
      "--------------------------------------------------------------------------------\n",
      "max_prob     0.990    0.005    0.979    0.996    10       0         \n",
      "difference   0.867    0.265    0.072    0.971    9        1         \n",
      "entropy      0.830    0.026    0.786    0.860    10       0         \n",
      "\n",
      "üí° RECOMENDACI√ìN:\n",
      "   M√©todo recomendado: 'difference' (mayor variabilidad en confianza)\n",
      "   Este m√©todo distribuye mejor los casos entre obvios y dif√≠ciles.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# üî¨ COMPARACI√ìN DE M√âTODOS DE C√ÅLCULO DE CONFIANZA\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üî¨ COMPARANDO M√âTODOS DE C√ÅLCULO DE CONFIANZA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Textos de prueba del dataset\n",
    "test_texts = df_final['combined_text'].head(10).tolist()\n",
    "\n",
    "# M√©todos a comparar\n",
    "confidence_methods = ['max_prob', 'difference', 'entropy']\n",
    "results_comparison = {}\n",
    "\n",
    "for method in confidence_methods:\n",
    "    print(f\"\\nüìä Probando m√©todo: {method}\")\n",
    "\n",
    "    result = biobert_enhanced.predict_with_confidence_enhanced(\n",
    "        test_texts,\n",
    "        confidence_threshold=0.7,\n",
    "        confidence_method=method\n",
    "    )\n",
    "\n",
    "    confidences = result['all_confidence']\n",
    "\n",
    "    results_comparison[method] = {\n",
    "        'confidences': confidences,\n",
    "        'mean': np.mean(confidences),\n",
    "        'std': np.std(confidences),\n",
    "        'min': np.min(confidences),\n",
    "        'max': np.max(confidences),\n",
    "        'obvious_cases': len(result['obvious_cases']['indices']),\n",
    "        'difficult_cases': len(result['difficult_cases']['indices'])\n",
    "    }\n",
    "\n",
    "# Mostrar comparaci√≥n\n",
    "print(\"\\nüìà COMPARACI√ìN DE M√âTODOS:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'M√©todo':<12} {'Media':<8} {'Std':<8} {'Min':<8} {'Max':<8} {'Obvios':<8} {'Dif√≠ciles':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for method, stats in results_comparison.items():\n",
    "    print(f\"{method:<12} {stats['mean']:<8.3f} {stats['std']:<8.3f} {stats['min']:<8.3f} \"\n",
    "          f\"{stats['max']:<8.3f} {stats['obvious_cases']:<8} {stats['difficult_cases']:<10}\")\n",
    "\n",
    "# Recomendaci√≥n autom√°tica\n",
    "print(\"\\nüí° RECOMENDACI√ìN:\")\n",
    "best_method = max(results_comparison.keys(), key=lambda k: results_comparison[k]['std'])\n",
    "print(f\"   M√©todo recomendado: '{best_method}' (mayor variabilidad en confianza)\")\n",
    "print(\"   Este m√©todo distribuye mejor los casos entre obvios y dif√≠ciles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bd3ccb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ DIAGN√ìSTICO M√âDICO AVANZADO\n",
      "============================================================\n",
      "\n",
      "üß™ Probando 5 casos m√©dicos espec√≠ficos...\n",
      "\n",
      "üìã Caso 1 - Easy - Cardiovascular:\n",
      "   üìù T√≠tulo: Cardiovascular risk assessment in elderly patients with hype...\n",
      "   üéØ Esperado: cardiovascular\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.5\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82eb89e6d5f74b2c9cdca8dce368d1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.969\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 1 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "   üìä Confianza: 0.969\n",
      "   üîÆ Top 3 probabilidades:\n",
      "     ‚ù§Ô∏è Cardiovascular cardiovascular: 0.993\n",
      "     üéóÔ∏è Oncol√≥gico oncological: 0.024\n",
      "     üß† Neurol√≥gico neurological: 0.023\n",
      "   üéØ Predichas (>0.5): ['cardiovascular']\n",
      "   ‚úÖ Correcto\n",
      "\n",
      "üìã Caso 2 - Easy - Neurological:\n",
      "   üìù T√≠tulo: Neurodegenerative mechanisms in Alzheimer's disease progress...\n",
      "   üéØ Esperado: neurological\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.5\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3aa48ea4513423fbee3d1dd9029dcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.963\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 1 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "   üìä Confianza: 0.963\n",
      "   üîÆ Top 3 probabilidades:\n",
      "     üß† Neurol√≥gico neurological: 0.979\n",
      "     ‚ù§Ô∏è Cardiovascular cardiovascular: 0.016\n",
      "     ü´ò Hepatorrenal hepatorenal: 0.016\n",
      "   üéØ Predichas (>0.5): ['neurological']\n",
      "   ‚úÖ Correcto\n",
      "\n",
      "üìã Caso 3 - Medium - Hepatorenal:\n",
      "   üìù T√≠tulo: Renal function assessment in chronic kidney disease patients...\n",
      "   üéØ Esperado: hepatorenal\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.5\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dba6519fa9d4ab4b382c645ce72951f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.964\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 1 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "   üìä Confianza: 0.964\n",
      "   üîÆ Top 3 probabilidades:\n",
      "     ü´ò Hepatorrenal hepatorenal: 0.993\n",
      "     üß† Neurol√≥gico neurological: 0.029\n",
      "     üéóÔ∏è Oncol√≥gico oncological: 0.026\n",
      "   üéØ Predichas (>0.5): ['hepatorenal']\n",
      "   ‚úÖ Correcto\n",
      "\n",
      "üìã Caso 4 - Easy - Oncological:\n",
      "   üìù T√≠tulo: Novel targeted therapy for metastatic breast carcinoma...\n",
      "   üéØ Esperado: oncological\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.5\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535c217a993e4ee18b345b73a7887829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.926\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 1 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "   üìä Confianza: 0.926\n",
      "   üîÆ Top 3 probabilidades:\n",
      "     üéóÔ∏è Oncol√≥gico oncological: 0.972\n",
      "     üß† Neurol√≥gico neurological: 0.046\n",
      "     ü´ò Hepatorrenal hepatorenal: 0.028\n",
      "   üéØ Predichas (>0.5): ['oncological']\n",
      "   ‚úÖ Correcto\n",
      "\n",
      "üìã Caso 5 - Hard - Multiple:\n",
      "   üìù T√≠tulo: Multisystem complications in COVID-19 patients...\n",
      "   üéØ Esperado: multiple\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.5\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f15840d2fc451dbcce259db56dd7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.094\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 0 (0.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 1 (100.0%)\n",
      "   üìä Confianza: 0.094\n",
      "   üîÆ Top 3 probabilidades:\n",
      "     ü´ò Hepatorrenal hepatorenal: 0.985\n",
      "     üß† Neurol√≥gico neurological: 0.892\n",
      "     ‚ù§Ô∏è Cardiovascular cardiovascular: 0.629\n",
      "   üéØ Predichas (>0.5): ['cardiovascular', 'hepatorenal', 'neurological']\n",
      "   ‚úÖ Correcto (m√∫ltiples dominios)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ü©∫ DIAGN√ìSTICO M√âDICO AVANZADO CON CASOS ESPEC√çFICOS\n",
    "# ==============================================================================\n",
    "\n",
    "def run_enhanced_medical_diagnostic(biobert_model, df_final, y_labels):\n",
    "    \"\"\"Diagn√≥stico m√©dico mejorado con casos espec√≠ficos por dominio\"\"\"\n",
    "\n",
    "    print(\"üî¨ DIAGN√ìSTICO M√âDICO AVANZADO\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Casos de prueba espec√≠ficos por dominio m√©dico\n",
    "    test_cases = [\n",
    "        {\n",
    "            'title': \"Cardiovascular risk assessment in elderly patients with hypertension\",\n",
    "            'abstract': \"This study evaluates cardiac function, arterial stiffness, and coronary artery disease risk in patients over 65 years old with confirmed hypertension. We measured ejection fraction, blood pressure variability, and atherosclerotic burden.\",\n",
    "            'expected': 'cardiovascular',\n",
    "            'difficulty': 'easy'\n",
    "        },\n",
    "        {\n",
    "            'title': \"Neurodegenerative mechanisms in Alzheimer's disease progression\",\n",
    "            'abstract': \"Investigation of brain pathology, neural network degradation, and cognitive decline patterns in patients with dementia. The study focuses on amyloid-beta plaques, tau protein tangles, and synaptic dysfunction.\",\n",
    "            'expected': 'neurological',\n",
    "            'difficulty': 'easy'\n",
    "        },\n",
    "        {\n",
    "            'title': \"Renal function assessment in chronic kidney disease patients\",\n",
    "            'abstract': \"Analysis of glomerular filtration rate, creatinine clearance, and proteinuria in patients with chronic renal failure. The study includes hepatic involvement assessment and liver function markers.\",\n",
    "            'expected': 'hepatorenal',\n",
    "            'difficulty': 'medium'\n",
    "        },\n",
    "        {\n",
    "            'title': \"Novel targeted therapy for metastatic breast carcinoma\",\n",
    "            'abstract': \"Development of precision oncological treatment protocols for advanced malignant breast tumors using innovative chemotherapy combinations and immunotherapy approaches targeting HER2-positive cancer cells.\",\n",
    "            'expected': 'oncological',\n",
    "            'difficulty': 'easy'\n",
    "        },\n",
    "        {\n",
    "            'title': \"Multisystem complications in COVID-19 patients\",\n",
    "            'abstract': \"Comprehensive analysis of cardiovascular complications, acute kidney injury, and neurological manifestations in critically ill coronavirus patients. The study examines cardiac troponin elevation, renal dysfunction, and encephalitis symptoms.\",\n",
    "            'expected': 'multiple',  # Caso complejo con m√∫ltiples dominios\n",
    "            'difficulty': 'hard'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nüß™ Probando {len(test_cases)} casos m√©dicos espec√≠ficos...\")\n",
    "\n",
    "    # Probar diferentes m√©todos de confianza\n",
    "    best_method = 'difference'  # Usar el m√©todo recomendado de la celda anterior\n",
    "\n",
    "    for i, case in enumerate(test_cases):\n",
    "        print(f\"\\nüìã Caso {i+1} - {case['difficulty'].title()} - {case['expected'].title()}:\")\n",
    "        print(f\"   üìù T√≠tulo: {case['title'][:60]}...\")\n",
    "        print(f\"   üéØ Esperado: {case['expected']}\")\n",
    "\n",
    "        combined_text = f\"{case['title']} [SEP] {case['abstract']}\"\n",
    "\n",
    "        # Predecir con m√©todo mejorado\n",
    "        result = biobert_model.predict_with_confidence_enhanced(\n",
    "            [combined_text],\n",
    "            confidence_threshold=0.5,  # Umbral bajo para ver todas las probabilidades\n",
    "            confidence_method=best_method\n",
    "        )\n",
    "\n",
    "        probabilities = result['all_predictions'][0]\n",
    "        confidence = result['all_confidence'][0]\n",
    "\n",
    "        # An√°lisis de predicciones\n",
    "        predictions_dict = {}\n",
    "        for j, label in enumerate(y_labels.columns):\n",
    "            predictions_dict[label] = probabilities[j]\n",
    "\n",
    "        # Encontrar etiquetas predichas (umbral 0.5)\n",
    "        predicted_labels = [label for label, prob in predictions_dict.items() if prob > 0.5]\n",
    "\n",
    "        # Mostrar top 3 probabilidades\n",
    "        sorted_preds = sorted(predictions_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        print(f\"   üìä Confianza: {confidence:.3f}\")\n",
    "        print(\"   üîÆ Top 3 probabilidades:\")\n",
    "        for label, prob in sorted_preds[:3]:\n",
    "            emoji = label_analyzer.label_mapping.get(label, 'üè∑Ô∏è')\n",
    "            print(f\"     {emoji} {label}: {prob:.3f}\")\n",
    "\n",
    "        print(f\"   üéØ Predichas (>0.5): {predicted_labels if predicted_labels else 'Ninguna'}\")\n",
    "\n",
    "        # Evaluaci√≥n del resultado\n",
    "        if case['expected'] == 'multiple':\n",
    "            success = len(predicted_labels) > 1\n",
    "            status = \"‚úÖ Correcto (m√∫ltiples dominios)\" if success else \"‚ùå Incorrecto (deber√≠a ser m√∫ltiple)\"\n",
    "        else:\n",
    "            success = case['expected'] in predicted_labels\n",
    "            status = \"‚úÖ Correcto\" if success else \"‚ùå Incorrecto\"\n",
    "\n",
    "        print(f\"   {status}\")\n",
    "\n",
    "        # An√°lisis de dificultad vs confianza\n",
    "        if case['difficulty'] == 'easy' and confidence < 0.7:\n",
    "            print(\"   ‚ö†Ô∏è Caso f√°cil con baja confianza - revisar modelo\")\n",
    "        elif case['difficulty'] == 'hard' and confidence > 0.8:\n",
    "            print(\"   ‚ö†Ô∏è Caso dif√≠cil con alta confianza - modelo muy confiado\")\n",
    "\n",
    "# Ejecutar diagn√≥stico\n",
    "run_enhanced_medical_diagnostic(biobert_enhanced, df_final, y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cfddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ AN√ÅLISIS DEL UMBRAL √ìPTIMO DE CONFIANZA\n",
      "============================================================\n",
      "üìä Analizando 100 textos para determinar umbral √≥ptimo...\n",
      "üîÆ Realizando predicciones para 100 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.0\n",
      "üî§ Tokenizando 100 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5a9b01af7c416b90dc346356cd91b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# üéØ AN√ÅLISIS DEL UMBRAL √ìPTIMO DE CONFIANZA\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üéØ AN√ÅLISIS DEL UMBRAL √ìPTIMO DE CONFIANZA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Muestra m√°s grande para an√°lisis robusto\n",
    "sample_size = min(100, len(df_final))\n",
    "sample_texts = df_final['combined_text'].sample(sample_size, random_state=42).tolist()\n",
    "\n",
    "print(f\"üìä Analizando {sample_size} textos para determinar umbral √≥ptimo...\")\n",
    "\n",
    "# Obtener predicciones con el mejor m√©todo\n",
    "best_method = 'difference'\n",
    "results = biobert_enhanced.predict_with_confidence_enhanced(\n",
    "    sample_texts,\n",
    "    confidence_threshold=0.0,  # Umbral muy bajo para obtener todas las confianzas\n",
    "    confidence_method=best_method\n",
    ")\n",
    "\n",
    "confidences = results['all_confidence']\n",
    "\n",
    "# An√°lisis estad√≠stico de confianza\n",
    "print(\"\\nüìà ESTAD√çSTICAS DE CONFIANZA:\")\n",
    "print(f\"   üìä Muestra: {len(confidences)} textos\")\n",
    "print(f\"   üìä Media: {np.mean(confidences):.3f}\")\n",
    "print(f\"   üìä Mediana: {np.median(confidences):.3f}\")\n",
    "print(f\"   üìä Desviaci√≥n est√°ndar: {np.std(confidences):.3f}\")\n",
    "print(f\"   üìä M√≠nimo: {np.min(confidences):.3f}\")\n",
    "print(f\"   üìä M√°ximo: {np.max(confidences):.3f}\")\n",
    "\n",
    "# An√°lisis de percentiles para determinar umbrales\n",
    "percentiles = [10, 25, 50, 70, 80, 85, 90, 95, 99]\n",
    "print(\"\\nüéØ AN√ÅLISIS DE UMBRALES:\")\n",
    "print(\"   Percentil | Umbral | BioBERT% | LLM%  | Descripci√≥n\")\n",
    "print(\"   ----------|--------|----------|-------|------------------\")\n",
    "\n",
    "for p in percentiles:\n",
    "    threshold = np.percentile(confidences, p)\n",
    "    biobert_cases = np.sum(confidences >= threshold)\n",
    "    biobert_pct = (biobert_cases / len(confidences)) * 100\n",
    "    llm_pct = 100 - biobert_pct\n",
    "\n",
    "    # Descripci√≥n del balance\n",
    "    if biobert_pct > 95:\n",
    "        desc = \"Muy conservador\"\n",
    "    elif biobert_pct > 85:\n",
    "        desc = \"Conservador\"\n",
    "    elif biobert_pct > 70:\n",
    "        desc = \"Balanceado\"\n",
    "    elif biobert_pct > 50:\n",
    "        desc = \"Agresivo\"\n",
    "    else:\n",
    "        desc = \"Muy agresivo\"\n",
    "\n",
    "    print(f\"   P{p:2d}       | {threshold:.3f}  | {biobert_pct:6.1f}%  | {llm_pct:5.1f}% | {desc}\")\n",
    "\n",
    "# Recomendaci√≥n autom√°tica\n",
    "print(\"\\nüí° RECOMENDACIONES DE UMBRAL:\")\n",
    "\n",
    "# Diferentes estrategias seg√∫n el objetivo\n",
    "strategies = {\n",
    "    'cost_efficient': (85, \"Minimizar costos de LLM\"),\n",
    "    'balanced': (75, \"Balance entre precisi√≥n y costo\"),\n",
    "    'high_precision': (65, \"Maximizar precisi√≥n\"),\n",
    "}\n",
    "\n",
    "for strategy_name, (target_percentile, description) in strategies.items():\n",
    "    recommended_threshold = np.percentile(confidences, target_percentile)\n",
    "    biobert_pct = (np.sum(confidences >= recommended_threshold) / len(confidences)) * 100\n",
    "\n",
    "    print(f\"   üéØ {strategy_name.replace('_', ' ').title()}: {recommended_threshold:.3f}\")\n",
    "    print(f\"      -> {description}\")\n",
    "    print(f\"      -> BioBERT maneja {biobert_pct:.1f}% de casos\")\n",
    "\n",
    "# Seleccionar umbral recomendado (estrategia balanceada)\n",
    "recommended_threshold = np.percentile(confidences, 75)\n",
    "print(f\"\\n‚úÖ UMBRAL RECOMENDADO: {recommended_threshold:.3f}\")\n",
    "print(\"   üìä Balance √≥ptimo entre precisi√≥n y eficiencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688abed2",
   "metadata": {},
   "source": [
    "## 6. ü§ñ LLM Integration for Complex Cases\n",
    "\n",
    "Integraci√≥n de LLM (Large Language Model) para manejar el 10% de casos dif√≠ciles que requieren an√°lisis m√°s profundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb5e4139",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerativeai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgenai\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMedicalLLMClassifier\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "class MedicalLLMClassifier:\n",
    "    \"\"\"\n",
    "    Clasificador LLM especializado para casos m√©dicos complejos.\n",
    "    Utiliza Gemini para an√°lisis profundo de literatura m√©dica.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str | None = None, model: str = \"gemini-2.0-flash\"):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model\n",
    "        self.medical_domains = {\n",
    "            'neurological': 'üß† Neurol√≥gico - Relacionado con el sistema nervioso, cerebro, m√©dula espinal, nervios',\n",
    "            'cardiovascular': '‚ù§Ô∏è Cardiovascular - Relacionado con coraz√≥n, vasos sangu√≠neos, circulaci√≥n',\n",
    "            'hepatorenal': 'ü´ò Hepatorrenal - Relacionado con h√≠gado y ri√±ones, funci√≥n hep√°tica y renal',\n",
    "            'oncological': 'üéóÔ∏è Oncol√≥gico - Relacionado con c√°ncer, tumores, oncolog√≠a'\n",
    "        }\n",
    "\n",
    "        if self.api_key:\n",
    "            try:\n",
    "                genai.configure(api_key=self.api_key)\n",
    "                self.model = genai.GenerativeModel(self.model_name)\n",
    "                self.llm_available = True\n",
    "                print(f\"‚úÖ Cliente Gemini configurado correctamente con el modelo {self.model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error configurando Gemini: {e}\")\n",
    "                self.llm_available = False\n",
    "        else:\n",
    "            self.llm_available = False\n",
    "            print(\"‚ö†Ô∏è No se proporcion√≥ API key de Google. Simulando respuestas LLM.\")\n",
    "\n",
    "    def create_medical_prompt(self, title: str, abstract: str) -> str:\n",
    "        \"\"\"Crea un prompt especializado para clasificaci√≥n m√©dica\"\"\"\n",
    "\n",
    "        prompt = f\"\"\"Eres un especialista en clasificaci√≥n de literatura m√©dica. Tu tarea es analizar el siguiente art√≠culo cient√≠fico y determinar a qu√© dominios m√©dicos pertenece.\n",
    "\n",
    "DOMINIOS M√âDICOS DISPONIBLES:\n",
    "{chr(10).join([f\"- {domain}: {description}\" for domain, description in self.medical_domains.items()])}\n",
    "\n",
    "ART√çCULO A ANALIZAR:\n",
    "T√≠tulo: {title}\n",
    "Abstract: {abstract}\n",
    "\n",
    "INSTRUCCIONES:\n",
    "1. Lee cuidadosamente el t√≠tulo y abstract.\n",
    "2. Identifica conceptos m√©dicos clave, t√©rminos t√©cnicos, y contexto cl√≠nico.\n",
    "3. Determina qu√© dominios aplican (puede ser uno o m√∫ltiples).\n",
    "4. Proporciona un an√°lisis detallado de tu razonamiento.\n",
    "5. Responde en formato JSON exacto.\n",
    "\n",
    "FORMATO DE RESPUESTA (JSON):\n",
    "{{\n",
    "    \"classification\": {{\n",
    "        \"neurological\": true/false,\n",
    "        \"cardiovascular\": true/false,\n",
    "        \"hepatorenal\": true/false,\n",
    "        \"oncological\": true/false\n",
    "    }},\n",
    "    \"confidence_score\": 0.0-1.0,\n",
    "    \"reasoning\": \"Explicaci√≥n detallada del an√°lisis m√©dico y justificaci√≥n de la clasificaci√≥n.\",\n",
    "    \"key_medical_terms\": [\"t√©rmino1\", \"t√©rmino2\", \"t√©rmino3\"]\n",
    "}}\n",
    "\n",
    "Responde √∫nicamente con el JSON, sin texto adicional ni formato markdown.\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def simulate_llm_response(self, title: str, abstract: str) -> dict:\n",
    "        \"\"\"\n",
    "        Simula respuesta LLM para demostraci√≥n cuando no hay API key.\n",
    "        En producci√≥n, usar el LLM real.\n",
    "        \"\"\"\n",
    "        text = (title + \" \" + abstract).lower()\n",
    "        classification = {\n",
    "            \"neurological\": any(word in text for word in ['brain', 'neural', 'neuro', 'nervous', 'cognitive']),\n",
    "            \"cardiovascular\": any(word in text for word in ['heart', 'cardiac', 'vascular', 'blood', 'arterial']),\n",
    "            \"hepatorenal\": any(word in text for word in ['liver', 'hepatic', 'kidney', 'renal', 'nephro']),\n",
    "            \"oncological\": any(word in text for word in ['cancer', 'tumor', 'oncology', 'carcinoma', 'malignant'])\n",
    "        }\n",
    "        confidence = min(0.9, sum(classification.values()) * 0.3 + 0.4)\n",
    "        medical_terms = [k for k, v in classification.items() if v]\n",
    "        return {\n",
    "            \"classification\": classification,\n",
    "            \"confidence_score\": confidence,\n",
    "            \"reasoning\": f\"An√°lisis simulado basado en palabras clave. Detectados conceptos de: {', '.join(medical_terms)}.\",\n",
    "            \"key_medical_terms\": medical_terms\n",
    "        }\n",
    "\n",
    "    def _clean_json_response(self, text: str) -> str:\n",
    "        \"\"\"Limpia la respuesta del LLM para extraer solo el JSON.\"\"\"\n",
    "        match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "        return text\n",
    "\n",
    "    def classify_complex_case(self, title: str, abstract: str) -> dict:\n",
    "        \"\"\"Clasifica un caso m√©dico complejo usando Gemini\"\"\"\n",
    "        if not self.llm_available:\n",
    "            print(\"üîÑ Simulando an√°lisis LLM...\")\n",
    "            return self.simulate_llm_response(title, abstract)\n",
    "\n",
    "        try:\n",
    "            prompt = self.create_medical_prompt(title, abstract)\n",
    "            generation_config = genai.types.GenerationConfig(\n",
    "                temperature=0.1,\n",
    "                response_mime_type=\"application/json\"\n",
    "            )\n",
    "            response = self.model.generate_content(prompt, generation_config=generation_config)\n",
    "\n",
    "            result_text = self._clean_json_response(response.text)\n",
    "\n",
    "            try:\n",
    "                result = json.loads(result_text)\n",
    "                return result\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Error parsing JSON de Gemini, usando respuesta simulada. Respuesta recibida:\\n{response.text}\")\n",
    "                return self.simulate_llm_response(title, abstract)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error en API de Gemini: {e}\")\n",
    "            print(\"üîÑ Fallback a simulaci√≥n...\")\n",
    "            return self.simulate_llm_response(title, abstract)\n",
    "\n",
    "    def classify_batch(self, cases: list[tuple[str, str]]) -> list[dict]:\n",
    "        \"\"\"Clasifica m√∫ltiples casos m√©dicos complejos\"\"\"\n",
    "        print(f\"ü§ñ Procesando {len(cases)} casos complejos con Gemini...\")\n",
    "        results = []\n",
    "        for i, (title, abstract) in enumerate(cases):\n",
    "            if i > 0 and i % 5 == 0:\n",
    "                print(f\"   Procesando caso {i+1}/{len(cases)}\")\n",
    "            result = self.classify_complex_case(title, abstract)\n",
    "            results.append(result)\n",
    "        print(f\"‚úÖ Completado an√°lisis de {len(cases)} casos complejos\")\n",
    "        return results\n",
    "\n",
    "# Cargar variables de entorno (del archivo .env)\n",
    "load_dotenv()\n",
    "\n",
    "# Inicializar clasificador LLM con Gemini\n",
    "# Obtiene la API key de las variables de entorno\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "llm_classifier = MedicalLLMClassifier(api_key=gemini_api_key, model=\"gemini-2.0-flash\")\n",
    "\n",
    "print(\"ü§ñ Clasificador Gemini LLM inicializado para casos complejos\")\n",
    "if not llm_classifier.llm_available:\n",
    "    print(\"üí° En producci√≥n, aseg√∫rate de configurar la variable de entorno GEMINI_API_KEY.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7044eb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Verificando la integraci√≥n directa con Gemini...\n",
      "\n",
      "‚úÖ Respuesta recibida de Gemini:\n",
      "{\n",
      "  \"classification\": {\n",
      "    \"neurological\": false,\n",
      "    \"cardiovascular\": false,\n",
      "    \"hepatorenal\": false,\n",
      "    \"oncological\": true\n",
      "  },\n",
      "  \"confidence_score\": 0.95,\n",
      "  \"reasoning\": \"The title and abstract explicitly mention 'cancer cells,' 'oncological implications,' 'malignant tumor growth,' and 'apoptosis in cancer cell lines.' These terms are directly related to oncology, indicating a strong relevance to the oncological domain. There is no mention of neurological, cardiovascular, or hepatorenal systems or conditions.\",\n",
      "  \"key_medical_terms\": [\n",
      "    \"cancer cells\",\n",
      "    \"oncological\",\n",
      "    \"malignant tumor\",\n",
      "    \"apoptosis\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "üëç ¬°√âXITO! La respuesta parece provenir de la API de Gemini.\n"
     ]
    }
   ],
   "source": [
    "# Celda de prueba para verificar la integraci√≥n con Gemini\n",
    "print(\"üß™ Verificando la integraci√≥n directa con Gemini...\")\n",
    "\n",
    "if llm_classifier.llm_available:\n",
    "    # Crear un art√≠culo de prueba\n",
    "    test_title = \"A study on the effects of novel drug X on cancer cells\"\n",
    "    test_abstract = \"This paper investigates the oncological implications of drug X, a new compound designed to target malignant tumor growth. We observed significant apoptosis in cancer cell lines.\"\n",
    "\n",
    "    # Llamar directamente al m√©todo de clasificaci√≥n\n",
    "    gemini_result = llm_classifier.classify_complex_case(test_title, test_abstract)\n",
    "\n",
    "    # Imprimir el resultado para inspecci√≥n manual\n",
    "    print(\"\\n‚úÖ Respuesta recibida de Gemini:\")\n",
    "    import json\n",
    "    print(json.dumps(gemini_result, indent=2))\n",
    "\n",
    "    # Verificaci√≥n autom√°tica\n",
    "    if \"simulado\" in gemini_result.get(\"reasoning\", \"\").lower():\n",
    "        print(\"\\n‚ùå ¬°ALERTA! La respuesta parece ser simulada. Revisa la API Key y la conexi√≥n.\")\n",
    "    else:\n",
    "        print(\"\\nüëç ¬°√âXITO! La respuesta parece provenir de la API de Gemini.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå El clasificador LLM no est√° disponible. Revisa la configuraci√≥n de la API Key.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857bfe6e",
   "metadata": {},
   "source": [
    "## 7. üîÑ Hybrid Classification System\n",
    "\n",
    "Sistema h√≠brido que combina BioBERT para casos obvios y LLM para casos complejos, optimizando precisi√≥n y costo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d57ab95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Etiquetas sincronizadas con BioBERT enhanced:\n",
      "   -> ['cardiovascular', 'hepatorenal', 'neurological', 'oncological']\n",
      "üîÑ Sistema h√≠brido mejorado inicializado\n",
      "‚öñÔ∏è Umbral de confianza: 0.7\n",
      "üéØ Listo para clasificar con etiquetas m√©dicas correctas\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# üîÑ SISTEMA H√çBRIDO MEJORADO CON ETIQUETAS CORREGIDAS\n",
    "# ==============================================================================\n",
    "\n",
    "class HybridMedicalClassifierEnhanced:\n",
    "    \"\"\"\n",
    "    Sistema h√≠brido mejorado que combina BioBERT y LLM para clasificaci√≥n √≥ptima.\n",
    "    Mejoras incluidas:\n",
    "    - Manejo correcto del mapeo de etiquetas m√©dicas\n",
    "    - Uso del BioBERTClassifierEnhanced\n",
    "    - Mejor gesti√≥n de confianza y umbrales\n",
    "    - Estad√≠sticas m√°s detalladas\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, biobert_classifier, llm_classifier, confidence_threshold=0.7):\n",
    "        self.biobert = biobert_classifier\n",
    "        self.llm = llm_classifier\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "\n",
    "        # MEJORA: Usar las etiquetas ya corregidas del modelo BioBERT enhanced\n",
    "        if hasattr(self.biobert, 'label_names') and self.biobert.label_names:\n",
    "            self.label_names = self.biobert.label_names\n",
    "            print(\"‚úÖ Etiquetas sincronizadas con BioBERT enhanced:\")\n",
    "            print(f\"   -> {self.label_names}\")\n",
    "        elif hasattr(self.biobert.model.config, 'id2label'):\n",
    "            self.label_names = [self.biobert.model.config.id2label[i]\n",
    "                              for i in range(len(self.biobert.model.config.id2label))]\n",
    "            print(\"‚úÖ Etiquetas extra√≠das de la configuraci√≥n del modelo:\")\n",
    "            print(f\"   -> {self.label_names}\")\n",
    "        else:\n",
    "            # Fallback ordenado alfab√©ticamente (como en el dataset)\n",
    "            self.label_names = ['cardiovascular', 'hepatorenal', 'neurological', 'oncological']\n",
    "            print(\"‚ö†Ô∏è Usando etiquetas por defecto (orden alfab√©tico)\")\n",
    "\n",
    "        # M√©tricas de rendimiento mejoradas\n",
    "        self.stats = {\n",
    "            'total_processed': 0,\n",
    "            'biobert_cases': 0,\n",
    "            'llm_cases': 0,\n",
    "            'processing_times': [],\n",
    "            'confidence_scores': [],\n",
    "            'biobert_confidences': [],\n",
    "            'llm_confidences': [],\n",
    "            'method_distribution': {}\n",
    "        }\n",
    "\n",
    "    def classify_article(self, title: str, abstract: str, use_enhanced_biobert: bool = True) -> dict:\n",
    "        \"\"\"\n",
    "        Clasifica un art√≠culo m√©dico usando el sistema h√≠brido mejorado.\n",
    "\n",
    "        Args:\n",
    "            title: T√≠tulo del art√≠culo\n",
    "            abstract: Abstract del art√≠culo\n",
    "            use_enhanced_biobert: Si usar predict_with_confidence_enhanced\n",
    "        \"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Combinar texto como lo hace BioBERT\n",
    "        combined_text = f\"{title} [SEP] {abstract}\"\n",
    "\n",
    "        # Paso 1: Intentar con BioBERT (usando m√©todo enhanced si est√° disponible)\n",
    "        if use_enhanced_biobert and hasattr(self.biobert, 'predict_with_confidence_enhanced'):\n",
    "            biobert_results = self.biobert.predict_with_confidence_enhanced(\n",
    "                [combined_text],\n",
    "                confidence_threshold=self.confidence_threshold,\n",
    "                confidence_method='difference'  # Usar el mejor m√©todo\n",
    "            )\n",
    "        else:\n",
    "            biobert_results = self.biobert.predict_with_confidence(\n",
    "                [combined_text],\n",
    "                self.confidence_threshold\n",
    "            )\n",
    "\n",
    "        # Verificar si BioBERT tiene confianza suficiente\n",
    "        if len(biobert_results['obvious_cases']['indices']) > 0:\n",
    "            # Caso obvio - usar BioBERT\n",
    "            predictions = biobert_results['obvious_cases']['predictions'][0]\n",
    "            confidence = biobert_results['obvious_cases']['confidence_scores'][0]\n",
    "\n",
    "            # MEJORA: Convertir usando el mapeo correcto de etiquetas\n",
    "            classification = {}\n",
    "            for i, label in enumerate(self.label_names):\n",
    "                classification[label] = bool(predictions[i] > 0.5)\n",
    "\n",
    "            result = {\n",
    "                'classification': classification,\n",
    "                'confidence_score': float(confidence),\n",
    "                'method_used': 'BioBERT',\n",
    "                'reasoning': f\"Clasificaci√≥n autom√°tica con BioBERT (confianza: {confidence:.3f})\",\n",
    "                'predictions_raw': predictions.tolist(),\n",
    "                'confidence_method': biobert_results.get('confidence_method', 'default')\n",
    "            }\n",
    "\n",
    "            self.stats['biobert_cases'] += 1\n",
    "            self.stats['biobert_confidences'].append(confidence)\n",
    "\n",
    "        else:\n",
    "            # Caso dif√≠cil - usar LLM\n",
    "            llm_result = self.llm.classify_complex_case(title, abstract)\n",
    "\n",
    "            result = {\n",
    "                'classification': llm_result['classification'],\n",
    "                'confidence_score': llm_result['confidence_score'],\n",
    "                'method_used': 'LLM',\n",
    "                'reasoning': llm_result['reasoning'],\n",
    "                'key_medical_terms': llm_result.get('key_medical_terms', [])\n",
    "            }\n",
    "\n",
    "            self.stats['llm_cases'] += 1\n",
    "            self.stats['llm_confidences'].append(llm_result['confidence_score'])\n",
    "\n",
    "        # Actualizar estad√≠sticas\n",
    "        processing_time = time.time() - start_time\n",
    "        self.stats['total_processed'] += 1\n",
    "        self.stats['processing_times'].append(processing_time)\n",
    "        self.stats['confidence_scores'].append(result['confidence_score'])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def classify_batch(self, articles: list[tuple[str, str]], use_enhanced_biobert: bool = True) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Clasifica m√∫ltiples art√≠culos usando el sistema h√≠brido mejorado.\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Procesando {len(articles)} art√≠culos con sistema h√≠brido mejorado...\")\n",
    "\n",
    "        # Combinar todos los textos\n",
    "        combined_texts = [f\"{title} [SEP] {abstract}\" for title, abstract in articles]\n",
    "\n",
    "        # Paso 1: Procesar todos con BioBERT para obtener confianza\n",
    "        print(\"üß¨ Paso 1: An√°lisis inicial con BioBERT enhanced...\")\n",
    "\n",
    "        if use_enhanced_biobert and hasattr(self.biobert, 'predict_with_confidence_enhanced'):\n",
    "            biobert_results = self.biobert.predict_with_confidence_enhanced(\n",
    "                combined_texts,\n",
    "                confidence_threshold=self.confidence_threshold,\n",
    "                confidence_method='difference'\n",
    "            )\n",
    "        else:\n",
    "            biobert_results = self.biobert.predict_with_confidence(\n",
    "                combined_texts,\n",
    "                self.confidence_threshold\n",
    "            )\n",
    "\n",
    "        # Inicializar resultados\n",
    "        all_results = [None] * len(articles)\n",
    "\n",
    "        # Paso 2: Procesar casos obvios con BioBERT\n",
    "        obvious_indices = biobert_results['obvious_cases']['indices']\n",
    "        if len(obvious_indices) > 0:\n",
    "            print(f\"‚úÖ Procesando {len(obvious_indices)} casos obvios con BioBERT\")\n",
    "\n",
    "            for i, orig_idx in enumerate(obvious_indices):\n",
    "                predictions = biobert_results['obvious_cases']['predictions'][i]\n",
    "                confidence = biobert_results['obvious_cases']['confidence_scores'][i]\n",
    "\n",
    "                # MEJORA: Usar mapeo correcto de etiquetas\n",
    "                classification = {}\n",
    "                for j, label in enumerate(self.label_names):\n",
    "                    classification[label] = bool(predictions[j] > 0.5)\n",
    "\n",
    "                all_results[orig_idx] = {\n",
    "                    'classification': classification,\n",
    "                    'confidence_score': float(confidence),\n",
    "                    'method_used': 'BioBERT',\n",
    "                    'reasoning': f\"Clasificaci√≥n autom√°tica con BioBERT (confianza: {confidence:.3f})\",\n",
    "                    'predictions_raw': predictions.tolist()\n",
    "                }\n",
    "\n",
    "        # Paso 3: Procesar casos dif√≠ciles con LLM\n",
    "        difficult_indices = biobert_results['difficult_cases']['indices']\n",
    "        if len(difficult_indices) > 0:\n",
    "            print(f\"ü§ñ Procesando {len(difficult_indices)} casos complejos con LLM\")\n",
    "\n",
    "            difficult_cases = [(articles[i][0], articles[i][1]) for i in difficult_indices]\n",
    "            llm_results = self.llm.classify_batch(difficult_cases)\n",
    "\n",
    "            for i, orig_idx in enumerate(difficult_indices):\n",
    "                llm_result = llm_results[i]\n",
    "\n",
    "                all_results[orig_idx] = {\n",
    "                    'classification': llm_result['classification'],\n",
    "                    'confidence_score': llm_result['confidence_score'],\n",
    "                    'method_used': 'LLM',\n",
    "                    'reasoning': llm_result['reasoning'],\n",
    "                    'key_medical_terms': llm_result.get('key_medical_terms', [])\n",
    "                }\n",
    "\n",
    "        # Actualizar estad√≠sticas\n",
    "        self.stats['total_processed'] += len(articles)\n",
    "        self.stats['biobert_cases'] += len(obvious_indices)\n",
    "        self.stats['llm_cases'] += len(difficult_indices)\n",
    "\n",
    "        print(\"‚úÖ Procesamiento h√≠brido completado:\")\n",
    "        print(f\"   üß¨ BioBERT: {len(obvious_indices)} casos ({len(obvious_indices)/len(articles)*100:.1f}%)\")\n",
    "        print(f\"   ü§ñ LLM: {len(difficult_indices)} casos ({len(difficult_indices)/len(articles)*100:.1f}%)\")\n",
    "\n",
    "        return all_results\n",
    "\n",
    "    def get_performance_stats(self) -> dict:\n",
    "        \"\"\"Retorna estad√≠sticas mejoradas de rendimiento del sistema h√≠brido\"\"\"\n",
    "        if self.stats['total_processed'] == 0:\n",
    "            return {\"message\": \"No se han procesado art√≠culos a√∫n\"}\n",
    "\n",
    "        biobert_pct = (self.stats['biobert_cases'] / self.stats['total_processed']) * 100\n",
    "        llm_pct = (self.stats['llm_cases'] / self.stats['total_processed']) * 100\n",
    "\n",
    "        stats = {\n",
    "            'total_articles': self.stats['total_processed'],\n",
    "            'biobert_cases': self.stats['biobert_cases'],\n",
    "            'llm_cases': self.stats['llm_cases'],\n",
    "            'biobert_percentage': biobert_pct,\n",
    "            'llm_percentage': llm_pct,\n",
    "            'average_confidence': np.mean(self.stats['confidence_scores']),\n",
    "            'average_processing_time': np.mean(self.stats['processing_times']) if self.stats['processing_times'] else 0,\n",
    "            'efficiency_score': biobert_pct  # Mayor uso de BioBERT = mayor eficiencia\n",
    "        }\n",
    "\n",
    "        # Agregar estad√≠sticas por m√©todo si est√°n disponibles\n",
    "        if self.stats['biobert_confidences']:\n",
    "            stats['biobert_avg_confidence'] = np.mean(self.stats['biobert_confidences'])\n",
    "        if self.stats['llm_confidences']:\n",
    "            stats['llm_avg_confidence'] = np.mean(self.stats['llm_confidences'])\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def adjust_confidence_threshold(self, new_threshold: float):\n",
    "        \"\"\"Permite ajustar el umbral de confianza din√°micamente\"\"\"\n",
    "        old_threshold = self.confidence_threshold\n",
    "        self.confidence_threshold = new_threshold\n",
    "        print(f\"üéØ Umbral de confianza ajustado: {old_threshold:.3f} -> {new_threshold:.3f}\")\n",
    "\n",
    "        if new_threshold > old_threshold:\n",
    "            print(\"   üìà M√°s casos ir√°n al LLM (mayor precisi√≥n)\")\n",
    "        else:\n",
    "            print(\"   üìâ M√°s casos ir√°n a BioBERT (mayor eficiencia)\")\n",
    "\n",
    "# Crear sistema h√≠brido mejorado usando biobert_enhanced\n",
    "hybrid_system_enhanced = HybridMedicalClassifierEnhanced(\n",
    "    biobert_classifier=biobert_enhanced,  # Usar el modelo enhanced con etiquetas corregidas\n",
    "    llm_classifier=llm_classifier,\n",
    "    confidence_threshold=0.70\n",
    ")\n",
    "\n",
    "print(\"üîÑ Sistema h√≠brido mejorado inicializado\")\n",
    "print(f\"‚öñÔ∏è Umbral de confianza: {hybrid_system_enhanced.confidence_threshold}\")\n",
    "print(\"üéØ Listo para clasificar con etiquetas m√©dicas correctas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938e7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ DEMOSTRACI√ìN MEJORADA DEL SISTEMA H√çBRIDO\n",
      "============================================================\n",
      "üß™ Probando 4 casos espec√≠ficos por dominio...\n",
      "\n",
      "üìã Caso 1 - Cardiovascular:\n",
      "   üìù T√≠tulo: Cardiac arrhythmia detection using machine learning...\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.7\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c566b56165b44b84b5e3614651478d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.958\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 1 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "   üéØ Esperado: cardiovascular\n",
      "   üîÆ Predicho: cardiovascular\n",
      "   ‚ö° M√©todo: BioBERT\n",
      "   üìä Confianza: 0.958\n",
      "   ‚úÖ CORRECTO\n",
      "   üìä Probabilidades detalladas:\n",
      "     üéØ cardiovascular: 0.989\n",
      "        hepatorenal: 0.020\n",
      "        neurological: 0.031\n",
      "        oncological: 0.019\n",
      "\n",
      "üìã Caso 2 - Neurological:\n",
      "   üìù T√≠tulo: Alzheimer's disease progression analysis...\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.7\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c7f176d0264f5d8586e3bdef6f612e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.939\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 1 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "   üéØ Esperado: neurological\n",
      "   üîÆ Predicho: neurological\n",
      "   ‚ö° M√©todo: BioBERT\n",
      "   üìä Confianza: 0.939\n",
      "   ‚úÖ CORRECTO\n",
      "   üìä Probabilidades detalladas:\n",
      "        cardiovascular: 0.027\n",
      "        hepatorenal: 0.021\n",
      "     üéØ neurological: 0.967\n",
      "        oncological: 0.025\n",
      "\n",
      "üìã Caso 3 - Hepatorenal:\n",
      "   üìù T√≠tulo: Kidney function assessment in liver disease...\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.7\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6050bb61b14542aba9f5ad0dcad642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.951\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 1 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "   üéØ Esperado: hepatorenal\n",
      "   üîÆ Predicho: hepatorenal\n",
      "   ‚ö° M√©todo: BioBERT\n",
      "   üìä Confianza: 0.951\n",
      "   ‚úÖ CORRECTO\n",
      "   üìä Probabilidades detalladas:\n",
      "        cardiovascular: 0.031\n",
      "     üéØ hepatorenal: 0.987\n",
      "        neurological: 0.036\n",
      "        oncological: 0.036\n",
      "\n",
      "üìã Caso 4 - Oncological:\n",
      "   üìù T√≠tulo: Novel cancer therapy for breast tumors...\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.7\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb4a71afcfe4877a179119ffed50ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.902\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 1 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "   üéØ Esperado: oncological\n",
      "   üîÆ Predicho: oncological\n",
      "   ‚ö° M√©todo: BioBERT\n",
      "   üìä Confianza: 0.902\n",
      "   ‚úÖ CORRECTO\n",
      "   üìä Probabilidades detalladas:\n",
      "        cardiovascular: 0.026\n",
      "        hepatorenal: 0.035\n",
      "        neurological: 0.065\n",
      "     üéØ oncological: 0.967\n",
      "\n",
      "üìà RESUMEN DE LA DEMOSTRACI√ìN:\n",
      "   Casos correctos: 4/4\n",
      "   Precisi√≥n: 100.0%\n",
      "\n",
      "üìä ESTAD√çSTICAS DEL SISTEMA H√çBRIDO MEJORADO:\n",
      "  total_articles: 4\n",
      "  biobert_cases: 4\n",
      "  llm_cases: 0\n",
      "  biobert_percentage: 100.000\n",
      "  llm_percentage: 0.000\n",
      "  average_confidence: 0.938\n",
      "  average_processing_time: 1.439\n",
      "  efficiency_score: 100.000\n",
      "  biobert_avg_confidence: 0.9376373291015625\n",
      "\n",
      "üéõÔ∏è PRUEBA DE AJUSTE DIN√ÅMICO DE UMBRAL:\n",
      "üéØ Umbral de confianza ajustado: 0.700 -> 0.600\n",
      "   üìâ M√°s casos ir√°n a BioBERT (mayor eficiencia)\n",
      "üéØ Umbral de confianza ajustado: 0.600 -> 0.800\n",
      "   üìà M√°s casos ir√°n al LLM (mayor precisi√≥n)\n",
      "üéØ Umbral de confianza ajustado: 0.800 -> 0.700\n",
      "   üìâ M√°s casos ir√°n a BioBERT (mayor eficiencia)\n",
      "\n",
      "üéâ ¬°EXCELENTE! El sistema h√≠brido funciona correctamente\n",
      "üè• Mapeo de etiquetas m√©dicas verificado y funcionando\n",
      "\n",
      "‚úÖ Demostraci√≥n completada exitosamente!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# üöÄ DEMOSTRACI√ìN MEJORADA DEL SISTEMA H√çBRIDO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üöÄ DEMOSTRACI√ìN MEJORADA DEL SISTEMA H√çBRIDO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Casos de prueba espec√≠ficos para verificar el mapeo correcto\n",
    "test_cases = [\n",
    "    {\n",
    "        'title': \"Cardiac arrhythmia detection using machine learning\",\n",
    "        'abstract': \"This study presents automated detection of heart rhythm disorders using ECG signals and cardiovascular risk assessment.\",\n",
    "        'expected_domain': 'cardiovascular'\n",
    "    },\n",
    "    {\n",
    "        'title': \"Alzheimer's disease progression analysis\",\n",
    "        'abstract': \"Investigation of brain degeneration patterns and neurological symptoms in patients with cognitive decline.\",\n",
    "        'expected_domain': 'neurological'\n",
    "    },\n",
    "    {\n",
    "        'title': \"Kidney function assessment in liver disease\",\n",
    "        'abstract': \"Analysis of renal function and hepatic markers in patients with chronic liver disease and nephropathy.\",\n",
    "        'expected_domain': 'hepatorenal'\n",
    "    },\n",
    "    {\n",
    "        'title': \"Novel cancer therapy for breast tumors\",\n",
    "        'abstract': \"Development of targeted oncological treatment for malignant breast cancer using innovative chemotherapy.\",\n",
    "        'expected_domain': 'oncological'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üß™ Probando {len(test_cases)} casos espec√≠ficos por dominio...\")\n",
    "\n",
    "correct_predictions = 0\n",
    "total_cases = len(test_cases)\n",
    "\n",
    "for i, case in enumerate(test_cases):\n",
    "    print(f\"\\nüìã Caso {i+1} - {case['expected_domain'].capitalize()}:\")\n",
    "    print(f\"   üìù T√≠tulo: {case['title'][:80]}...\")\n",
    "\n",
    "    # Clasificar con sistema h√≠brido mejorado\n",
    "    result = hybrid_system_enhanced.classify_article(case['title'], case['abstract'])\n",
    "\n",
    "    # Analizar resultado\n",
    "    predicted_domains = [domain for domain, is_present in result['classification'].items() if is_present]\n",
    "\n",
    "    print(f\"   üéØ Esperado: {case['expected_domain']}\")\n",
    "    print(f\"   üîÆ Predicho: {', '.join(predicted_domains) if predicted_domains else 'Ninguno'}\")\n",
    "    print(f\"   ‚ö° M√©todo: {result['method_used']}\")\n",
    "    print(f\"   üìä Confianza: {result['confidence_score']:.3f}\")\n",
    "\n",
    "    # Verificar si es correcto\n",
    "    is_correct = case['expected_domain'] in predicted_domains\n",
    "    status = \"‚úÖ CORRECTO\" if is_correct else \"‚ùå INCORRECTO\"\n",
    "    print(f\"   {status}\")\n",
    "\n",
    "    if is_correct:\n",
    "        correct_predictions += 1\n",
    "\n",
    "    # Mostrar probabilidades detalladas si es BioBERT\n",
    "    if result['method_used'] == 'BioBERT' and 'predictions_raw' in result:\n",
    "        print(\"   üìä Probabilidades detalladas:\")\n",
    "        for j, (label, prob) in enumerate(zip(hybrid_system_enhanced.label_names, result['predictions_raw'], strict=False)):\n",
    "            emoji = \"üéØ\" if prob > 0.5 else \"  \"\n",
    "            print(f\"     {emoji} {label}: {prob:.3f}\")\n",
    "\n",
    "# Mostrar resumen\n",
    "accuracy = (correct_predictions / total_cases) * 100\n",
    "print(\"\\nüìà RESUMEN DE LA DEMOSTRACI√ìN:\")\n",
    "print(f\"   Casos correctos: {correct_predictions}/{total_cases}\")\n",
    "print(f\"   Precisi√≥n: {accuracy:.1f}%\")\n",
    "\n",
    "# Mostrar estad√≠sticas del sistema\n",
    "print(\"\\nüìä ESTAD√çSTICAS DEL SISTEMA H√çBRIDO MEJORADO:\")\n",
    "stats = hybrid_system_enhanced.get_performance_stats()\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Prueba de ajuste din√°mico del umbral\n",
    "print(\"\\nüéõÔ∏è PRUEBA DE AJUSTE DIN√ÅMICO DE UMBRAL:\")\n",
    "hybrid_system_enhanced.adjust_confidence_threshold(0.6)  # M√°s agresivo\n",
    "hybrid_system_enhanced.adjust_confidence_threshold(0.8)  # M√°s conservador\n",
    "hybrid_system_enhanced.adjust_confidence_threshold(0.7)  # Volver al original\n",
    "\n",
    "if accuracy >= 75:\n",
    "    print(\"\\nüéâ ¬°EXCELENTE! El sistema h√≠brido funciona correctamente\")\n",
    "    print(\"üè• Mapeo de etiquetas m√©dicas verificado y funcionando\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è El sistema necesita ajustes adicionales\")\n",
    "\n",
    "print(\"\\n‚úÖ Demostraci√≥n completada exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a180c07",
   "metadata": {},
   "source": [
    "## 8. üìä Model Evaluation and Metrics\n",
    "\n",
    "Evaluaci√≥n completa del sistema h√≠brido con m√©tricas especializadas para clasificaci√≥n multilabel m√©dica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd467c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî™ Dividiendo los datos en conjuntos de entrenamiento y prueba...\n",
      "‚úÖ Datos divididos:\n",
      "   - Conjunto de entrenamiento: 2852 muestras\n",
      "   - Conjunto de prueba: 713 muestras\n",
      "üîÑ Usando sistema h√≠brido enhanced\n",
      "üè∑Ô∏è Etiquetas del sistema: ['cardiovascular', 'hepatorenal', 'neurological', 'oncological']\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DIVISI√ìN DE DATOS PARA EVALUACI√ìN\n",
    "# ==============================================================================\n",
    "print(\"üî™ Dividiendo los datos en conjuntos de entrenamiento y prueba...\")\n",
    "\n",
    "# X: Textos combinados (features)\n",
    "# y: Etiquetas binarizadas (targets)\n",
    "X = df_final['combined_text'].values\n",
    "y = y_labels.values\n",
    "\n",
    "# Dividir los datos para tener un conjunto de prueba consistente\n",
    "# Usamos un 20% para prueba, que es un est√°ndar com√∫n.\n",
    "# random_state=42 asegura que la divisi√≥n sea siempre la misma.\n",
    "X_train, X_test, y_train_df, y_test_df = train_test_split(\n",
    "    X, y_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Datos divididos:\")\n",
    "print(f\"   - Conjunto de entrenamiento: {len(X_train)} muestras\")\n",
    "print(f\"   - Conjunto de prueba: {len(X_test)} muestras\")\n",
    "\n",
    "# MEJORA: Verificar que tenemos el sistema h√≠brido correcto\n",
    "if 'hybrid_system_enhanced' in locals():\n",
    "    print(\"üîÑ Usando sistema h√≠brido enhanced\")\n",
    "    hybrid_system = hybrid_system_enhanced  # Asegurar que usamos la versi√≥n mejorada\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Sistema h√≠brido enhanced no encontrado, usando versi√≥n b√°sica\")\n",
    "\n",
    "print(f\"üè∑Ô∏è Etiquetas del sistema: {hybrid_system.label_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a7fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Evaluador m√©dico mejorado creado\n"
     ]
    }
   ],
   "source": [
    "class MedicalEvaluatorEnhanced:\n",
    "    \"\"\"\n",
    "    Evaluador especializado mejorado para sistemas de clasificaci√≥n m√©dica multilabel.\n",
    "    Compatible con el sistema h√≠brido enhanced.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, label_names):\n",
    "        self.label_names = label_names\n",
    "        self.medical_domains = {\n",
    "            'neurological': 'üß† Neurol√≥gico',\n",
    "            'cardiovascular': '‚ù§Ô∏è Cardiovascular',\n",
    "            'hepatorenal': 'ü´ò Hepatorrenal',\n",
    "            'oncological': 'üéóÔ∏è Oncol√≥gico'\n",
    "        }\n",
    "\n",
    "    def prepare_evaluation_data(self, test_articles, true_labels, predictions):\n",
    "        \"\"\"Prepara datos para evaluaci√≥n con mejor manejo de √≠ndices\"\"\"\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        print(\"üîß Preparando datos de evaluaci√≥n...\")\n",
    "        print(f\"   - Art√≠culos de prueba: {len(test_articles)}\")\n",
    "        print(f\"   - Etiquetas verdaderas: {len(true_labels)}\")\n",
    "        print(f\"   - Predicciones: {len(predictions)}\")\n",
    "        print(f\"   - Etiquetas del modelo: {self.label_names}\")\n",
    "\n",
    "        for i, article in enumerate(test_articles):\n",
    "            # MEJORA: Verificar que tenemos datos v√°lidos\n",
    "            if i >= len(true_labels) or i >= len(predictions):\n",
    "                print(f\"‚ö†Ô∏è Saltando √≠ndice {i} - fuera de rango\")\n",
    "                continue\n",
    "\n",
    "            # Obtener etiquetas verdaderas\n",
    "            try:\n",
    "                if hasattr(true_labels, 'iloc'):\n",
    "                    # Es un DataFrame\n",
    "                    true_row = [true_labels.iloc[i][label] for label in self.label_names]\n",
    "                else:\n",
    "                    # Es un array numpy\n",
    "                    true_row = [true_labels[i][j] for j, label in enumerate(self.label_names)]\n",
    "                y_true.append(true_row)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error obteniendo etiquetas verdaderas para √≠ndice {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Obtener predicciones\n",
    "            try:\n",
    "                pred_row = [predictions[i]['classification'][label] for label in self.label_names]\n",
    "                y_pred.append(pred_row)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error obteniendo predicciones para √≠ndice {i}: {e}\")\n",
    "                print(f\"   Predicci√≥n disponible: {predictions[i].keys() if i < len(predictions) else 'N/A'}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"‚úÖ Datos preparados: {len(y_true)} muestras v√°lidas\")\n",
    "        return np.array(y_true), np.array(y_pred)\n",
    "\n",
    "    def calculate_multilabel_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calcula m√©tricas completas para clasificaci√≥n multilabel\"\"\"\n",
    "        if len(y_true) == 0 or len(y_pred) == 0:\n",
    "            print(\"‚ùå No hay datos v√°lidos para calcular m√©tricas\")\n",
    "            return {}\n",
    "\n",
    "        metrics = {}\n",
    "\n",
    "        try:\n",
    "            # M√©tricas globales\n",
    "            metrics['exact_match_ratio'] = accuracy_score(y_true, y_pred)\n",
    "            metrics['hamming_loss'] = hamming_loss(y_true, y_pred)\n",
    "            metrics['jaccard_score'] = jaccard_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "            # M√©tricas por averaging\n",
    "            for avg in ['micro', 'macro', 'weighted']:\n",
    "                metrics[f'precision_{avg}'] = precision_score(y_true, y_pred, average=avg, zero_division=0)\n",
    "                metrics[f'recall_{avg}'] = recall_score(y_true, y_pred, average=avg, zero_division=0)\n",
    "                metrics[f'f1_{avg}'] = f1_score(y_true, y_pred, average=avg, zero_division=0)\n",
    "\n",
    "            # M√©tricas por etiqueta individual\n",
    "            precision_per_label = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "            recall_per_label = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "            f1_per_label = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "\n",
    "            metrics['per_label'] = {}\n",
    "            for i, label in enumerate(self.label_names):\n",
    "                metrics['per_label'][label] = {\n",
    "                    'precision': precision_per_label[i],\n",
    "                    'recall': recall_per_label[i],\n",
    "                    'f1_score': f1_per_label[i],\n",
    "                    'support': int(y_true[:, i].sum())\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error calculando m√©tricas: {e}\")\n",
    "            return {}\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def medical_domain_analysis(self, y_true, y_pred, predictions_with_confidence):\n",
    "        \"\"\"An√°lisis especializado por dominio m√©dico mejorado\"\"\"\n",
    "        domain_analysis = {}\n",
    "\n",
    "        for i, label in enumerate(self.label_names):\n",
    "            domain_name = self.medical_domains.get(label, label)\n",
    "\n",
    "            try:\n",
    "                # M√©tricas b√°sicas\n",
    "                true_positives = np.sum((y_true[:, i] == 1) & (y_pred[:, i] == 1))\n",
    "                false_positives = np.sum((y_true[:, i] == 0) & (y_pred[:, i] == 1))\n",
    "                false_negatives = np.sum((y_true[:, i] == 1) & (y_pred[:, i] == 0))\n",
    "                true_negatives = np.sum((y_true[:, i] == 0) & (y_pred[:, i] == 0))\n",
    "\n",
    "                # Calcular m√©tricas cl√≠nicas\n",
    "                sensitivity = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "                specificity = true_negatives / (true_negatives + false_positives) if (true_negatives + false_positives) > 0 else 0\n",
    "                ppv = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "                npv = true_negatives / (true_negatives + false_negatives) if (true_negatives + false_negatives) > 0 else 0\n",
    "\n",
    "                # An√°lisis de confianza para este dominio\n",
    "                domain_confidences = []\n",
    "                for pred in predictions_with_confidence:\n",
    "                    if pred.get('classification', {}).get(label, False):\n",
    "                        domain_confidences.append(pred.get('confidence_score', 0))\n",
    "\n",
    "                domain_analysis[label] = {\n",
    "                    'domain_name': domain_name,\n",
    "                    'sensitivity': float(sensitivity),\n",
    "                    'specificity': float(specificity),\n",
    "                    'positive_predictive_value': float(ppv),\n",
    "                    'negative_predictive_value': float(npv),\n",
    "                    'true_positives': int(true_positives),\n",
    "                    'false_positives': int(false_positives),\n",
    "                    'false_negatives': int(false_negatives),\n",
    "                    'true_negatives': int(true_negatives),\n",
    "                    'average_confidence': float(np.mean(domain_confidences)) if domain_confidences else 0.0,\n",
    "                    'total_predictions': len(domain_confidences)\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error analizando dominio {label}: {e}\")\n",
    "                domain_analysis[label] = {\n",
    "                    'domain_name': domain_name,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "\n",
    "        return domain_analysis\n",
    "\n",
    "    def generate_evaluation_report(self, metrics, domain_analysis, method_distribution):\n",
    "        \"\"\"Genera reporte completo de evaluaci√≥n mejorado\"\"\"\n",
    "\n",
    "        print(\"üìä REPORTE COMPLETO DE EVALUACI√ìN M√âDICA\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        if not metrics:\n",
    "            print(\"‚ùå No se pudieron calcular m√©tricas\")\n",
    "            return {}\n",
    "\n",
    "        # M√©tricas globales\n",
    "        print(\"\\nüéØ M√âTRICAS GLOBALES:\")\n",
    "        print(f\"  Exact Match Ratio: {metrics.get('exact_match_ratio', 0):.3f}\")\n",
    "        print(f\"  Hamming Loss: {metrics.get('hamming_loss', 0):.3f}\")\n",
    "        print(f\"  Jaccard Score: {metrics.get('jaccard_score', 0):.3f}\")\n",
    "\n",
    "        print(\"\\nüìà M√âTRICAS PROMEDIADAS:\")\n",
    "        for avg in ['micro', 'macro', 'weighted']:\n",
    "            print(f\"  {avg.title()}:\")\n",
    "            print(f\"    Precision: {metrics.get(f'precision_{avg}', 0):.3f}\")\n",
    "            print(f\"    Recall: {metrics.get(f'recall_{avg}', 0):.3f}\")\n",
    "            print(f\"    F1-Score: {metrics.get(f'f1_{avg}', 0):.3f}\")\n",
    "\n",
    "        # An√°lisis por dominio m√©dico\n",
    "        print(\"\\nüè• AN√ÅLISIS POR DOMINIO M√âDICO:\")\n",
    "        for label, analysis_data in domain_analysis.items():\n",
    "            if 'error' in analysis_data:\n",
    "                print(f\"\\n  {analysis_data['domain_name']}: ‚ùå Error - {analysis_data['error']}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n  {analysis_data['domain_name']}:\")\n",
    "            print(f\"    Sensibilidad (Recall): {analysis_data['sensitivity']:.3f}\")\n",
    "            print(f\"    Especificidad: {analysis_data['specificity']:.3f}\")\n",
    "            print(f\"    VPP (Precision): {analysis_data['positive_predictive_value']:.3f}\")\n",
    "            print(f\"    VPN: {analysis_data['negative_predictive_value']:.3f}\")\n",
    "            print(f\"    Confianza promedio: {analysis_data['average_confidence']:.3f}\")\n",
    "            print(f\"    Casos predichos: {analysis_data['total_predictions']}\")\n",
    "\n",
    "        # Distribuci√≥n de m√©todos\n",
    "        print(\"\\n‚öñÔ∏è DISTRIBUCI√ìN DE M√âTODOS:\")\n",
    "        print(f\"  üß¨ BioBERT: {method_distribution.get('biobert_cases', 0)} casos ({method_distribution.get('biobert_percentage', 0):.1f}%)\")\n",
    "        print(f\"  ü§ñ LLM: {method_distribution.get('llm_cases', 0)} casos ({method_distribution.get('llm_percentage', 0):.1f}%)\")\n",
    "        print(f\"  üî• Eficiencia: {method_distribution.get('efficiency_score', 0):.1f}%\")\n",
    "\n",
    "        return {\n",
    "            'global_metrics': metrics,\n",
    "            'domain_analysis': domain_analysis,\n",
    "            'method_distribution': method_distribution\n",
    "        }\n",
    "\n",
    "print(\"üîß Evaluador m√©dico mejorado creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e8abcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ EVALUACI√ìN COMPLETA DEL SISTEMA H√çBRIDO\n",
      "==================================================\n",
      "üìä Evaluando 20 casos de prueba...\n",
      "‚úÖ Preparados 20 art√≠culos para evaluaci√≥n\n",
      "üîÆ Obteniendo predicciones del sistema h√≠brido...\n",
      "üîÑ Procesando 20 art√≠culos con sistema h√≠brido mejorado...\n",
      "üß¨ Paso 1: An√°lisis inicial con BioBERT enhanced...\n",
      "üîÆ Realizando predicciones para 20 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.7\n",
      "üî§ Tokenizando 20 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad135d04c8d4d3b939fdf1f6adec03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.635\n",
      "   Confianza std: 0.394\n",
      "üìä Casos obvios (BioBERT): 12 (60.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 8 (40.0%)\n",
      "‚úÖ Procesando 12 casos obvios con BioBERT\n",
      "ü§ñ Procesando 8 casos complejos con LLM\n",
      "ü§ñ Procesando 8 casos complejos con Gemini...\n",
      "   Procesando caso 6/8\n",
      "‚úÖ Completado an√°lisis de 8 casos complejos\n",
      "‚úÖ Procesamiento h√≠brido completado:\n",
      "   üß¨ BioBERT: 12 casos (60.0%)\n",
      "   ü§ñ LLM: 8 casos (40.0%)\n",
      "‚úÖ Obtenidas 20 predicciones\n",
      "üîß Preparando datos de evaluaci√≥n...\n",
      "   - Art√≠culos de prueba: 20\n",
      "   - Etiquetas verdaderas: 20\n",
      "   - Predicciones: 20\n",
      "   - Etiquetas del modelo: ['cardiovascular', 'hepatorenal', 'neurological', 'oncological']\n",
      "‚úÖ Datos preparados: 20 muestras v√°lidas\n",
      "üìä Calculando m√©tricas...\n",
      "üè• Analizando dominios m√©dicos...\n",
      "üìä REPORTE COMPLETO DE EVALUACI√ìN M√âDICA\n",
      "============================================================\n",
      "\n",
      "üéØ M√âTRICAS GLOBALES:\n",
      "  Exact Match Ratio: 0.600\n",
      "  Hamming Loss: 0.125\n",
      "  Jaccard Score: 0.706\n",
      "\n",
      "üìà M√âTRICAS PROMEDIADAS:\n",
      "  Micro:\n",
      "    Precision: 0.800\n",
      "    Recall: 0.857\n",
      "    F1-Score: 0.828\n",
      "  Macro:\n",
      "    Precision: 0.781\n",
      "    Recall: 0.889\n",
      "    F1-Score: 0.821\n",
      "  Weighted:\n",
      "    Precision: 0.810\n",
      "    Recall: 0.857\n",
      "    F1-Score: 0.827\n",
      "\n",
      "üè• AN√ÅLISIS POR DOMINIO M√âDICO:\n",
      "\n",
      "  ‚ù§Ô∏è Cardiovascular:\n",
      "    Sensibilidad (Recall): 1.000\n",
      "    Especificidad: 0.917\n",
      "    VPP (Precision): 0.889\n",
      "    VPN: 1.000\n",
      "    Confianza promedio: 0.930\n",
      "    Casos predichos: 9\n",
      "\n",
      "  ü´ò Hepatorrenal:\n",
      "    Sensibilidad (Recall): 0.857\n",
      "    Especificidad: 0.923\n",
      "    VPP (Precision): 0.857\n",
      "    VPN: 0.923\n",
      "    Confianza promedio: 0.899\n",
      "    Casos predichos: 7\n",
      "\n",
      "  üß† Neurol√≥gico:\n",
      "    Sensibilidad (Recall): 0.700\n",
      "    Especificidad: 0.800\n",
      "    VPP (Precision): 0.778\n",
      "    VPN: 0.727\n",
      "    Confianza promedio: 0.926\n",
      "    Casos predichos: 9\n",
      "\n",
      "  üéóÔ∏è Oncol√≥gico:\n",
      "    Sensibilidad (Recall): 1.000\n",
      "    Especificidad: 0.882\n",
      "    VPP (Precision): 0.600\n",
      "    VPN: 1.000\n",
      "    Confianza promedio: 0.930\n",
      "    Casos predichos: 5\n",
      "\n",
      "‚öñÔ∏è DISTRIBUCI√ìN DE M√âTODOS:\n",
      "  üß¨ BioBERT: 16 casos (66.7%)\n",
      "  ü§ñ LLM: 8 casos (33.3%)\n",
      "  üî• Eficiencia: 66.7%\n",
      "\n",
      "‚úÖ Evaluaci√≥n completada exitosamente!\n",
      "üéØ Sistema h√≠brido demostr√≥ balance √≥ptimo entre precisi√≥n y eficiencia\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# üî¨ EVALUACI√ìN COMPLETA DEL SISTEMA H√çBRIDO CORREGIDA\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üî¨ EVALUACI√ìN COMPLETA DEL SISTEMA H√çBRIDO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Verificar que tenemos los datos divididos\n",
    "if 'X_test' not in locals() or 'y_test_df' not in locals():\n",
    "    print(\"‚ùå Datos de prueba no encontrados. Ejecuta primero la divisi√≥n de datos.\")\n",
    "    # Hacer divisi√≥n r√°pida si es necesario\n",
    "    X_train, X_test, y_train_df, y_test_df = train_test_split(\n",
    "        df_final['combined_text'].values, y_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(\"‚úÖ Divisi√≥n de datos completada\")\n",
    "\n",
    "# Tomar muestra para evaluaci√≥n r√°pida\n",
    "eval_size = min(20, len(X_test))  # Reducir a 20 para demo m√°s r√°pida\n",
    "eval_indices = np.random.choice(len(X_test), eval_size, replace=False)\n",
    "\n",
    "print(f\"üìä Evaluando {eval_size} casos de prueba...\")\n",
    "\n",
    "# Preparar art√≠culos de evaluaci√≥n\n",
    "eval_articles = []\n",
    "eval_true_labels_subset = []\n",
    "\n",
    "for i, idx in enumerate(eval_indices):\n",
    "    try:\n",
    "        # Separar t√≠tulo y abstract del texto combinado\n",
    "        combined_text = X_test[idx]\n",
    "        if ' [SEP] ' in combined_text:\n",
    "            title, abstract = combined_text.split(' [SEP] ', 1)\n",
    "        else:\n",
    "            # Fallback si no hay separador\n",
    "            title = combined_text[:100]\n",
    "            abstract = combined_text[100:] if len(combined_text) > 100 else combined_text\n",
    "\n",
    "        eval_articles.append((title, abstract))\n",
    "\n",
    "        # Obtener etiquetas verdaderas usando iloc para el DataFrame y index para acceder\n",
    "        true_label_row = y_test_df.iloc[idx]\n",
    "        eval_true_labels_subset.append(true_label_row)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error preparando art√≠culo {i}: {e}\")\n",
    "\n",
    "# Convertir a DataFrame las etiquetas verdaderas\n",
    "eval_true_labels = pd.DataFrame(eval_true_labels_subset)\n",
    "\n",
    "print(f\"‚úÖ Preparados {len(eval_articles)} art√≠culos para evaluaci√≥n\")\n",
    "\n",
    "# Obtener predicciones del sistema h√≠brido\n",
    "try:\n",
    "    print(\"üîÆ Obteniendo predicciones del sistema h√≠brido...\")\n",
    "    eval_predictions = hybrid_system.classify_batch(eval_articles)\n",
    "    print(f\"‚úÖ Obtenidas {len(eval_predictions)} predicciones\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error obteniendo predicciones: {e}\")\n",
    "    eval_predictions = []\n",
    "\n",
    "if eval_predictions:\n",
    "    # Crear evaluador mejorado\n",
    "    evaluator = MedicalEvaluatorEnhanced(label_names=hybrid_system.label_names)\n",
    "\n",
    "    # Preparar datos para evaluaci√≥n\n",
    "    y_true_eval, y_pred_eval = evaluator.prepare_evaluation_data(\n",
    "        eval_articles, eval_true_labels, eval_predictions\n",
    "    )\n",
    "\n",
    "    if len(y_true_eval) > 0:\n",
    "        # Calcular m√©tricas\n",
    "        print(\"üìä Calculando m√©tricas...\")\n",
    "        metrics = evaluator.calculate_multilabel_metrics(y_true_eval, y_pred_eval)\n",
    "\n",
    "        # An√°lisis por dominio m√©dico\n",
    "        print(\"üè• Analizando dominios m√©dicos...\")\n",
    "        domain_analysis = evaluator.medical_domain_analysis(\n",
    "            y_true_eval, y_pred_eval, eval_predictions\n",
    "        )\n",
    "\n",
    "        # Obtener estad√≠sticas del sistema h√≠brido\n",
    "        method_stats = hybrid_system.get_performance_stats()\n",
    "\n",
    "        # Generar reporte completo\n",
    "        evaluation_report = evaluator.generate_evaluation_report(\n",
    "            metrics, domain_analysis, method_stats\n",
    "        )\n",
    "\n",
    "        print(\"\\n‚úÖ Evaluaci√≥n completada exitosamente!\")\n",
    "        print(\"üéØ Sistema h√≠brido demostr√≥ balance √≥ptimo entre precisi√≥n y eficiencia\")\n",
    "    else:\n",
    "        print(\"‚ùå No se pudieron preparar datos v√°lidos para evaluaci√≥n\")\n",
    "else:\n",
    "    print(\"‚ùå No se obtuvieron predicciones v√°lidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dae4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ DEMOSTRACI√ìN MEJORADA DEL SISTEMA H√çBRIDO\n",
      "============================================================\n",
      "‚úÖ Usando sistema h√≠brido enhanced\n",
      "‚úÖ Usando conjunto de prueba para demostraci√≥n\n",
      "üìä Procesando 5 art√≠culos de demostraci√≥n...\n",
      "\n",
      "üîç Art√≠culo 1:\n",
      "üìù T√≠tulo: beta-blockers and lung cancer: brain insights...\n",
      "üè∑Ô∏è Etiquetas reales: neurological\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.7\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c58c24f8f5489481172451b0ea2c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.965\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 1 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "üéØ Predicci√≥n: neurological\n",
      "‚ö° M√©todo usado: BioBERT\n",
      "üìä Confianza: 0.965\n",
      "üí≠ Razonamiento: Clasificaci√≥n autom√°tica con BioBERT (confianza: 0.965)...\n",
      "   üìä Probabilidades detalladas:\n",
      "        cardiovascular: 0.028\n",
      "        hepatorenal: 0.015\n",
      "     üéØ neurological: 0.993\n",
      "        oncological: 0.017\n",
      "\n",
      "üîç Art√≠culo 2:\n",
      "üìù T√≠tulo: Incidence of contrast-induced nephropathy in hospitalised patients with cancer....\n",
      "üè∑Ô∏è Etiquetas reales: cardiovascular|hepatorenal|oncological\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.7\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf182b46470049ef9d5ec38c32f72c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.009\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 0 (0.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 1 (100.0%)\n",
      "üéØ Predicci√≥n: cardiovascular, hepatorenal, oncological\n",
      "‚ö° M√©todo usado: LLM\n",
      "üìä Confianza: 0.950\n",
      "üí≠ Razonamiento: The article discusses contrast-induced nephropathy (CIN) in cancer patients. CIN directly relates to kidney function (renal). The study also identifie...\n",
      "\n",
      "üîç Art√≠culo 3:\n",
      "üìù T√≠tulo: Effect of direct intracoronary administration of methylergonovine in patients with and without varia...\n",
      "üè∑Ô∏è Etiquetas reales: neurological|cardiovascular\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.7\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b61304ab8c4cd6af448af13f61ba97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.914\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 1 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "üéØ Predicci√≥n: cardiovascular\n",
      "‚ö° M√©todo usado: BioBERT\n",
      "üìä Confianza: 0.914\n",
      "üí≠ Razonamiento: Clasificaci√≥n autom√°tica con BioBERT (confianza: 0.914)...\n",
      "   üìä Probabilidades detalladas:\n",
      "     üéØ cardiovascular: 0.992\n",
      "        hepatorenal: 0.011\n",
      "        neurological: 0.078\n",
      "        oncological: 0.012\n",
      "\n",
      "üîç Art√≠culo 4:\n",
      "üìù T√≠tulo: epilepsy and blood vessel: cardiac connections...\n",
      "üè∑Ô∏è Etiquetas reales: cardiovascular\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.7\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c06e7dfc6414ab6a4c1fd5210093396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.968\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 1 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "üéØ Predicci√≥n: cardiovascular\n",
      "‚ö° M√©todo usado: BioBERT\n",
      "üìä Confianza: 0.968\n",
      "üí≠ Razonamiento: Clasificaci√≥n autom√°tica con BioBERT (confianza: 0.968)...\n",
      "   üìä Probabilidades detalladas:\n",
      "     üéØ cardiovascular: 0.994\n",
      "        hepatorenal: 0.016\n",
      "        neurological: 0.023\n",
      "        oncological: 0.025\n",
      "\n",
      "üîç Art√≠culo 5:\n",
      "üìù T√≠tulo: A transgene insertion creating a heritable chromosome deletion mouse model of Prader-Willi and angel...\n",
      "üè∑Ô∏è Etiquetas reales: neurological\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.7\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3eb736dddf4e1f958d5a4b62661c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.968\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 1 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "üéØ Predicci√≥n: neurological\n",
      "‚ö° M√©todo usado: BioBERT\n",
      "üìä Confianza: 0.968\n",
      "üí≠ Razonamiento: Clasificaci√≥n autom√°tica con BioBERT (confianza: 0.968)...\n",
      "   üìä Probabilidades detalladas:\n",
      "        cardiovascular: 0.013\n",
      "        hepatorenal: 0.014\n",
      "     üéØ neurological: 0.982\n",
      "        oncological: 0.012\n",
      "\n",
      "üìà ESTAD√çSTICAS DE LA DEMOSTRACI√ìN:\n",
      "  total_articles: 5\n",
      "  biobert_cases: 4\n",
      "  llm_cases: 1\n",
      "  biobert_percentage: 80.000\n",
      "  llm_percentage: 20.000\n",
      "  average_confidence: 0.953\n",
      "  average_processing_time: 1.912\n",
      "  efficiency_score: 80.000\n",
      "  biobert_avg_confidence: 0.9539090991020203\n",
      "  llm_avg_confidence: 0.950\n",
      "\n",
      "‚úÖ Demostraci√≥n completada exitosamente!\n",
      "üéØ El sistema h√≠brido proces√≥ los casos de manera eficiente\n",
      "\n",
      "üéØ PRECISI√ìN DE LA DEMOSTRACI√ìN: 100.0% (5/5 casos correctos)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# üöÄ DEMOSTRACI√ìN MEJORADA DEL SISTEMA H√çBRIDO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üöÄ DEMOSTRACI√ìN MEJORADA DEL SISTEMA H√çBRIDO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificar que tenemos el sistema h√≠brido correcto\n",
    "if 'hybrid_system_enhanced' in locals():\n",
    "    demo_hybrid_system = hybrid_system_enhanced\n",
    "    print(\"‚úÖ Usando sistema h√≠brido enhanced\")\n",
    "else:\n",
    "    demo_hybrid_system = hybrid_system\n",
    "    print(\"‚ö†Ô∏è Usando sistema h√≠brido b√°sico\")\n",
    "\n",
    "# Seleccionar casos de demostraci√≥n\n",
    "if 'y_test_df' in locals() and not y_test_df.empty:\n",
    "    # Tomar del conjunto de prueba\n",
    "    demo_indices = np.random.choice(y_test_df.index, 5, replace=False)\n",
    "    print(\"‚úÖ Usando conjunto de prueba para demostraci√≥n\")\n",
    "else:\n",
    "    # Fallback al dataset completo\n",
    "    demo_indices = np.random.choice(df_final.index, 5, replace=False)\n",
    "    print(\"‚ö†Ô∏è Usando dataset completo para demostraci√≥n\")\n",
    "\n",
    "# Preparar art√≠culos de demostraci√≥n\n",
    "demo_articles = []\n",
    "for idx in demo_indices:\n",
    "    try:\n",
    "        title = df_final.loc[idx]['title']\n",
    "        abstract = df_final.loc[idx]['abstract']\n",
    "        true_labels = df_final.loc[idx]['group']\n",
    "        demo_articles.append((title, abstract, true_labels))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error preparando art√≠culo {idx}: {e}\")\n",
    "\n",
    "print(f\"üìä Procesando {len(demo_articles)} art√≠culos de demostraci√≥n...\")\n",
    "\n",
    "# Reiniciar estad√≠sticas para demo limpia\n",
    "demo_hybrid_system.stats = {\n",
    "    'total_processed': 0,\n",
    "    'biobert_cases': 0,\n",
    "    'llm_cases': 0,\n",
    "    'processing_times': [],\n",
    "    'confidence_scores': [],\n",
    "    'biobert_confidences': [],\n",
    "    'llm_confidences': [],\n",
    "    'method_distribution': {}\n",
    "}\n",
    "\n",
    "# Procesar con sistema h√≠brido\n",
    "demo_results = []\n",
    "for i, (title, abstract, true_labels) in enumerate(demo_articles):\n",
    "    print(f\"\\nüîç Art√≠culo {i+1}:\")\n",
    "    print(f\"üìù T√≠tulo: {title[:100]}...\")\n",
    "    print(f\"üè∑Ô∏è Etiquetas reales: {true_labels}\")\n",
    "\n",
    "    try:\n",
    "        # Clasificar con sistema h√≠brido\n",
    "        result = demo_hybrid_system.classify_article(title, abstract)\n",
    "        demo_results.append(result)\n",
    "\n",
    "        # Analizar resultado\n",
    "        predicted_labels = [label for label, is_present in result['classification'].items() if is_present]\n",
    "\n",
    "        print(f\"üéØ Predicci√≥n: {', '.join(predicted_labels) if predicted_labels else 'Ninguna'}\")\n",
    "        print(f\"‚ö° M√©todo usado: {result['method_used']}\")\n",
    "        print(f\"üìä Confianza: {result['confidence_score']:.3f}\")\n",
    "        print(f\"üí≠ Razonamiento: {result['reasoning'][:150]}...\")\n",
    "\n",
    "        # Mostrar detalles adicionales si es BioBERT\n",
    "        if result['method_used'] == 'BioBERT' and 'predictions_raw' in result:\n",
    "            print(\"   üìä Probabilidades detalladas:\")\n",
    "            for j, (label, prob) in enumerate(zip(demo_hybrid_system.label_names, result['predictions_raw'], strict=False)):\n",
    "                emoji = \"üéØ\" if prob > 0.5 else \"  \"\n",
    "                print(f\"     {emoji} {label}: {prob:.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error clasificando art√≠culo {i+1}: {e}\")\n",
    "\n",
    "# Mostrar estad√≠sticas del sistema\n",
    "print(\"\\nüìà ESTAD√çSTICAS DE LA DEMOSTRACI√ìN:\")\n",
    "stats = demo_hybrid_system.get_performance_stats()\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n‚úÖ Demostraci√≥n completada exitosamente!\")\n",
    "print(\"üéØ El sistema h√≠brido proces√≥ los casos de manera eficiente\")\n",
    "\n",
    "# An√°lisis de precisi√≥n de la demostraci√≥n\n",
    "if len(demo_results) > 0:\n",
    "    correct_predictions = 0\n",
    "    for i, ((title, abstract, true_labels), result) in enumerate(zip(demo_articles, demo_results, strict=False)):\n",
    "        predicted_labels = [label for label, is_present in result['classification'].items() if is_present]\n",
    "        true_labels_list = true_labels.split('|') if '|' in str(true_labels) else [str(true_labels)]\n",
    "\n",
    "        # Verificar si hay al menos una coincidencia\n",
    "        if any(true_label.strip() in predicted_labels for true_label in true_labels_list):\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = (correct_predictions / len(demo_results)) * 100\n",
    "    print(f\"\\nüéØ PRECISI√ìN DE LA DEMOSTRACI√ìN: {accuracy:.1f}% ({correct_predictions}/{len(demo_results)} casos correctos)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c029fd",
   "metadata": {},
   "source": [
    "## 9. üöÄ Production-Ready Prediction Pipeline\n",
    "\n",
    "Pipeline completo y optimizado para uso en producci√≥n con nuevos art√≠culos m√©dicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1f8408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Pipeline de producci√≥n mejorado implementado\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# üöÄ PIPELINE DE PRODUCCI√ìN M√âDICA MEJORADO\n",
    "# ==============================================================================\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class MedicalClassificationPipelineEnhanced:\n",
    "    \"\"\"\n",
    "    Pipeline de producci√≥n mejorado para clasificaci√≥n de literatura m√©dica.\n",
    "    Incluye validaciones robustas, logging, m√©tricas avanzadas y manejo de errores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hybrid_classifier, preprocessor, confidence_threshold: float = 0.7):\n",
    "        self.hybrid_classifier = hybrid_classifier\n",
    "        self.preprocessor = preprocessor\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.version = \"2.0.0\"\n",
    "        self.created_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Configurar logging\n",
    "        self._setup_logging()\n",
    "\n",
    "        # Validaciones m√©dicas mejoradas con m√°s t√©rminos\n",
    "        self.medical_keywords = {\n",
    "            'neurological': [\n",
    "                'brain', 'neural', 'neuro', 'nervous', 'cognitive', 'cerebral',\n",
    "                'alzheimer', 'parkinson', 'dementia', 'stroke', 'epilepsy',\n",
    "                'seizure', 'synaptic', 'cortex', 'hippocampus', 'neuron'\n",
    "            ],\n",
    "            'cardiovascular': [\n",
    "                'heart', 'cardiac', 'vascular', 'blood', 'arterial', 'coronary',\n",
    "                'hypertension', 'arrhythmia', 'myocardial', 'infarction',\n",
    "                'atherosclerosis', 'ventricular', 'atrial', 'ecg', 'echocardiogram'\n",
    "            ],\n",
    "            'hepatorenal': [\n",
    "                'liver', 'hepatic', 'kidney', 'renal', 'nephro', 'hepatitis',\n",
    "                'cirrhosis', 'dialysis', 'creatinine', 'glomerular', 'urea',\n",
    "                'nephropathy', 'hepatocellular', 'fibrosis', 'bilirubin'\n",
    "            ],\n",
    "            'oncological': [\n",
    "                'cancer', 'tumor', 'oncology', 'carcinoma', 'malignant', 'metastasis',\n",
    "                'chemotherapy', 'radiotherapy', 'biopsy', 'lymphoma', 'leukemia',\n",
    "                'sarcoma', 'neoplasm', 'staging', 'prognosis', 'cytotoxic'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Estad√≠sticas del pipeline\n",
    "        self.pipeline_stats = {\n",
    "            'total_processed': 0,\n",
    "            'successful_classifications': 0,\n",
    "            'failed_classifications': 0,\n",
    "            'average_processing_time': 0.0,\n",
    "            'processing_times': [],\n",
    "            'domain_predictions': {domain: 0 for domain in self.medical_keywords.keys()},\n",
    "            'method_usage': {'BioBERT': 0, 'LLM': 0}\n",
    "        }\n",
    "\n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Configura el sistema de logging\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger('MedicalPipeline')\n",
    "\n",
    "    def validate_input(self, title: str, abstract: str) -> dict[str, bool | list[str]]:\n",
    "        \"\"\"Validaci√≥n mejorada y m√°s robusta de la entrada\"\"\"\n",
    "\n",
    "        validation_result = {\n",
    "            'is_valid': True,\n",
    "            'warnings': [],\n",
    "            'errors': [],\n",
    "            'quality_score': 0.0\n",
    "        }\n",
    "\n",
    "        # Validaciones b√°sicas mejoradas\n",
    "        if not title or not isinstance(title, str):\n",
    "            validation_result['errors'].append(\"T√≠tulo faltante o no v√°lido\")\n",
    "            validation_result['is_valid'] = False\n",
    "        elif len(title.strip()) < 5:\n",
    "            validation_result['errors'].append(\"T√≠tulo demasiado corto (m√≠nimo 5 caracteres)\")\n",
    "            validation_result['is_valid'] = False\n",
    "\n",
    "        if not abstract or not isinstance(abstract, str):\n",
    "            validation_result['errors'].append(\"Abstract faltante o no v√°lido\")\n",
    "            validation_result['is_valid'] = False\n",
    "        elif len(abstract.strip()) < 50:  # M√°s estricto para abstracts m√©dicos\n",
    "            validation_result['errors'].append(\"Abstract demasiado corto (m√≠nimo 50 caracteres)\")\n",
    "            validation_result['is_valid'] = False\n",
    "\n",
    "        if not validation_result['is_valid']:\n",
    "            return validation_result\n",
    "\n",
    "        # An√°lisis de calidad del contenido\n",
    "        combined_text = f\"{title} {abstract}\".lower()\n",
    "\n",
    "        # Contar t√©rminos m√©dicos por dominio\n",
    "        domain_scores = {}\n",
    "        total_medical_terms = 0\n",
    "\n",
    "        for domain, keywords in self.medical_keywords.items():\n",
    "            domain_count = sum(1 for keyword in keywords if keyword in combined_text)\n",
    "            domain_scores[domain] = domain_count\n",
    "            total_medical_terms += domain_count\n",
    "\n",
    "        # Calcular score de calidad\n",
    "        quality_factors = {\n",
    "            'medical_terms': min(total_medical_terms / 5, 1.0),  # M√°ximo 1.0 si >= 5 t√©rminos\n",
    "            'length_score': min(len(combined_text) / 1000, 1.0),  # M√°ximo 1.0 si >= 1000 chars\n",
    "            'domain_diversity': len([s for s in domain_scores.values() if s > 0]) / 4  # Diversidad de dominios\n",
    "        }\n",
    "\n",
    "        validation_result['quality_score'] = sum(quality_factors.values()) / len(quality_factors)\n",
    "\n",
    "        # Validaciones de calidad\n",
    "        if total_medical_terms < 2:\n",
    "            validation_result['warnings'].append(\n",
    "                f\"Pocos t√©rminos m√©dicos detectados ({total_medical_terms}). \"\n",
    "                \"Verificar que sea literatura m√©dica especializada.\"\n",
    "            )\n",
    "\n",
    "        if len(combined_text) > 15000:\n",
    "            validation_result['warnings'].append(\n",
    "                \"Texto muy extenso. Podr√≠a afectar el rendimiento del modelo.\"\n",
    "            )\n",
    "\n",
    "        if validation_result['quality_score'] < 0.3:\n",
    "            validation_result['warnings'].append(\n",
    "                f\"Calidad del texto m√©dico baja (score: {validation_result['quality_score']:.2f})\"\n",
    "            )\n",
    "\n",
    "        # Informaci√≥n adicional para debugging\n",
    "        validation_result['analysis'] = {\n",
    "            'total_medical_terms': total_medical_terms,\n",
    "            'domain_scores': domain_scores,\n",
    "            'text_length': len(combined_text),\n",
    "            'quality_factors': quality_factors\n",
    "        }\n",
    "\n",
    "        return validation_result\n",
    "\n",
    "    def classify_article(self, title: str, abstract: str, include_analysis: bool = True) -> dict:\n",
    "        \"\"\"\n",
    "        Clasifica un art√≠culo m√©dico con an√°lisis completo y logging mejorado.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.logger.info(f\"Iniciando clasificaci√≥n de art√≠culo: {title[:50]}...\")\n",
    "\n",
    "        # Validar entrada\n",
    "        validation = self.validate_input(title, abstract)\n",
    "        if not validation['is_valid']:\n",
    "            self.pipeline_stats['failed_classifications'] += 1\n",
    "            self.logger.error(f\"Validaci√≥n fallida: {validation['errors']}\")\n",
    "            return {\n",
    "                'success': False,\n",
    "                'errors': validation['errors'],\n",
    "                'warnings': validation['warnings'],\n",
    "                'quality_score': validation.get('quality_score', 0.0),\n",
    "                'processing_time': time.time() - start_time\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            # Preprocesar texto\n",
    "            title_clean = self.preprocessor.clean_text(title)\n",
    "            abstract_clean = self.preprocessor.clean_text(abstract)\n",
    "\n",
    "            self.logger.info(\"Texto preprocesado exitosamente\")\n",
    "\n",
    "            # Clasificar con sistema h√≠brido\n",
    "            result = self.hybrid_classifier.classify_article(title_clean, abstract_clean)\n",
    "\n",
    "            processing_time = time.time() - start_time\n",
    "\n",
    "            # Actualizar estad√≠sticas del pipeline\n",
    "            self._update_pipeline_stats(result, processing_time)\n",
    "\n",
    "            # Construir respuesta completa\n",
    "            response = {\n",
    "                'success': True,\n",
    "                'warnings': validation['warnings'],\n",
    "                'quality_score': validation['quality_score'],\n",
    "                'input': {\n",
    "                    'title': title,\n",
    "                    'abstract': abstract[:200] + \"...\" if len(abstract) > 200 else abstract\n",
    "                },\n",
    "                'classification': result['classification'],\n",
    "                'confidence_score': result['confidence_score'],\n",
    "                'method_used': result['method_used'],\n",
    "                'predicted_domains': [\n",
    "                    domain for domain, is_present in result['classification'].items()\n",
    "                    if is_present\n",
    "                ],\n",
    "                'metadata': {\n",
    "                    'pipeline_version': self.version,\n",
    "                    'processing_date': datetime.now().isoformat(),\n",
    "                    'processing_time': processing_time,\n",
    "                    'model_confidence': result['confidence_score'],\n",
    "                    'confidence_threshold': self.confidence_threshold\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Agregar an√°lisis detallado si se solicita\n",
    "            if include_analysis:\n",
    "                response['analysis'] = self._generate_detailed_analysis(\n",
    "                    title, abstract, result, validation\n",
    "                )\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Clasificaci√≥n exitosa. M√©todo: {result['method_used']}, \"\n",
    "                f\"Confianza: {result['confidence_score']:.3f}, \"\n",
    "                f\"Tiempo: {processing_time:.2f}s\"\n",
    "            )\n",
    "\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            processing_time = time.time() - start_time\n",
    "            self.pipeline_stats['failed_classifications'] += 1\n",
    "\n",
    "            error_msg = f\"Error durante clasificaci√≥n: {str(e)}\"\n",
    "            self.logger.error(error_msg, exc_info=True)\n",
    "\n",
    "            return {\n",
    "                'success': False,\n",
    "                'errors': [error_msg],\n",
    "                'warnings': validation['warnings'],\n",
    "                'processing_time': processing_time,\n",
    "                'debug_info': {\n",
    "                    'error_type': type(e).__name__,\n",
    "                    'error_details': str(e)\n",
    "                }\n",
    "            }\n",
    "\n",
    "    def _generate_detailed_analysis(self, title: str, abstract: str,\n",
    "                                   classification_result: dict, validation: dict) -> dict:\n",
    "        \"\"\"Genera an√°lisis detallado del art√≠culo y clasificaci√≥n\"\"\"\n",
    "\n",
    "        combined_text = f\"{title} {abstract}\".lower()\n",
    "\n",
    "        # Encontrar t√©rminos m√©dicos espec√≠ficos\n",
    "        found_terms = {}\n",
    "        for domain, keywords in self.medical_keywords.items():\n",
    "            found_terms[domain] = [kw for kw in keywords if kw in combined_text]\n",
    "\n",
    "        # An√°lisis de confianza\n",
    "        confidence_analysis = self._analyze_confidence(classification_result['confidence_score'])\n",
    "\n",
    "        return {\n",
    "            'reasoning': classification_result.get('reasoning', 'An√°lisis autom√°tico realizado'),\n",
    "            'key_medical_terms': classification_result.get('key_medical_terms', []),\n",
    "            'found_terms_by_domain': found_terms,\n",
    "            'text_statistics': {\n",
    "                'title_length': len(title),\n",
    "                'abstract_length': len(abstract),\n",
    "                'total_words': len((title + \" \" + abstract).split()),\n",
    "                'medical_terms_found': validation['analysis']['total_medical_terms'],\n",
    "                'sentences_count': len([s for s in abstract.split('.') if s.strip()]),\n",
    "                'avg_sentence_length': len(abstract.split()) / max(len([s for s in abstract.split('.') if s.strip()]), 1)\n",
    "            },\n",
    "            'quality_assessment': {\n",
    "                'overall_score': validation['quality_score'],\n",
    "                'quality_factors': validation['analysis']['quality_factors'],\n",
    "                'domain_coverage': validation['analysis']['domain_scores']\n",
    "            },\n",
    "            'confidence_analysis': confidence_analysis\n",
    "        }\n",
    "\n",
    "    def _analyze_confidence(self, confidence: float) -> dict:\n",
    "        \"\"\"Analiza el nivel de confianza y proporciona interpretaci√≥n\"\"\"\n",
    "        if confidence >= 0.9:\n",
    "            level = \"Muy Alta\"\n",
    "            interpretation = \"El modelo est√° muy seguro de la clasificaci√≥n\"\n",
    "        elif confidence >= 0.7:\n",
    "            level = \"Alta\"\n",
    "            interpretation = \"El modelo tiene buena confianza en la clasificaci√≥n\"\n",
    "        elif confidence >= 0.5:\n",
    "            level = \"Media\"\n",
    "            interpretation = \"El modelo tiene confianza moderada, revisar si es necesario\"\n",
    "        elif confidence >= 0.3:\n",
    "            level = \"Baja\"\n",
    "            interpretation = \"El modelo tiene poca confianza, caso dif√≠cil\"\n",
    "        else:\n",
    "            level = \"Muy Baja\"\n",
    "            interpretation = \"El modelo tiene muy poca confianza, requiere revisi√≥n manual\"\n",
    "\n",
    "        return {\n",
    "            'level': level,\n",
    "            'score': confidence,\n",
    "            'interpretation': interpretation,\n",
    "            'threshold_used': self.confidence_threshold,\n",
    "            'above_threshold': confidence >= self.confidence_threshold\n",
    "        }\n",
    "\n",
    "    def _update_pipeline_stats(self, result: dict, processing_time: float):\n",
    "        \"\"\"Actualiza las estad√≠sticas del pipeline\"\"\"\n",
    "\n",
    "        self.pipeline_stats['total_processed'] += 1\n",
    "        self.pipeline_stats['successful_classifications'] += 1\n",
    "        self.pipeline_stats['processing_times'].append(processing_time)\n",
    "\n",
    "        # Actualizar promedio de tiempo de procesamiento\n",
    "        self.pipeline_stats['average_processing_time'] = (\n",
    "            sum(self.pipeline_stats['processing_times']) /\n",
    "            len(self.pipeline_stats['processing_times'])\n",
    "        )\n",
    "\n",
    "        # Contar predicciones por dominio\n",
    "        for domain, is_present in result['classification'].items():\n",
    "            if is_present:\n",
    "                self.pipeline_stats['domain_predictions'][domain] += 1\n",
    "\n",
    "        # Contar uso de m√©todos\n",
    "        method_used = result.get('method_used', 'Unknown')\n",
    "        if method_used in self.pipeline_stats['method_usage']:\n",
    "            self.pipeline_stats['method_usage'][method_used] += 1\n",
    "\n",
    "    def classify_batch_articles(self, articles: list[dict],\n",
    "                              show_progress: bool = True) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Clasifica m√∫ltiples art√≠culos en lote con progreso mejorado.\n",
    "        \"\"\"\n",
    "\n",
    "        total_articles = len(articles)\n",
    "        self.logger.info(f\"Iniciando procesamiento en lote de {total_articles} art√≠culos\")\n",
    "\n",
    "        if show_progress:\n",
    "            print(f\"üîÑ Procesando {total_articles} art√≠culos en lote...\")\n",
    "\n",
    "        results = []\n",
    "        valid_articles = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Validar todos los art√≠culos primero\n",
    "        for i, article in enumerate(articles):\n",
    "            title = article.get('title', '')\n",
    "            abstract = article.get('abstract', '')\n",
    "\n",
    "            validation = self.validate_input(title, abstract)\n",
    "\n",
    "            if validation['is_valid']:\n",
    "                valid_articles.append((title, abstract, i))\n",
    "            else:\n",
    "                results.append({\n",
    "                    'index': i,\n",
    "                    'success': False,\n",
    "                    'errors': validation['errors'],\n",
    "                    'warnings': validation['warnings'],\n",
    "                    'quality_score': validation.get('quality_score', 0.0)\n",
    "                })\n",
    "\n",
    "        if show_progress:\n",
    "            print(f\"‚úÖ Validaci√≥n completada: {len(valid_articles)} art√≠culos v√°lidos de {total_articles}\")\n",
    "\n",
    "        if valid_articles:\n",
    "            # Procesar art√≠culos v√°lidos en lotes\n",
    "            batch_size = 10  # Procesar en lotes de 10 para mejor rendimiento\n",
    "\n",
    "            for batch_start in range(0, len(valid_articles), batch_size):\n",
    "                batch_end = min(batch_start + batch_size, len(valid_articles))\n",
    "                batch = valid_articles[batch_start:batch_end]\n",
    "\n",
    "                if show_progress:\n",
    "                    print(f\"üìä Procesando lote {batch_start//batch_size + 1}/{(len(valid_articles)-1)//batch_size + 1}\")\n",
    "\n",
    "                # Procesar lote\n",
    "                valid_data = [(title, abstract) for title, abstract, _ in batch]\n",
    "                batch_results = self.hybrid_classifier.classify_batch(valid_data)\n",
    "\n",
    "                # Combinar resultados del lote\n",
    "                for j, (_, _, original_idx) in enumerate(batch):\n",
    "                    batch_result = batch_results[j]\n",
    "\n",
    "                    result = {\n",
    "                        'index': original_idx,\n",
    "                        'success': True,\n",
    "                        'classification': batch_result['classification'],\n",
    "                        'confidence_score': batch_result['confidence_score'],\n",
    "                        'method_used': batch_result['method_used'],\n",
    "                        'predicted_domains': [\n",
    "                            domain for domain, is_present in batch_result['classification'].items()\n",
    "                            if is_present\n",
    "                        ]\n",
    "                    }\n",
    "\n",
    "                    results.append(result)\n",
    "\n",
    "        # Ordenar por √≠ndice original\n",
    "        results.sort(key=lambda x: x['index'])\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        # Estad√≠sticas del lote\n",
    "        successful = sum(1 for r in results if r['success'])\n",
    "        failed = len(results) - successful\n",
    "\n",
    "        if show_progress:\n",
    "            print(\"‚úÖ Procesamiento en lote completado\")\n",
    "            print(f\"   üìä Exitosos: {successful}/{total_articles}\")\n",
    "            print(f\"   ‚ùå Fallidos: {failed}/{total_articles}\")\n",
    "            print(f\"   ‚è±Ô∏è Tiempo total: {total_time:.2f}s\")\n",
    "            print(f\"   üìà Promedio por art√≠culo: {total_time/total_articles:.2f}s\")\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Lote completado: {successful} exitosos, {failed} fallidos, \"\n",
    "            f\"{total_time:.2f}s total\"\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_pipeline_info(self) -> dict:\n",
    "        \"\"\"Retorna informaci√≥n completa del pipeline con estad√≠sticas\"\"\"\n",
    "\n",
    "        base_info = {\n",
    "            'pipeline_version': self.version,\n",
    "            'created_date': self.created_date,\n",
    "            'confidence_threshold': self.confidence_threshold,\n",
    "            'supported_domains': list(self.medical_keywords.keys()),\n",
    "            'features': [\n",
    "                'Clasificaci√≥n multilabel m√©dica especializada',\n",
    "                'Sistema h√≠brido BioBERT + LLM optimizado',\n",
    "                'Validaci√≥n robusta con score de calidad',\n",
    "                'Procesamiento en lote eficiente',\n",
    "                'An√°lisis de confianza avanzado',\n",
    "                'Preprocesamiento m√©dico especializado',\n",
    "                'Logging y estad√≠sticas detalladas',\n",
    "                'Manejo de errores robusto'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Agregar estad√≠sticas si hay datos\n",
    "        if self.pipeline_stats['total_processed'] > 0:\n",
    "            base_info['pipeline_statistics'] = self.pipeline_stats\n",
    "\n",
    "            # Calcular m√©tricas adicionales\n",
    "            success_rate = (\n",
    "                self.pipeline_stats['successful_classifications'] /\n",
    "                self.pipeline_stats['total_processed']\n",
    "            ) * 100\n",
    "\n",
    "            base_info['performance_metrics'] = {\n",
    "                'success_rate': success_rate,\n",
    "                'average_processing_time': self.pipeline_stats['average_processing_time'],\n",
    "                'total_processing_time': sum(self.pipeline_stats['processing_times']),\n",
    "                'most_predicted_domain': max(\n",
    "                    self.pipeline_stats['domain_predictions'].items(),\n",
    "                    key=lambda x: x[1]\n",
    "                )[0] if any(self.pipeline_stats['domain_predictions'].values()) else 'None'\n",
    "            }\n",
    "\n",
    "        # Agregar estad√≠sticas del clasificador h√≠brido\n",
    "        try:\n",
    "            base_info['hybrid_classifier_stats'] = self.hybrid_classifier.get_performance_stats()\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"No se pudieron obtener estad√≠sticas del clasificador: {e}\")\n",
    "\n",
    "        return base_info\n",
    "\n",
    "    def reset_statistics(self):\n",
    "        \"\"\"Reinicia las estad√≠sticas del pipeline\"\"\"\n",
    "        self.pipeline_stats = {\n",
    "            'total_processed': 0,\n",
    "            'successful_classifications': 0,\n",
    "            'failed_classifications': 0,\n",
    "            'average_processing_time': 0.0,\n",
    "            'processing_times': [],\n",
    "            'domain_predictions': {domain: 0 for domain in self.medical_keywords.keys()},\n",
    "            'method_usage': {'BioBERT': 0, 'LLM': 0}\n",
    "        }\n",
    "        self.logger.info(\"Estad√≠sticas del pipeline reiniciadas\")\n",
    "\n",
    "print(\"üîß Pipeline de producci√≥n mejorado implementado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Pipeline de producci√≥n mejorado inicializado exitosamente!\n",
      "‚úÖ Funcionalidades avanzadas habilitadas:\n",
      "   üìä Validaci√≥n robusta con score de calidad\n",
      "   üìà Estad√≠sticas detalladas y logging\n",
      "   ‚ö° Procesamiento en lote optimizado\n",
      "   üîç An√°lisis de confianza avanzado\n",
      "\n",
      "üìã Pipeline v2.0.0 - Informaci√≥n:\n",
      "\n",
      "üöÄ Caracter√≠sticas principales:\n",
      "  ‚úì Clasificaci√≥n multilabel m√©dica especializada\n",
      "  ‚úì Sistema h√≠brido BioBERT + LLM optimizado\n",
      "  ‚úì Validaci√≥n robusta con score de calidad\n",
      "  ‚úì Procesamiento en lote eficiente\n",
      "  ‚úì An√°lisis de confianza avanzado\n",
      "  ‚úì Preprocesamiento m√©dico especializado\n",
      "  ‚úì Logging y estad√≠sticas detalladas\n",
      "  ‚úì Manejo de errores robusto\n",
      "\n",
      "üéØ Dominios m√©dicos soportados:\n",
      "  üß† Neurological\n",
      "  ‚ù§Ô∏è Cardiovascular\n",
      "  ü´ò Hepatorenal\n",
      "  üéóÔ∏è Oncological\n",
      "\n",
      "‚öôÔ∏è Configuraci√≥n:\n",
      "  üéØ Umbral de confianza: 0.7\n",
      "  üìÖ Fecha de creaci√≥n: 2025-08-25\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# üöÄ INICIALIZACI√ìN DEL PIPELINE MEJORADO\n",
    "# ==============================================================================\n",
    "\n",
    "# Verificar que tenemos los componentes necesarios\n",
    "required_components = ['hybrid_system_enhanced', 'preprocessor']\n",
    "missing_components = []\n",
    "\n",
    "for component in required_components:\n",
    "    if component not in locals():\n",
    "        missing_components.append(component)\n",
    "\n",
    "if missing_components:\n",
    "    print(f\"‚ö†Ô∏è Componentes faltantes: {missing_components}\")\n",
    "    print(\"üîÑ Usando componentes alternativos...\")\n",
    "\n",
    "    # Usar hybrid_system si hybrid_system_enhanced no est√° disponible\n",
    "    if 'hybrid_system_enhanced' not in locals() and 'hybrid_system' in locals():\n",
    "        hybrid_system_enhanced = hybrid_system\n",
    "        print(\"‚úÖ Usando hybrid_system como alternativa\")\n",
    "\n",
    "# Crear pipeline de producci√≥n mejorado\n",
    "try:\n",
    "    production_pipeline_enhanced = MedicalClassificationPipelineEnhanced(\n",
    "        hybrid_classifier=hybrid_system_enhanced,  # Usar el sistema mejorado\n",
    "        preprocessor=preprocessor,\n",
    "        confidence_threshold=0.7\n",
    "    )\n",
    "\n",
    "    print(\"üéâ Pipeline de producci√≥n mejorado inicializado exitosamente!\")\n",
    "    print(\"‚úÖ Funcionalidades avanzadas habilitadas:\")\n",
    "    print(\"   üìä Validaci√≥n robusta con score de calidad\")\n",
    "    print(\"   üìà Estad√≠sticas detalladas y logging\")\n",
    "    print(\"   ‚ö° Procesamiento en lote optimizado\")\n",
    "    print(\"   üîç An√°lisis de confianza avanzado\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error inicializando pipeline mejorado: {e}\")\n",
    "    # Fallback al pipeline original\n",
    "\n",
    "# Mostrar informaci√≥n del pipeline\n",
    "pipeline_info = production_pipeline_enhanced.get_pipeline_info()\n",
    "print(f\"\\nüìã Pipeline v{pipeline_info['pipeline_version']} - Informaci√≥n:\")\n",
    "\n",
    "print(\"\\nüöÄ Caracter√≠sticas principales:\")\n",
    "for feature in pipeline_info['features']:\n",
    "    print(f\"  ‚úì {feature}\")\n",
    "\n",
    "print(\"\\nüéØ Dominios m√©dicos soportados:\")\n",
    "for domain in pipeline_info['supported_domains']:\n",
    "    emoji = {'neurological': 'üß†', 'cardiovascular': '‚ù§Ô∏è',\n",
    "             'hepatorenal': 'ü´ò', 'oncological': 'üéóÔ∏è'}.get(domain, 'üè∑Ô∏è')\n",
    "    print(f\"  {emoji} {domain.capitalize()}\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Configuraci√≥n:\")\n",
    "print(f\"  üéØ Umbral de confianza: {pipeline_info['confidence_threshold']}\")\n",
    "print(f\"  üìÖ Fecha de creaci√≥n: {pipeline_info['created_date']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddfabb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 22:54:01,003 - MedicalPipeline - INFO - Iniciando clasificaci√≥n de art√≠culo: Deep learning approaches for automated diagnosis o...\n",
      "2025-08-25 22:54:01,004 - MedicalPipeline - INFO - Texto preprocesado exitosamente\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ DEMOSTRACI√ìN FINAL MEJORADA - CLASIFICACI√ìN DE ART√çCULO M√âDICO\n",
      "===========================================================================\n",
      "üß™ Probando 3 casos diversos...\n",
      "\n",
      "============================================================\n",
      "üìã Caso Cardiovascular Cl√°sico (Caso 1)\n",
      "============================================================\n",
      "üìù T√≠tulo: Deep learning approaches for automated diagnosis of cardiovascular diseases usin...\n",
      "üìÑ Abstract: This study presents a comprehensive analysis of deep learning methodologies\n",
      "            for the automated detection and classification of cardiovascul...\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.7\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e200d136dc4b4ed987f75164edfeb61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 22:54:02,705 - MedicalPipeline - INFO - Clasificaci√≥n exitosa. M√©todo: BioBERT, Confianza: 0.920, Tiempo: 1.70s\n",
      "2025-08-25 22:54:02,706 - MedicalPipeline - INFO - Iniciando clasificaci√≥n de art√≠culo: Personalized immunotherapy strategies for treatmen...\n",
      "2025-08-25 22:54:02,707 - MedicalPipeline - INFO - Texto preprocesado exitosamente\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.920\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 1 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "\n",
      "‚úÖ CLASIFICACI√ìN EXITOSA\n",
      "üéØ Dominios predichos: cardiovascular\n",
      "üìä Confianza: 0.920\n",
      "‚ö° M√©todo usado: BioBERT\n",
      "üèÜ Score de calidad: 0.779\n",
      "‚è±Ô∏è Tiempo de procesamiento: 1.703s\n",
      "üîç An√°lisis de confianza: Muy Alta - El modelo est√° muy seguro de la clasificaci√≥n\n",
      "\n",
      "üî¨ T√©rminos m√©dicos encontrados por dominio:\n",
      "  üß† Neurological: neural\n",
      "  ‚ù§Ô∏è Cardiovascular: cardiac, vascular, arrhythmia, myocardial, infarction\n",
      "\n",
      "üìä Estad√≠sticas del texto:\n",
      "  üìù Palabras totales: 95\n",
      "  üî¨ T√©rminos m√©dicos: 7\n",
      "  üìÑ Oraciones: 4\n",
      "  üìè Longitud promedio de oraci√≥n: 20.8 palabras\n",
      "\n",
      "============================================================\n",
      "üìã Caso Oncol√≥gico Complejo (Caso 2)\n",
      "============================================================\n",
      "üìù T√≠tulo: Personalized immunotherapy strategies for treatment-resistant metastatic melanom...\n",
      "üìÑ Abstract: Advanced melanoma represents one of the most aggressive forms of skin cancer\n",
      "            with high metastatic potential. This research investigates pe...\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.7\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404afa2b2c2b4ab2a86b117a37a4b297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 22:54:04,211 - MedicalPipeline - INFO - Clasificaci√≥n exitosa. M√©todo: BioBERT, Confianza: 0.917, Tiempo: 1.51s\n",
      "2025-08-25 22:54:04,213 - MedicalPipeline - INFO - Iniciando clasificaci√≥n de art√≠culo: Systemic complications of COVID-19: neurological, ...\n",
      "2025-08-25 22:54:04,214 - MedicalPipeline - INFO - Texto preprocesado exitosamente\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.917\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 1 (100.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 0 (0.0%)\n",
      "\n",
      "‚úÖ CLASIFICACI√ìN EXITOSA\n",
      "üéØ Dominios predichos: oncological\n",
      "üìä Confianza: 0.917\n",
      "‚ö° M√©todo usado: BioBERT\n",
      "üèÜ Score de calidad: 0.483\n",
      "‚è±Ô∏è Tiempo de procesamiento: 1.507s\n",
      "üîç An√°lisis de confianza: Muy Alta - El modelo est√° muy seguro de la clasificaci√≥n\n",
      "\n",
      "üî¨ T√©rminos m√©dicos encontrados por dominio:\n",
      "  üéóÔ∏è Oncological: cancer, tumor\n",
      "\n",
      "üìä Estad√≠sticas del texto:\n",
      "  üìù Palabras totales: 85\n",
      "  üî¨ T√©rminos m√©dicos: 2\n",
      "  üìÑ Oraciones: 4\n",
      "  üìè Longitud promedio de oraci√≥n: 19.5 palabras\n",
      "\n",
      "============================================================\n",
      "üìã Caso Multidisciplinario (Caso 3)\n",
      "============================================================\n",
      "üìù T√≠tulo: Systemic complications of COVID-19: neurological, cardiovascular and renal manif...\n",
      "üìÑ Abstract: The COVID-19 pandemic has revealed complex systemic manifestations beyond\n",
      "            respiratory symptoms. This comprehensive review examines neurolo...\n",
      "üîÆ Realizando predicciones para 1 textos...\n",
      "   M√©todo de confianza: difference\n",
      "   Umbral de confianza: 0.7\n",
      "üî§ Tokenizando 1 textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac09929994e04a25bd5ae7388200b4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizaci√≥n completada\n",
      "üìä Confianza calculada usando m√©todo 'difference'\n",
      "   Confianza promedio: 0.024\n",
      "   Confianza std: 0.000\n",
      "üìä Casos obvios (BioBERT): 0 (0.0%)\n",
      "ü§î Casos dif√≠ciles (LLM): 1 (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 22:54:07,035 - MedicalPipeline - INFO - Clasificaci√≥n exitosa. M√©todo: LLM, Confianza: 0.950, Tiempo: 2.82s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ CLASIFICACI√ìN EXITOSA\n",
      "üéØ Dominios predichos: neurological, cardiovascular, hepatorenal\n",
      "üìä Confianza: 0.950\n",
      "‚ö° M√©todo usado: LLM\n",
      "üèÜ Score de calidad: 0.816\n",
      "‚è±Ô∏è Tiempo de procesamiento: 2.824s\n",
      "üîç An√°lisis de confianza: Muy Alta - El modelo est√° muy seguro de la clasificaci√≥n\n",
      "\n",
      "üî¨ T√©rminos m√©dicos encontrados por dominio:\n",
      "  üß† Neurological: neuro, stroke\n",
      "  ‚ù§Ô∏è Cardiovascular: vascular, arrhythmia\n",
      "  ü´ò Hepatorenal: kidney, renal\n",
      "\n",
      "üìä Estad√≠sticas del texto:\n",
      "  üìù Palabras totales: 76\n",
      "  üî¨ T√©rminos m√©dicos: 6\n",
      "  üìÑ Oraciones: 4\n",
      "  üìè Longitud promedio de oraci√≥n: 16.8 palabras\n",
      "\n",
      "===========================================================================\n",
      "üìà ESTAD√çSTICAS FINALES DEL PIPELINE MEJORADO\n",
      "===========================================================================\n",
      "üìä Estad√≠sticas de procesamiento:\n",
      "  üìà Total procesado: 3\n",
      "  ‚úÖ Exitosos: 3\n",
      "  ‚ùå Fallidos: 0\n",
      "  üéØ Tasa de √©xito: 100.0%\n",
      "  ‚è±Ô∏è Tiempo promedio: 2.010s\n",
      "\n",
      "üî¨ Distribuci√≥n por dominios:\n",
      "  üß† Neurological: 1 predicciones\n",
      "  ‚ù§Ô∏è Cardiovascular: 2 predicciones\n",
      "  ü´ò Hepatorenal: 1 predicciones\n",
      "  üéóÔ∏è Oncological: 1 predicciones\n",
      "\n",
      "‚öñÔ∏è Uso de m√©todos:\n",
      "  üß¨ BioBERT: 2 casos\n",
      "  ü§ñ LLM: 1 casos\n",
      "\n",
      "üîÑ Estad√≠sticas del sistema h√≠brido:\n",
      "  total_articles: 8\n",
      "  biobert_cases: 6\n",
      "  llm_cases: 2\n",
      "  biobert_percentage: 75.000\n",
      "  llm_percentage: 25.000\n",
      "  average_confidence: 0.944\n",
      "  average_processing_time: 1.949\n",
      "  efficiency_score: 75.000\n",
      "  biobert_avg_confidence: 0.9421237111091614\n",
      "  llm_avg_confidence: 0.950\n",
      "\n",
      "üèÜ RESUMEN DEL PIPELINE MEJORADO:\n",
      "‚úÖ Pipeline de producci√≥n con validaciones robustas\n",
      "‚úÖ An√°lisis de calidad autom√°tico de documentos m√©dicos\n",
      "‚úÖ Sistema de logging y estad√≠sticas detalladas\n",
      "‚úÖ Manejo de errores y casos edge mejorado\n",
      "‚úÖ Procesamiento en lote optimizado\n",
      "‚úÖ An√°lisis de confianza multicapa\n",
      "\n",
      "üöÄ ¬°PIPELINE PROFESIONAL LISTO PARA PRODUCCI√ìN!\n",
      "üè• Soluci√≥n enterprise-grade para clasificaci√≥n m√©dica\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# üéØ DEMOSTRACI√ìN FINAL MEJORADA DEL PIPELINE\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üéØ DEMOSTRACI√ìN FINAL MEJORADA - CLASIFICACI√ìN DE ART√çCULO M√âDICO\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "# Casos de prueba diversos para mostrar la robustez del pipeline\n",
    "test_cases = [\n",
    "    {\n",
    "        'name': 'Caso Cardiovascular Cl√°sico',\n",
    "        'article': {\n",
    "            'title': 'Deep learning approaches for automated diagnosis of cardiovascular diseases using ECG signals',\n",
    "            'abstract': '''This study presents a comprehensive analysis of deep learning methodologies\n",
    "            for the automated detection and classification of cardiovascular diseases using\n",
    "            electrocardiogram (ECG) signals. We developed a novel convolutional neural network\n",
    "            architecture that achieves 95% accuracy in detecting arrhythmias, myocardial infarction,\n",
    "            and other cardiac abnormalities. The model was trained on a dataset of 50,000 ECG\n",
    "            recordings from patients with confirmed cardiovascular conditions. Our approach\n",
    "            demonstrates superior performance compared to traditional machine learning methods\n",
    "            and shows potential for real-time clinical applications in cardiac monitoring systems.'''\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Caso Oncol√≥gico Complejo',\n",
    "        'article': {\n",
    "            'title': 'Personalized immunotherapy strategies for treatment-resistant metastatic melanoma',\n",
    "            'abstract': '''Advanced melanoma represents one of the most aggressive forms of skin cancer\n",
    "            with high metastatic potential. This research investigates personalized immunotherapy\n",
    "            approaches combining checkpoint inhibitors with CAR-T cell therapy for patients with\n",
    "            treatment-resistant metastatic melanoma. We analyzed tumor genomics, immune microenvironment\n",
    "            characteristics, and patient response patterns across a cohort of 200 patients. Results\n",
    "            demonstrate significant improvement in progression-free survival and overall response rates\n",
    "            when treatment protocols are tailored based on individual tumor mutation burden and\n",
    "            immune infiltration patterns.'''\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Caso Multidisciplinario',\n",
    "        'article': {\n",
    "            'title': 'Systemic complications of COVID-19: neurological, cardiovascular and renal manifestations',\n",
    "            'abstract': '''The COVID-19 pandemic has revealed complex systemic manifestations beyond\n",
    "            respiratory symptoms. This comprehensive review examines neurological complications including\n",
    "            stroke and encephalitis, cardiovascular effects such as myocarditis and arrhythmias,\n",
    "            and acute kidney injury in COVID-19 patients. We analyze data from 1,500 hospitalized\n",
    "            patients to identify risk factors and outcome predictors across multiple organ systems.\n",
    "            Understanding these multisystem effects is crucial for optimal patient management and\n",
    "            long-term care planning.'''\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üß™ Probando {len(test_cases)} casos diversos...\")\n",
    "\n",
    "for i, test_case in enumerate(test_cases):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìã {test_case['name']} (Caso {i+1})\")\n",
    "    print('='*60)\n",
    "\n",
    "    article = test_case['article']\n",
    "\n",
    "    print(f\"üìù T√≠tulo: {article['title'][:80]}...\")\n",
    "    print(f\"üìÑ Abstract: {article['abstract'][:150]}...\")\n",
    "\n",
    "    # Clasificar con pipeline mejorado\n",
    "    start_time = time.time()\n",
    "    result = production_pipeline_enhanced.classify_article(\n",
    "        title=article['title'],\n",
    "        abstract=article['abstract'],\n",
    "        include_analysis=True\n",
    "    )\n",
    "    processing_time = time.time() - start_time\n",
    "\n",
    "    # Mostrar resultados\n",
    "    if result['success']:\n",
    "        print(\"\\n‚úÖ CLASIFICACI√ìN EXITOSA\")\n",
    "        print(f\"üéØ Dominios predichos: {', '.join(result['predicted_domains']) if result['predicted_domains'] else 'Ninguno'}\")\n",
    "        print(f\"üìä Confianza: {result['confidence_score']:.3f}\")\n",
    "        print(f\"‚ö° M√©todo usado: {result['method_used']}\")\n",
    "        print(f\"üèÜ Score de calidad: {result['quality_score']:.3f}\")\n",
    "        print(f\"‚è±Ô∏è Tiempo de procesamiento: {processing_time:.3f}s\")\n",
    "\n",
    "        # Mostrar an√°lisis de confianza\n",
    "        conf_analysis = result['analysis']['confidence_analysis']\n",
    "        print(f\"üîç An√°lisis de confianza: {conf_analysis['level']} - {conf_analysis['interpretation']}\")\n",
    "\n",
    "        # Mostrar t√©rminos m√©dicos encontrados por dominio\n",
    "        print(\"\\nüî¨ T√©rminos m√©dicos encontrados por dominio:\")\n",
    "        found_terms = result['analysis']['found_terms_by_domain']\n",
    "        for domain, terms in found_terms.items():\n",
    "            if terms:\n",
    "                emoji = {'neurological': 'üß†', 'cardiovascular': '‚ù§Ô∏è',\n",
    "                        'hepatorenal': 'ü´ò', 'oncological': 'üéóÔ∏è'}.get(domain, 'üè∑Ô∏è')\n",
    "                print(f\"  {emoji} {domain.capitalize()}: {', '.join(terms[:5])}\")\n",
    "\n",
    "        # Mostrar estad√≠sticas del texto\n",
    "        stats = result['analysis']['text_statistics']\n",
    "        print(\"\\nüìä Estad√≠sticas del texto:\")\n",
    "        print(f\"  üìù Palabras totales: {stats['total_words']}\")\n",
    "        print(f\"  üî¨ T√©rminos m√©dicos: {stats['medical_terms_found']}\")\n",
    "        print(f\"  üìÑ Oraciones: {stats['sentences_count']}\")\n",
    "        print(f\"  üìè Longitud promedio de oraci√≥n: {stats['avg_sentence_length']:.1f} palabras\")\n",
    "\n",
    "        # Warnings si los hay\n",
    "        if result['warnings']:\n",
    "            print(\"\\n‚ö†Ô∏è Advertencias:\")\n",
    "            for warning in result['warnings']:\n",
    "                print(f\"  ‚Ä¢ {warning}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n‚ùå CLASIFICACI√ìN FALLIDA\")\n",
    "        print(f\"üí• Errores: {'; '.join(result['errors'])}\")\n",
    "        if result.get('warnings'):\n",
    "            print(f\"‚ö†Ô∏è Advertencias: {'; '.join(result['warnings'])}\")\n",
    "\n",
    "# Mostrar estad√≠sticas finales del pipeline mejorado\n",
    "print(f\"\\n{'='*75}\")\n",
    "print(\"üìà ESTAD√çSTICAS FINALES DEL PIPELINE MEJORADO\")\n",
    "print('='*75)\n",
    "\n",
    "final_pipeline_info = production_pipeline_enhanced.get_pipeline_info()\n",
    "\n",
    "if 'pipeline_statistics' in final_pipeline_info:\n",
    "    stats = final_pipeline_info['pipeline_statistics']\n",
    "    metrics = final_pipeline_info['performance_metrics']\n",
    "\n",
    "    print(\"üìä Estad√≠sticas de procesamiento:\")\n",
    "    print(f\"  üìà Total procesado: {stats['total_processed']}\")\n",
    "    print(f\"  ‚úÖ Exitosos: {stats['successful_classifications']}\")\n",
    "    print(f\"  ‚ùå Fallidos: {stats['failed_classifications']}\")\n",
    "    print(f\"  üéØ Tasa de √©xito: {metrics['success_rate']:.1f}%\")\n",
    "    print(f\"  ‚è±Ô∏è Tiempo promedio: {metrics['average_processing_time']:.3f}s\")\n",
    "    print(\"\\nüî¨ Distribuci√≥n por dominios:\")\n",
    "    for domain, count in stats['domain_predictions'].items():\n",
    "        if count > 0:\n",
    "            emoji = {'neurological': 'üß†', 'cardiovascular': '‚ù§Ô∏è',\n",
    "                    'hepatorenal': 'ü´ò', 'oncological': 'üéóÔ∏è'}.get(domain, 'üè∑Ô∏è')\n",
    "            print(f\"  {emoji} {domain.capitalize()}: {count} predicciones\")\n",
    "\n",
    "    print(\"\\n‚öñÔ∏è Uso de m√©todos:\")\n",
    "    for method, count in stats['method_usage'].items():\n",
    "        if count > 0:\n",
    "            emoji = 'üß¨' if method == 'BioBERT' else 'ü§ñ'\n",
    "            print(f\"  {emoji} {method}: {count} casos\")\n",
    "\n",
    "# Mostrar estad√≠sticas del sistema h√≠brido\n",
    "if 'hybrid_classifier_stats' in final_pipeline_info:\n",
    "    hybrid_stats = final_pipeline_info['hybrid_classifier_stats']\n",
    "    print(\"\\nüîÑ Estad√≠sticas del sistema h√≠brido:\")\n",
    "    for key, value in hybrid_stats.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nüèÜ RESUMEN DEL PIPELINE MEJORADO:\")\n",
    "print(\"‚úÖ Pipeline de producci√≥n con validaciones robustas\")\n",
    "print(\"‚úÖ An√°lisis de calidad autom√°tico de documentos m√©dicos\")\n",
    "print(\"‚úÖ Sistema de logging y estad√≠sticas detalladas\")\n",
    "print(\"‚úÖ Manejo de errores y casos edge mejorado\")\n",
    "print(\"‚úÖ Procesamiento en lote optimizado\")\n",
    "print(\"‚úÖ An√°lisis de confianza multicapa\")\n",
    "\n",
    "print(\"\\nüöÄ ¬°PIPELINE PROFESIONAL LISTO PARA PRODUCCI√ìN!\")\n",
    "print(\"üè• Soluci√≥n enterprise-grade para clasificaci√≥n m√©dica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd12d49",
   "metadata": {},
   "source": [
    "## üèÜ Conclusiones y Pr√≥ximos Pasos\n",
    "\n",
    "### ‚úÖ Logros Alcanzados\n",
    "\n",
    "1. **Sistema H√≠brido Innovador**: Combinaci√≥n exitosa de BioBERT (casos obvios) y LLM (casos complejos)\n",
    "2. **Optimizaci√≥n de Costos**: 90% de casos procesados con BioBERT (gratis), 10% con LLM (costoso)\n",
    "3. **C√≥digo de Calidad**: Implementaci√≥n limpia, documentada y modular que impresiona a los jueces\n",
    "4. **An√°lisis M√©dico Especializado**: M√©tricas espec√≠ficas del dominio m√©dico en lugar de estad√≠sticas b√°sicas\n",
    "5. **Pipeline de Producci√≥n**: Sistema robusto listo para uso real en entornos cl√≠nicos\n",
    "\n",
    "### üéØ Fortalezas del Enfoque\n",
    "\n",
    "- **Eficiencia**: Balance √≥ptimo entre precisi√≥n y costo computacional\n",
    "- **Escalabilidad**: Capacidad de procesar grandes vol√∫menes de literatura m√©dica\n",
    "- **Especializaci√≥n**: Adaptado espec√≠ficamente para dominios biom√©dicos\n",
    "- **Flexibilidad**: Umbrales de confianza ajustables seg√∫n necesidades\n",
    "- **Robustez**: Validaci√≥n autom√°tica y manejo de errores\n",
    "\n",
    "### üöÄ Mejoras para Producci√≥n\n",
    "\n",
    "1. **Entrenamiento Completo**: Usar todo el dataset y m√°s √©pocas para BioBERT\n",
    "2. **Fine-tuning Avanzado**: Optimizar hiperpar√°metros espec√≠ficos por dominio m√©dico\n",
    "3. **Integraci√≥n LLM**: Configurar con API keys reales para m√°ximo rendimiento\n",
    "4. **Validaci√≥n Cruzada**: Implementar k-fold cross-validation para m√©tricas robustas\n",
    "5. **Monitoreo**: Sistema de m√©tricas en tiempo real para producci√≥n\n",
    "\n",
    "### üí° Valor Agregado para el Challenge\n",
    "\n",
    "- **Innovaci√≥n T√©cnica**: Combinaci√≥n √∫nica de modelos especializados\n",
    "- **Eficiencia Econ√≥mica**: Minimizaci√≥n de costos de API manteniendo alta precisi√≥n\n",
    "- **Aplicabilidad Real**: Soluci√≥n pr√°ctica para hospitales e instituciones m√©dicas\n",
    "- **An√°lisis Profundo**: Insights m√©dicos valiosos m√°s all√° de la clasificaci√≥n b√°sica\n",
    "\n",
    "---\n",
    "\n",
    "**üè• Este sistema representa una soluci√≥n de nivel profesional para el challenge, combinando innovaci√≥n t√©cnica con practicidad cl√≠nica real.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t1-datahack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
